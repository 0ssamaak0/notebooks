{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this tutorial, we are going to fine-tune the new [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) on the [ELI5](https://huggingface.co/datasets/eli5) dataset to improve the explanation and question-answering skills of the agent. The [ELI5](https://huggingface.co/datasets/eli5) dataset is an English-language dataset of questions and answers gathered from three subreddits where users ask factual questions requiring paragraph-length or longer answers. We are going to use Hugging Face Transformers and DeepSpeed ZeRO to fine-tune our model.\n",
    "\n",
    "\n",
    "\n",
    "https://engineering.fb.com/2021/07/15/open-source/fsdp/\n",
    "https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/\n",
    "https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" \"sagemaker>=2.150.0\" --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [ELI5](https://huggingface.co/datasets/eli5) dataset, but before fine-tuning the model, we need to preprocess the data. We will create a \"chat\" version of the dataset by adding `<user>` and `<bot>`tokens and add an end-of-sequence `<|endoftext|>` token to help the model learn to distinguish consecutive examples. Additionally, we create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face. The dataset contains `272634` samples for `eli5`. We will downsample the dataset to `10 000` to make it more realistic for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 12:33:15.163974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 12:33:18.118510: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-28 12:33:18.864615: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-28 12:33:18.864635: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-28 12:33:21.344311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-28 12:33:21.344413: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-28 12:33:21.344421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Found cached dataset eli5 (/home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-ff13b89bd5550ed9.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "# Load Tokenizer \n",
    "model_id = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset_id = \"eli5\"\n",
    "dataset = load_dataset(dataset_id, split=\"train_eli5\")\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42).select(range(10_000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [ELI5](https://huggingface.co/datasets/eli5) sample can include multiple answers to a “question”. We will select the answer with the highest user score for our explanation. \n",
    "\n",
    "*Note: This dataset is a good example of using reinforcement learning for training transformers learning to generate answers with higher scores. Let me know if you are interested in an example of that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-8abed608d38c178b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<human>: Explain like I am five: What does it mean when the United States has spent over five trillion dollars as a result of wars after 9/11?\\n<bot>: It means that the US has borrowed 5 trillion from the federal reserve and has paid both for equipment, salary to troops, and for resources to fund the wars.<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# dataset template for chat conversation\n",
    "template=f'''<human>: Explain like I am five: {{question}}\n",
    "<bot>: {{answer}}{{eos_token}}'''\n",
    "\n",
    "eos_token = tokenizer.eos_token \n",
    "\n",
    "def template_dataset(sample):\n",
    "\tsample[\"text\"] = template.format(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tquestion=sample[\"title\"], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tanswer=sample[\"answers\"][\"text\"][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\teos_token=eos_token\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\treturn sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(dataset[randint(0, 10_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-c4de07e830066b12.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-276409c2387343d5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 902\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d610615399bf4796b7bedc658b18139f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-558105141721/processed/eli-5/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/eli-5/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = \"s3://sagemaker-us-east-1-558105141721/processed/eli-5/train\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the GPT model using PyTorch FSDP \n",
    "\n",
    "In addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for BLOOMZ ~4x.  \n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements uses PEFT to train our model. If you are interested in how this works check-out [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft) blog, where we explain the training script in detail. T\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': 'togethercomputer/GPT-NeoXT-Chat-Base-20B',\n",
    "    # 'model_id': 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
    "    # 'model_id': 'EleutherAI/pythia-12b',\n",
    "    'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "    'gradient_checkpointing': True,\n",
    "    'bf16': True,\n",
    "    # 'optimizer': \"adafactor\",\n",
    "    'optimizer': \"adamw_apex_fused\",\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'epochs': 1,\n",
    "    'fsdp': '\"full_shard auto_wrap\"',\n",
    "    'fsdp_transformer_layer_cls_to_wrap': \"GPTNeoXLayer\",\n",
    "}\n",
    "\n",
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_clm.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=2,\n",
    "    volume_size=200,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version=\"py39\",\n",
    "    hyperparameters = hyperparameters,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "# huggingface_estimator.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-04-28-20-12-03-592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 20:12:05 Starting - Starting the training job......\n",
      "2023-04-28 20:13:00 Starting - Preparing the instances for training............\n",
      "2023-04-28 20:15:04 Downloading - Downloading input data\n",
      "2023-04-28 20:15:04 Training - Downloading the training image............\n",
      "2023-04-28 20:17:15 Training - Training image download completed. Training in progress........bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-04-28 20:18:16,162 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-04-28 20:18:16,222 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:16,231 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-04-28 20:18:16,233 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-04-28 20:18:16,233 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-04-28 20:18:16,442 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.28.1\n",
      "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 74.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Installing collected packages: transformers\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.26.0\n",
      "Uninstalling transformers-4.26.0:\n",
      "Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed transformers-4.28.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-04-28 20:18:23,105 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 20:18:23,105 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-28 20:18:23,174 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:23,249 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:23,259 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-04-28 20:18:23,346 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:23,356 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"fsdp\": \"\\\"full_shard auto_wrap\\\"\",\n",
      "        \"fsdp_transformer_layer_cls_to_wrap\": \"GPTNeoXLayer\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"model_id\": \"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n",
      "        \"optimizer\": \"adamw_apex_fused\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-04-28-20-12-03-592\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"GPTNeoXLayer\",\"gradient_checkpointing\":true,\"model_id\":\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"optimizer\":\"adamw_apex_fused\",\"per_device_train_batch_size\":2}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=96\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"GPTNeoXLayer\",\"gradient_checkpointing\":true,\"model_id\":\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"optimizer\":\"adamw_apex_fused\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-04-28-20-12-03-592\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--fsdp\",\"\\\"full_shard auto_wrap\\\"\",\"--fsdp_transformer_layer_cls_to_wrap\",\"GPTNeoXLayer\",\"--gradient_checkpointing\",\"True\",\"--model_id\",\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"--optimizer\",\"adamw_apex_fused\",\"--per_device_train_batch_size\",\"2\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_BF16=true\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_FSDP=\"full_shard auto_wrap\"\n",
      "SM_HP_FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP=GPTNeoXLayer\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_MODEL_ID=togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "SM_HP_OPTIMIZER=adamw_apex_fused\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 run_clm.py --bf16 True --dataset_path /opt/ml/input/data/training --epochs 1 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap GPTNeoXLayer --gradient_checkpointing True --model_id togethercomputer/GPT-NeoXT-Chat-Base-20B --optimizer adamw_apex_fused --per_device_train_batch_size 2\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-04-28 20:18:20,664 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-04-28 20:18:20,726 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:20,735 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-04-28 20:18:20,737 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-04-28 20:18:20,737 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-04-28 20:18:20,942 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.28.1\n",
      "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 63.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2.1.1)\n",
      "Installing collected packages: transformers\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.26.0\n",
      "Uninstalling transformers-4.26.0:\n",
      "Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed transformers-4.28.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-04-28 20:18:27,786 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 20:18:27,786 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-28 20:18:27,852 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:27,925 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:27,934 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-04-28 20:18:27,997 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 20:18:28,007 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"fsdp\": \"\\\"full_shard auto_wrap\\\"\",\n",
      "        \"fsdp_transformer_layer_cls_to_wrap\": \"GPTNeoXLayer\",\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"model_id\": \"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n",
      "        \"optimizer\": \"adamw_apex_fused\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-04-28-20-12-03-592\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"GPTNeoXLayer\",\"gradient_checkpointing\":true,\"model_id\":\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"optimizer\":\"adamw_apex_fused\",\"per_device_train_batch_size\":2}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=96\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"fsdp\":\"\\\"full_shard auto_wrap\\\"\",\"fsdp_transformer_layer_cls_to_wrap\":\"GPTNeoXLayer\",\"gradient_checkpointing\":true,\"model_id\":\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"optimizer\":\"adamw_apex_fused\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-04-28-20-12-03-592\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-20-12-03-592/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--fsdp\",\"\\\"full_shard auto_wrap\\\"\",\"--fsdp_transformer_layer_cls_to_wrap\",\"GPTNeoXLayer\",\"--gradient_checkpointing\",\"True\",\"--model_id\",\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\"--optimizer\",\"adamw_apex_fused\",\"--per_device_train_batch_size\",\"2\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_BF16=true\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_FSDP=\"full_shard auto_wrap\"\n",
      "SM_HP_FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP=GPTNeoXLayer\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_MODEL_ID=togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "SM_HP_OPTIMIZER=adamw_apex_fused\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 0 run_clm.py --bf16 True --dataset_path /opt/ml/input/data/training --epochs 1 --fsdp \"full_shard auto_wrap\" --fsdp_transformer_layer_cls_to_wrap GPTNeoXLayer --gradient_checkpointing True --model_id togethercomputer/GPT-NeoXT-Chat-Base-20B --optimizer adamw_apex_fused --per_device_train_batch_size 2\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/671 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 671/671 [00:00<00:00, 109kB/s]\n",
      "Downloading (…)model.bin.index.json:   0%|          | 0.00/57.7k [00:00<?, ?B/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 57.7k/57.7k [00:00<00:00, 8.82MB/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading (…)00001-of-00005.bin\";:   0%|          | 0.00/9.95G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 671/671 [00:00<00:00, 110kB/s]\n",
      "Downloading (…)model.bin.index.json:   0%|          | 0.00/57.7k [00:00<?, ?B/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 57.7k/57.7k [00:00<00:00, 8.88MB/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Downloading (…)00001-of-00005.bin\";:   0%|          | 0.00/9.95G [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   0%|          | 21.0M/9.95G [00:00<00:51, 194MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|          | 62.9M/9.95G [00:00<00:34, 284MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|          | 105M/9.95G [00:00<00:31, 312MB/s] #033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|▏         | 147M/9.95G [00:00<00:28, 349MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   2%|▏         | 189M/9.95G [00:00<00:26, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   2%|▏         | 231M/9.95G [00:00<00:25, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   0%|          | 21.0M/9.95G [00:00<00:50, 197MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|          | 52.4M/9.95G [00:00<00:39, 248MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|          | 94.4M/9.95G [00:00<00:34, 284MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   1%|▏         | 136M/9.95G [00:00<00:32, 303MB/s] #033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   2%|▏         | 178M/9.95G [00:00<00:31, 309MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   2%|▏         | 220M/9.95G [00:00<00:30, 316MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   3%|▎         | 262M/9.95G [00:00<00:32, 299MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   3%|▎         | 294M/9.95G [00:00<00:31, 302MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   3%|▎         | 273M/9.95G [00:00<00:25, 382MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   3%|▎         | 315M/9.95G [00:00<00:24, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   4%|▎         | 357M/9.95G [00:00<00:24, 398MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   4%|▍         | 398M/9.95G [00:01<00:23, 404MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▍         | 451M/9.95G [00:01<00:23, 409MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▍         | 493M/9.95G [00:01<00:23, 410MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▌         | 545M/9.95G [00:01<00:22, 413MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   6%|▌         | 587M/9.95G [00:01<00:22, 414MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   6%|▋         | 629M/9.95G [00:01<00:23, 400MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   7%|▋         | 671M/9.95G [00:01<00:23, 402MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   3%|▎         | 336M/9.95G [00:01<00:31, 310MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   4%|▍         | 377M/9.95G [00:01<00:30, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   4%|▍         | 419M/9.95G [00:01<00:29, 322MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▍         | 461M/9.95G [00:01<00:29, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▌         | 503M/9.95G [00:01<00:29, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   5%|▌         | 545M/9.95G [00:01<00:28, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   6%|▌         | 587M/9.95G [00:01<00:28, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   7%|▋         | 713M/9.95G [00:01<00:22, 402MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 755M/9.95G [00:01<00:22, 404MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 797M/9.95G [00:02<00:23, 394MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 839M/9.95G [00:02<00:22, 401MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   9%|▉         | 891M/9.95G [00:02<00:22, 407MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   9%|▉         | 933M/9.95G [00:02<00:21, 410MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  10%|▉         | 986M/9.95G [00:02<00:21, 415MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  10%|█         | 1.04G/9.95G [00:02<00:21, 419MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  11%|█         | 1.08G/9.95G [00:02<00:21, 415MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   6%|▋         | 629M/9.95G [00:02<00:29, 316MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   7%|▋         | 671M/9.95G [00:02<00:31, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   7%|▋         | 713M/9.95G [00:02<00:30, 300MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 755M/9.95G [00:02<00:29, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 797M/9.95G [00:02<00:28, 318MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   8%|▊         | 839M/9.95G [00:02<00:28, 323MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   9%|▉         | 881M/9.95G [00:02<00:28, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:   9%|▉         | 923M/9.95G [00:02<00:27, 329MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  11%|█▏        | 1.12G/9.95G [00:02<00:22, 399MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  12%|█▏        | 1.16G/9.95G [00:02<00:22, 399MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  12%|█▏        | 1.21G/9.95G [00:03<00:21, 398MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.25G/9.95G [00:03<00:21, 399MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.29G/9.95G [00:03<00:21, 395MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.33G/9.95G [00:03<00:21, 393MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  14%|█▍        | 1.37G/9.95G [00:03<00:22, 382MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  14%|█▍        | 1.42G/9.95G [00:03<00:22, 380MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▍        | 1.46G/9.95G [00:03<00:22, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  10%|▉         | 965M/9.95G [00:03<00:27, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  10%|█         | 1.01G/9.95G [00:03<00:27, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  11%|█         | 1.05G/9.95G [00:03<00:27, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  11%|█         | 1.09G/9.95G [00:03<00:27, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  11%|█▏        | 1.13G/9.95G [00:03<00:27, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  12%|█▏        | 1.17G/9.95G [00:03<00:26, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  12%|█▏        | 1.22G/9.95G [00:03<00:26, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.26G/9.95G [00:03<00:26, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.30G/9.95G [00:04<00:26, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  13%|█▎        | 1.34G/9.95G [00:04<00:26, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  14%|█▍        | 1.38G/9.95G [00:04<00:26, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  14%|█▍        | 1.43G/9.95G [00:04<00:27, 309MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▍        | 1.46G/9.95G [00:04<00:27, 308MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▌        | 1.50G/9.95G [00:04<00:26, 314MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▌        | 1.54G/9.95G [00:04<00:26, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  16%|█▌        | 1.58G/9.95G [00:05<00:26, 314MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  16%|█▌        | 1.61G/9.95G [00:05<00:31, 267MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  17%|█▋        | 1.65G/9.95G [00:05<00:31, 262MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  17%|█▋        | 1.69G/9.95G [00:05<00:29, 284MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  17%|█▋        | 1.73G/9.95G [00:05<00:27, 301MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  18%|█▊        | 1.77G/9.95G [00:05<00:26, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  18%|█▊        | 1.81G/9.95G [00:05<00:25, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  19%|█▊        | 1.86G/9.95G [00:05<00:25, 318MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▌        | 1.50G/9.95G [00:03<00:22, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  15%|█▌        | 1.54G/9.95G [00:03<00:23, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  16%|█▌        | 1.58G/9.95G [00:04<00:23, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  16%|█▋        | 1.63G/9.95G [00:04<00:22, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  17%|█▋        | 1.67G/9.95G [00:04<00:22, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  17%|█▋        | 1.71G/9.95G [00:04<00:22, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  18%|█▊        | 1.75G/9.95G [00:04<00:23, 348MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  18%|█▊        | 1.79G/9.95G [00:04<00:22, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  18%|█▊        | 1.84G/9.95G [00:04<00:22, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  19%|█▉        | 1.88G/9.95G [00:04<00:21, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  19%|█▉        | 1.92G/9.95G [00:04<00:21, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  20%|█▉        | 1.96G/9.95G [00:05<00:21, 364MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  20%|██        | 2.00G/9.95G [00:05<00:21, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  21%|██        | 2.04G/9.95G [00:05<00:21, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  21%|██        | 2.09G/9.95G [00:05<00:20, 378MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  21%|██▏       | 2.13G/9.95G [00:05<00:20, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  22%|██▏       | 2.17G/9.95G [00:05<00:21, 358MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  22%|██▏       | 2.21G/9.95G [00:05<00:21, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  23%|██▎       | 2.25G/9.95G [00:05<00:21, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  23%|██▎       | 2.30G/9.95G [00:06<00:20, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  23%|██▎       | 2.34G/9.95G [00:06<00:20, 375MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  24%|██▍       | 2.38G/9.95G [00:06<00:20, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  24%|██▍       | 2.42G/9.95G [00:06<00:19, 378MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  25%|██▍       | 2.46G/9.95G [00:06<00:19, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  25%|██▌       | 2.51G/9.95G [00:06<00:19, 377MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  26%|██▌       | 2.55G/9.95G [00:06<00:19, 375MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  19%|█▉        | 1.90G/9.95G [00:06<00:24, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  19%|█▉        | 1.94G/9.95G [00:06<00:24, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  20%|█▉        | 1.98G/9.95G [00:06<00:24, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  20%|██        | 2.02G/9.95G [00:06<00:23, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  21%|██        | 2.07G/9.95G [00:06<00:23, 329MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  21%|██        | 2.11G/9.95G [00:06<00:24, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  22%|██▏       | 2.15G/9.95G [00:06<00:24, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  22%|██▏       | 2.19G/9.95G [00:06<00:24, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  26%|██▌       | 2.59G/9.95G [00:06<00:19, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  26%|██▋       | 2.63G/9.95G [00:06<00:19, 375MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  27%|██▋       | 2.67G/9.95G [00:07<00:19, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  27%|██▋       | 2.72G/9.95G [00:07<00:19, 378MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  28%|██▊       | 2.76G/9.95G [00:07<00:19, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  28%|██▊       | 2.80G/9.95G [00:07<00:20, 343MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▊       | 2.84G/9.95G [00:07<00:19, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▉       | 2.88G/9.95G [00:07<00:19, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▉       | 2.93G/9.95G [00:07<00:19, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  22%|██▏       | 2.23G/9.95G [00:07<00:24, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  23%|██▎       | 2.28G/9.95G [00:07<00:23, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  23%|██▎       | 2.32G/9.95G [00:07<00:23, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  24%|██▎       | 2.36G/9.95G [00:07<00:23, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  24%|██▍       | 2.40G/9.95G [00:07<00:23, 322MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  25%|██▍       | 2.44G/9.95G [00:07<00:23, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  25%|██▍       | 2.49G/9.95G [00:07<00:23, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  30%|██▉       | 2.97G/9.95G [00:07<00:18, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  30%|███       | 3.01G/9.95G [00:07<00:18, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███       | 3.05G/9.95G [00:08<00:19, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███       | 3.09G/9.95G [00:08<00:19, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███▏      | 3.14G/9.95G [00:08<00:18, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  32%|███▏      | 3.18G/9.95G [00:08<00:18, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  32%|███▏      | 3.22G/9.95G [00:08<00:17, 377MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  33%|███▎      | 3.26G/9.95G [00:08<00:19, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  25%|██▌       | 2.53G/9.95G [00:08<00:23, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  26%|██▌       | 2.57G/9.95G [00:08<00:23, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  26%|██▌       | 2.61G/9.95G [00:08<00:22, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  27%|██▋       | 2.65G/9.95G [00:08<00:23, 317MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  27%|██▋       | 2.69G/9.95G [00:08<00:22, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  27%|██▋       | 2.74G/9.95G [00:08<00:22, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  28%|██▊       | 2.78G/9.95G [00:08<00:22, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  28%|██▊       | 2.82G/9.95G [00:08<00:22, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  33%|███▎      | 3.30G/9.95G [00:08<00:18, 360MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▎      | 3.34G/9.95G [00:08<00:17, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▍      | 3.39G/9.95G [00:08<00:17, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▍      | 3.43G/9.95G [00:09<00:17, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  35%|███▍      | 3.47G/9.95G [00:09<00:17, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  35%|███▌      | 3.51G/9.95G [00:09<00:16, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  36%|███▌      | 3.55G/9.95G [00:09<00:16, 389MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  36%|███▌      | 3.60G/9.95G [00:09<00:16, 391MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.64G/9.95G [00:09<00:16, 381MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▊       | 2.85G/9.95G [00:09<00:23, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▉       | 2.88G/9.95G [00:09<00:23, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  29%|██▉       | 2.92G/9.95G [00:09<00:23, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  30%|██▉       | 2.95G/9.95G [00:09<00:22, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  30%|██▉       | 2.98G/9.95G [00:09<00:22, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  30%|███       | 3.01G/9.95G [00:09<00:22, 307MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███       | 3.04G/9.95G [00:09<00:22, 304MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███       | 3.07G/9.95G [00:09<00:22, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███       | 3.10G/9.95G [00:09<00:22, 300MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  31%|███▏      | 3.14G/9.95G [00:09<00:22, 297MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.68G/9.95G [00:09<00:16, 377MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.72G/9.95G [00:09<00:16, 377MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  38%|███▊      | 3.76G/9.95G [00:09<00:16, 381MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  38%|███▊      | 3.81G/9.95G [00:10<00:15, 386MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  39%|███▊      | 3.85G/9.95G [00:10<00:15, 387MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  39%|███▉      | 3.89G/9.95G [00:10<00:15, 388MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|███▉      | 3.93G/9.95G [00:10<00:15, 388MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|███▉      | 3.97G/9.95G [00:10<00:16, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|████      | 4.02G/9.95G [00:10<00:17, 345MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  32%|███▏      | 3.17G/9.95G [00:10<00:22, 301MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  32%|███▏      | 3.20G/9.95G [00:10<00:22, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  32%|███▏      | 3.23G/9.95G [00:10<00:22, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  33%|███▎      | 3.26G/9.95G [00:10<00:21, 306MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  33%|███▎      | 3.30G/9.95G [00:10<00:21, 309MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  33%|███▎      | 3.33G/9.95G [00:10<00:21, 304MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▍      | 3.37G/9.95G [00:10<00:23, 283MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▍      | 3.40G/9.95G [00:10<00:22, 287MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  34%|███▍      | 3.43G/9.95G [00:10<00:23, 282MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  41%|████      | 4.06G/9.95G [00:10<00:16, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  41%|████      | 4.10G/9.95G [00:10<00:16, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.14G/9.95G [00:11<00:16, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.18G/9.95G [00:11<00:18, 310MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.23G/9.95G [00:11<00:17, 323MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  43%|████▎     | 4.27G/9.95G [00:11<00:16, 338MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  43%|████▎     | 4.31G/9.95G [00:11<00:17, 323MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  44%|████▎     | 4.35G/9.95G [00:11<00:17, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  35%|███▍      | 3.46G/9.95G [00:11<00:23, 273MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  35%|███▌      | 3.49G/9.95G [00:11<00:22, 282MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  35%|███▌      | 3.52G/9.95G [00:11<00:22, 282MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  36%|███▌      | 3.57G/9.95G [00:11<00:21, 295MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  36%|███▌      | 3.61G/9.95G [00:11<00:21, 302MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.64G/9.95G [00:11<00:20, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.68G/9.95G [00:11<00:20, 308MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  37%|███▋      | 3.72G/9.95G [00:11<00:21, 296MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  44%|████▍     | 4.39G/9.95G [00:11<00:16, 343MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  45%|████▍     | 4.44G/9.95G [00:11<00:15, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  45%|████▍     | 4.48G/9.95G [00:12<00:15, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  45%|████▌     | 4.52G/9.95G [00:12<00:15, 349MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  46%|████▌     | 4.56G/9.95G [00:12<00:15, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  46%|████▌     | 4.60G/9.95G [00:12<00:14, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  47%|████▋     | 4.65G/9.95G [00:12<00:14, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  47%|████▋     | 4.69G/9.95G [00:12<00:14, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  48%|████▊     | 4.73G/9.95G [00:12<00:15, 340MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  38%|███▊      | 3.75G/9.95G [00:12<00:20, 300MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  38%|███▊      | 3.79G/9.95G [00:12<00:20, 301MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  38%|███▊      | 3.83G/9.95G [00:12<00:20, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  39%|███▉      | 3.86G/9.95G [00:12<00:19, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  39%|███▉      | 3.89G/9.95G [00:12<00:20, 302MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  39%|███▉      | 3.92G/9.95G [00:12<00:20, 297MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|███▉      | 3.96G/9.95G [00:12<00:20, 299MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|████      | 4.00G/9.95G [00:12<00:20, 293MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  40%|████      | 4.03G/9.95G [00:12<00:19, 298MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  48%|████▊     | 4.77G/9.95G [00:12<00:14, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  48%|████▊     | 4.81G/9.95G [00:12<00:14, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  49%|████▉     | 4.85G/9.95G [00:13<00:14, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  49%|████▉     | 4.90G/9.95G [00:13<00:13, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|████▉     | 4.94G/9.95G [00:13<00:13, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|█████     | 4.98G/9.95G [00:13<00:13, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|█████     | 5.02G/9.95G [00:13<00:13, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  51%|█████     | 5.06G/9.95G [00:13<00:13, 350MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  41%|████      | 4.07G/9.95G [00:13<00:19, 307MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  41%|████▏     | 4.11G/9.95G [00:13<00:18, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.14G/9.95G [00:13<00:18, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.18G/9.95G [00:13<00:18, 314MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  42%|████▏     | 4.22G/9.95G [00:13<00:18, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  43%|████▎     | 4.26G/9.95G [00:13<00:18, 314MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  43%|████▎     | 4.30G/9.95G [00:13<00:17, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  44%|████▎     | 4.34G/9.95G [00:13<00:17, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  44%|████▍     | 4.38G/9.95G [00:14<00:18, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  44%|████▍     | 4.41G/9.95G [00:14<00:18, 303MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  45%|████▍     | 4.46G/9.95G [00:14<00:17, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  45%|████▌     | 4.49G/9.95G [00:14<00:17, 307MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  46%|████▌     | 4.53G/9.95G [00:14<00:17, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  46%|████▌     | 4.56G/9.95G [00:14<00:17, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  46%|████▌     | 4.60G/9.95G [00:14<00:16, 315MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  47%|████▋     | 4.65G/9.95G [00:15<00:18, 292MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  47%|████▋     | 4.68G/9.95G [00:15<00:19, 276MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  47%|████▋     | 4.72G/9.95G [00:15<00:18, 288MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  48%|████▊     | 4.76G/9.95G [00:15<00:17, 297MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  48%|████▊     | 4.79G/9.95G [00:15<00:17, 299MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  49%|████▊     | 4.83G/9.95G [00:15<00:16, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  49%|████▉     | 4.87G/9.95G [00:15<00:16, 302MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  49%|████▉     | 4.90G/9.95G [00:15<00:16, 302MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|████▉     | 4.94G/9.95G [00:15<00:16, 309MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  51%|█████▏    | 5.11G/9.95G [00:13<00:13, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  52%|█████▏    | 5.15G/9.95G [00:13<00:13, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  52%|█████▏    | 5.19G/9.95G [00:14<00:13, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.23G/9.95G [00:14<00:14, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.27G/9.95G [00:14<00:13, 339MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.32G/9.95G [00:14<00:14, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  54%|█████▍    | 5.36G/9.95G [00:14<00:13, 339MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  54%|█████▍    | 5.40G/9.95G [00:14<00:12, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  55%|█████▍    | 5.44G/9.95G [00:14<00:12, 364MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  55%|█████▌    | 5.48G/9.95G [00:14<00:12, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  56%|█████▌    | 5.53G/9.95G [00:14<00:11, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  56%|█████▌    | 5.57G/9.95G [00:15<00:11, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  56%|█████▋    | 5.61G/9.95G [00:15<00:11, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  57%|█████▋    | 5.65G/9.95G [00:15<00:12, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  57%|█████▋    | 5.69G/9.95G [00:15<00:11, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.74G/9.95G [00:15<00:11, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.78G/9.95G [00:15<00:12, 345MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.82G/9.95G [00:15<00:11, 353MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  59%|█████▉    | 5.86G/9.95G [00:15<00:11, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  59%|█████▉    | 5.90G/9.95G [00:16<00:11, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  60%|█████▉    | 5.95G/9.95G [00:16<00:10, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  60%|██████    | 5.99G/9.95G [00:16<00:10, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████    | 6.03G/9.95G [00:16<00:10, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████    | 6.07G/9.95G [00:16<00:10, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████▏   | 6.11G/9.95G [00:16<00:10, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  62%|██████▏   | 6.16G/9.95G [00:16<00:10, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|█████     | 4.98G/9.95G [00:16<00:15, 313MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  50%|█████     | 5.01G/9.95G [00:16<00:16, 300MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  51%|█████     | 5.04G/9.95G [00:16<00:16, 301MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  51%|█████     | 5.08G/9.95G [00:16<00:16, 294MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  51%|█████▏    | 5.11G/9.95G [00:16<00:16, 296MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  52%|█████▏    | 5.15G/9.95G [00:16<00:15, 304MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  52%|█████▏    | 5.19G/9.95G [00:16<00:15, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  52%|█████▏    | 5.22G/9.95G [00:16<00:15, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.25G/9.95G [00:17<00:15, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  62%|██████▏   | 6.20G/9.95G [00:16<00:09, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  63%|██████▎   | 6.24G/9.95G [00:16<00:09, 377MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  63%|██████▎   | 6.28G/9.95G [00:17<00:09, 381MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▎   | 6.32G/9.95G [00:17<00:09, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▍   | 6.36G/9.95G [00:17<00:09, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▍   | 6.41G/9.95G [00:17<00:09, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  65%|██████▍   | 6.45G/9.95G [00:17<00:09, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  65%|██████▌   | 6.49G/9.95G [00:17<00:09, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▌   | 6.53G/9.95G [00:17<00:09, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.28G/9.95G [00:17<00:15, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  53%|█████▎    | 5.32G/9.95G [00:17<00:17, 260MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  54%|█████▍    | 5.36G/9.95G [00:17<00:16, 276MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  54%|█████▍    | 5.39G/9.95G [00:17<00:16, 283MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  55%|█████▍    | 5.43G/9.95G [00:17<00:15, 295MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  55%|█████▍    | 5.47G/9.95G [00:17<00:14, 301MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  55%|█████▌    | 5.52G/9.95G [00:17<00:14, 309MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▌   | 6.57G/9.95G [00:17<00:09, 341MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▋   | 6.62G/9.95G [00:18<00:09, 336MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  67%|██████▋   | 6.66G/9.95G [00:18<00:09, 341MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  67%|██████▋   | 6.70G/9.95G [00:18<00:09, 335MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  68%|██████▊   | 6.74G/9.95G [00:18<00:09, 334MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  68%|██████▊   | 6.78G/9.95G [00:18<00:09, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▊   | 6.83G/9.95G [00:18<00:09, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  56%|█████▌    | 5.56G/9.95G [00:18<00:13, 316MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  56%|█████▋    | 5.60G/9.95G [00:18<00:14, 307MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  57%|█████▋    | 5.63G/9.95G [00:18<00:14, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  57%|█████▋    | 5.66G/9.95G [00:18<00:14, 298MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  57%|█████▋    | 5.69G/9.95G [00:18<00:14, 288MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.73G/9.95G [00:18<00:14, 289MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.76G/9.95G [00:18<00:14, 290MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.79G/9.95G [00:18<00:14, 291MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  58%|█████▊    | 5.82G/9.95G [00:18<00:14, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▉   | 6.87G/9.95G [00:18<00:09, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▉   | 6.91G/9.95G [00:18<00:09, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|██████▉   | 6.95G/9.95G [00:19<00:09, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|███████   | 6.99G/9.95G [00:19<00:09, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  71%|███████   | 7.04G/9.95G [00:19<00:08, 336MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  71%|███████   | 7.08G/9.95G [00:19<00:08, 332MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.12G/9.95G [00:19<00:08, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.16G/9.95G [00:19<00:08, 332MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  59%|█████▉    | 5.85G/9.95G [00:19<00:14, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  59%|█████▉    | 5.88G/9.95G [00:19<00:14, 288MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  59%|█████▉    | 5.91G/9.95G [00:19<00:14, 274MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  60%|█████▉    | 5.95G/9.95G [00:19<00:15, 259MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  60%|██████    | 5.98G/9.95G [00:19<00:16, 248MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  60%|██████    | 6.01G/9.95G [00:19<00:16, 243MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████    | 6.04G/9.95G [00:19<00:15, 247MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████    | 6.07G/9.95G [00:19<00:16, 237MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.20G/9.95G [00:19<00:08, 334MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  73%|███████▎  | 7.25G/9.95G [00:19<00:08, 337MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  73%|███████▎  | 7.29G/9.95G [00:20<00:08, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▎  | 7.33G/9.95G [00:20<00:07, 334MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▍  | 7.37G/9.95G [00:20<00:07, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▍  | 7.41G/9.95G [00:20<00:07, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  75%|███████▍  | 7.46G/9.95G [00:20<00:08, 294MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  75%|███████▌  | 7.49G/9.95G [00:20<00:08, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  61%|██████▏   | 6.10G/9.95G [00:20<00:17, 225MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  62%|██████▏   | 6.13G/9.95G [00:20<00:17, 222MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  62%|██████▏   | 6.17G/9.95G [00:20<00:16, 223MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  62%|██████▏   | 6.20G/9.95G [00:20<00:16, 235MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  63%|██████▎   | 6.23G/9.95G [00:20<00:15, 244MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  63%|██████▎   | 6.26G/9.95G [00:20<00:14, 250MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  63%|██████▎   | 6.29G/9.95G [00:20<00:14, 259MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▎   | 6.32G/9.95G [00:21<00:13, 262MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▌  | 7.53G/9.95G [00:20<00:07, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▌  | 7.57G/9.95G [00:20<00:07, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▋  | 7.61G/9.95G [00:21<00:06, 339MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  77%|███████▋  | 7.65G/9.95G [00:21<00:07, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  77%|███████▋  | 7.70G/9.95G [00:21<00:06, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  78%|███████▊  | 7.74G/9.95G [00:21<00:06, 342MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  78%|███████▊  | 7.78G/9.95G [00:21<00:06, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▊  | 7.82G/9.95G [00:21<00:05, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▉  | 7.86G/9.95G [00:21<00:05, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▍   | 6.35G/9.95G [00:21<00:13, 266MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▍   | 6.39G/9.95G [00:21<00:13, 268MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  64%|██████▍   | 6.42G/9.95G [00:21<00:14, 252MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  65%|██████▍   | 6.45G/9.95G [00:21<00:16, 215MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  65%|██████▌   | 6.48G/9.95G [00:21<00:16, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  65%|██████▌   | 6.51G/9.95G [00:21<00:16, 205MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▉  | 7.91G/9.95G [00:21<00:05, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|███████▉  | 7.95G/9.95G [00:21<00:05, 389MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|████████  | 7.99G/9.95G [00:22<00:04, 393MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████  | 8.04G/9.95G [00:22<00:04, 394MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████  | 8.08G/9.95G [00:22<00:04, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.14G/9.95G [00:22<00:04, 401MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.18G/9.95G [00:22<00:04, 401MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.22G/9.95G [00:22<00:04, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.26G/9.95G [00:22<00:04, 395MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▌   | 6.54G/9.95G [00:22<00:16, 202MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▌   | 6.56G/9.95G [00:22<00:17, 199MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▋   | 6.60G/9.95G [00:22<00:16, 204MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  66%|██████▋   | 6.62G/9.95G [00:22<00:16, 204MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  67%|██████▋   | 6.64G/9.95G [00:22<00:16, 204MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  67%|██████▋   | 6.67G/9.95G [00:22<00:15, 208MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  67%|██████▋   | 6.70G/9.95G [00:22<00:15, 214MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  68%|██████▊   | 6.73G/9.95G [00:22<00:14, 225MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.30G/9.95G [00:22<00:04, 401MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▍ | 8.35G/9.95G [00:22<00:04, 398MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▍ | 8.39G/9.95G [00:23<00:04, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  85%|████████▍ | 8.43G/9.95G [00:23<00:04, 358MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  85%|████████▌ | 8.47G/9.95G [00:23<00:04, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▌ | 8.51G/9.95G [00:23<00:03, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▌ | 8.56G/9.95G [00:23<00:03, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▋ | 8.61G/9.95G [00:23<00:03, 405MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  87%|████████▋ | 8.65G/9.95G [00:23<00:03, 403MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  68%|██████▊   | 6.76G/9.95G [00:23<00:14, 222MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  68%|██████▊   | 6.79G/9.95G [00:23<00:13, 231MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▊   | 6.83G/9.95G [00:23<00:13, 239MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▉   | 6.86G/9.95G [00:23<00:13, 231MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  69%|██████▉   | 6.89G/9.95G [00:23<00:13, 225MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|██████▉   | 6.92G/9.95G [00:23<00:13, 218MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|██████▉   | 6.95G/9.95G [00:23<00:14, 214MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  87%|████████▋ | 8.69G/9.95G [00:23<00:03, 405MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  88%|████████▊ | 8.75G/9.95G [00:23<00:02, 405MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  88%|████████▊ | 8.79G/9.95G [00:24<00:02, 396MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  89%|████████▊ | 8.83G/9.95G [00:24<00:02, 391MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  89%|████████▉ | 8.88G/9.95G [00:24<00:02, 404MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|████████▉ | 8.92G/9.95G [00:24<00:02, 404MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|█████████ | 8.97G/9.95G [00:24<00:02, 409MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|█████████ | 9.01G/9.95G [00:24<00:02, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  91%|█████████ | 9.05G/9.95G [00:24<00:02, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|███████   | 6.98G/9.95G [00:24<00:14, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  70%|███████   | 7.00G/9.95G [00:24<00:14, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  71%|███████   | 7.03G/9.95G [00:24<00:14, 205MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  71%|███████   | 7.06G/9.95G [00:24<00:13, 209MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  71%|███████   | 7.09G/9.95G [00:24<00:13, 212MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.12G/9.95G [00:24<00:13, 214MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.15G/9.95G [00:24<00:13, 210MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  91%|█████████▏| 9.09G/9.95G [00:24<00:02, 384MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.13G/9.95G [00:24<00:02, 386MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.18G/9.95G [00:25<00:02, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.22G/9.95G [00:25<00:02, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.26G/9.95G [00:25<00:02, 336MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.30G/9.95G [00:25<00:01, 348MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  94%|█████████▍| 9.34G/9.95G [00:25<00:01, 335MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  94%|█████████▍| 9.40G/9.95G [00:25<00:01, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.18G/9.95G [00:25<00:13, 211MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  72%|███████▏  | 7.21G/9.95G [00:25<00:12, 214MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  73%|███████▎  | 7.25G/9.95G [00:25<00:12, 223MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  73%|███████▎  | 7.28G/9.95G [00:25<00:11, 226MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  73%|███████▎  | 7.31G/9.95G [00:25<00:11, 238MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▎  | 7.34G/9.95G [00:25<00:11, 229MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▍  | 7.37G/9.95G [00:25<00:10, 241MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  74%|███████▍  | 7.40G/9.95G [00:25<00:09, 257MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  95%|█████████▍| 9.44G/9.95G [00:25<00:01, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  95%|█████████▌| 9.48G/9.95G [00:25<00:01, 384MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  96%|█████████▌| 9.52G/9.95G [00:26<00:01, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  96%|█████████▌| 9.56G/9.95G [00:26<00:01, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  97%|█████████▋| 9.62G/9.95G [00:26<00:00, 397MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  97%|█████████▋| 9.66G/9.95G [00:26<00:00, 401MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.71G/9.95G [00:26<00:00, 416MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.75G/9.95G [00:26<00:00, 417MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.79G/9.95G [00:26<00:00, 407MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  99%|█████████▉| 9.85G/9.95G [00:26<00:00, 413MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  99%|█████████▉| 9.89G/9.95G [00:26<00:00, 397MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";: 100%|█████████▉| 9.93G/9.95G [00:27<00:00, 358MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";: 100%|██████████| 9.95G/9.95G [00:27<00:00, 366MB/s]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:49, 27.26s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:48, 27.25s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:48, 27.25s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:48, 27.25s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:48, 27.25s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:49, 27.30s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:49, 27.29s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:27<01:49, 27.29s/it]\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 0.00/9.79G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 21.0M/9.79G [00:00<01:04, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 52.4M/9.79G [00:00<00:48, 201MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 83.9M/9.79G [00:00<00:44, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 115M/9.79G [00:00<00:41, 234MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|▏         | 147M/9.79G [00:00<00:39, 244MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 178M/9.79G [00:00<00:38, 251MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 210M/9.79G [00:00<00:36, 259MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 241M/9.79G [00:00<00:36, 263MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 273M/9.79G [00:01<00:36, 264MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 304M/9.79G [00:01<00:35, 267MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 336M/9.79G [00:01<00:35, 269MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  75%|███████▍  | 7.43G/9.95G [00:26<00:09, 267MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  75%|███████▌  | 7.47G/9.95G [00:26<00:09, 251MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  75%|███████▌  | 7.50G/9.95G [00:26<00:09, 261MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▌  | 7.54G/9.95G [00:26<00:08, 278MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▌  | 7.57G/9.95G [00:26<00:09, 261MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  76%|███████▋  | 7.60G/9.95G [00:26<00:09, 238MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  77%|███████▋  | 7.63G/9.95G [00:26<00:10, 228MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  77%|███████▋  | 7.67G/9.95G [00:26<00:09, 232MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  77%|███████▋  | 7.70G/9.95G [00:27<00:10, 223MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  78%|███████▊  | 7.73G/9.95G [00:27<00:09, 224MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  78%|███████▊  | 7.76G/9.95G [00:27<00:09, 221MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  78%|███████▊  | 7.79G/9.95G [00:27<00:09, 221MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▊  | 7.82G/9.95G [00:27<00:09, 231MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▉  | 7.85G/9.95G [00:27<00:09, 217MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▉  | 7.89G/9.95G [00:28<00:10, 203MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  79%|███████▉  | 7.91G/9.95G [00:28<00:10, 195MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|███████▉  | 7.94G/9.95G [00:28<00:10, 191MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|███████▉  | 7.96G/9.95G [00:28<00:10, 195MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|████████  | 7.98G/9.95G [00:28<00:10, 193MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  80%|████████  | 8.00G/9.95G [00:28<00:10, 192MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████  | 8.03G/9.95G [00:28<00:09, 212MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████  | 8.06G/9.95G [00:29<00:10, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▎         | 367M/9.79G [00:01<00:36, 260MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▍         | 398M/9.79G [00:01<00:36, 256MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▍         | 430M/9.79G [00:01<00:37, 252MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▍         | 461M/9.79G [00:01<00:37, 252MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▌         | 493M/9.79G [00:01<00:38, 244MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▌         | 524M/9.79G [00:02<00:38, 241MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 556M/9.79G [00:02<00:38, 242MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 587M/9.79G [00:02<00:37, 242MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████  | 8.08G/9.95G [00:29<00:11, 161MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  81%|████████▏ | 8.11G/9.95G [00:29<00:12, 145MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.13G/9.95G [00:29<00:12, 148MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.15G/9.95G [00:29<00:12, 148MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.17G/9.95G [00:29<00:11, 154MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  82%|████████▏ | 8.19G/9.95G [00:29<00:11, 160MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.22G/9.95G [00:30<00:09, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▋         | 619M/9.79G [00:02<00:38, 240MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 650M/9.79G [00:02<00:37, 244MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 682M/9.79G [00:02<00:37, 245MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 713M/9.79G [00:02<00:36, 249MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 744M/9.79G [00:03<00:36, 249MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 776M/9.79G [00:03<00:36, 244MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 807M/9.79G [00:03<00:36, 245MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.24G/9.95G [00:30<00:09, 176MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.26G/9.95G [00:30<00:09, 171MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  83%|████████▎ | 8.29G/9.95G [00:30<00:08, 193MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▎ | 8.33G/9.95G [00:30<00:07, 223MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▍ | 8.36G/9.95G [00:30<00:08, 190MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▍ | 8.38G/9.95G [00:30<00:08, 175MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  84%|████████▍ | 8.40G/9.95G [00:30<00:08, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▊         | 839M/9.79G [00:03<00:39, 227MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▉         | 870M/9.79G [00:03<00:38, 233MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▉         | 902M/9.79G [00:03<00:37, 238MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|▉         | 933M/9.79G [00:03<00:36, 243MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|▉         | 965M/9.79G [00:03<00:36, 239MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|█         | 996M/9.79G [00:04<00:37, 233MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|█         | 1.03G/9.79G [00:04<00:38, 229MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.06G/9.79G [00:04<00:37, 236MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  85%|████████▍ | 8.43G/9.95G [00:31<00:07, 199MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  85%|████████▌ | 8.46G/9.95G [00:31<00:06, 223MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  85%|████████▌ | 8.49G/9.95G [00:31<00:06, 242MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▌ | 8.52G/9.95G [00:31<00:05, 245MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▌ | 8.57G/9.95G [00:31<00:05, 262MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  86%|████████▋ | 8.60G/9.95G [00:31<00:05, 267MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  87%|████████▋ | 8.63G/9.95G [00:31<00:04, 278MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  87%|████████▋ | 8.66G/9.95G [00:31<00:05, 257MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.09G/9.79G [00:04<00:37, 234MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█▏        | 1.12G/9.79G [00:04<00:36, 239MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.15G/9.79G [00:04<00:36, 238MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.18G/9.79G [00:04<00:36, 237MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.22G/9.79G [00:05<00:35, 238MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.25G/9.79G [00:05<00:35, 240MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.28G/9.79G [00:05<00:35, 242MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.31G/9.79G [00:05<00:34, 245MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  87%|████████▋ | 8.69G/9.95G [00:32<00:05, 242MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  88%|████████▊ | 8.72G/9.95G [00:32<00:05, 239MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  88%|████████▊ | 8.76G/9.95G [00:32<00:04, 250MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  88%|████████▊ | 8.80G/9.95G [00:32<00:04, 278MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  89%|████████▊ | 8.83G/9.95G [00:32<00:04, 262MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  89%|████████▉ | 8.86G/9.95G [00:32<00:04, 230MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  89%|████████▉ | 8.89G/9.95G [00:32<00:04, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▎        | 1.34G/9.79G [00:05<00:33, 250MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.37G/9.79G [00:05<00:34, 247MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.41G/9.79G [00:05<00:33, 249MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▍        | 1.44G/9.79G [00:05<00:33, 247MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▍        | 1.47G/9.79G [00:06<00:33, 251MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▌        | 1.50G/9.79G [00:06<00:31, 265MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.53G/9.79G [00:06<00:30, 274MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.56G/9.79G [00:06<00:29, 279MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|████████▉ | 8.92G/9.95G [00:33<00:04, 212MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|████████▉ | 8.95G/9.95G [00:33<00:04, 208MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  90%|█████████ | 8.99G/9.95G [00:33<00:04, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  91%|█████████ | 9.02G/9.95G [00:33<00:04, 211MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  91%|█████████ | 9.05G/9.95G [00:33<00:04, 219MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  91%|█████████ | 9.08G/9.95G [00:33<00:03, 232MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.11G/9.95G [00:33<00:03, 211MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▋        | 1.59G/9.79G [00:06<00:29, 273MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.63G/9.79G [00:06<00:30, 268MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.66G/9.79G [00:06<00:31, 260MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.69G/9.79G [00:06<00:30, 263MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.72G/9.79G [00:06<00:30, 262MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.75G/9.79G [00:07<00:30, 263MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.78G/9.79G [00:07<00:30, 264MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▊        | 1.81G/9.79G [00:07<00:30, 264MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▉        | 1.85G/9.79G [00:07<00:30, 263MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.14G/9.95G [00:34<00:03, 217MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.18G/9.95G [00:34<00:03, 212MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  92%|█████████▏| 9.21G/9.95G [00:34<00:03, 208MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.24G/9.95G [00:34<00:03, 222MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.27G/9.95G [00:34<00:02, 238MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  93%|█████████▎| 9.30G/9.95G [00:34<00:02, 237MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  94%|█████████▍| 9.33G/9.95G [00:34<00:02, 236MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▉        | 1.88G/9.79G [00:07<00:29, 266MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▉        | 1.91G/9.79G [00:07<00:30, 262MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|█▉        | 1.94G/9.79G [00:07<00:29, 265MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|██        | 1.97G/9.79G [00:07<00:29, 267MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|██        | 2.00G/9.79G [00:08<00:28, 269MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██        | 2.03G/9.79G [00:08<00:29, 263MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██        | 2.07G/9.79G [00:08<00:30, 254MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██▏       | 2.10G/9.79G [00:08<00:30, 251MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  94%|█████████▍| 9.36G/9.95G [00:35<00:02, 243MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  94%|█████████▍| 9.40G/9.95G [00:35<00:02, 217MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  95%|█████████▍| 9.43G/9.95G [00:35<00:02, 221MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  95%|█████████▌| 9.46G/9.95G [00:35<00:02, 239MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  95%|█████████▌| 9.50G/9.95G [00:35<00:01, 265MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  96%|█████████▌| 9.53G/9.95G [00:35<00:01, 234MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  96%|█████████▌| 9.56G/9.95G [00:35<00:01, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.13G/9.79G [00:08<00:30, 252MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.16G/9.79G [00:08<00:30, 250MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.19G/9.79G [00:08<00:31, 245MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.22G/9.79G [00:08<00:31, 239MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.25G/9.79G [00:09<00:31, 240MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.29G/9.79G [00:09<00:31, 241MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▎       | 2.32G/9.79G [00:09<00:30, 243MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▍       | 2.35G/9.79G [00:09<00:30, 245MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▍       | 2.38G/9.79G [00:09<00:29, 248MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▍       | 2.41G/9.79G [00:09<00:29, 250MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▍       | 2.44G/9.79G [00:09<00:29, 248MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▌       | 2.47G/9.79G [00:09<00:31, 232MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.51G/9.79G [00:10<00:32, 227MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.54G/9.79G [00:10<00:31, 231MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.57G/9.79G [00:10<00:31, 231MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.60G/9.79G [00:10<00:31, 232MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.63G/9.79G [00:10<00:31, 228MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.66G/9.79G [00:10<00:36, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.68G/9.79G [00:11<00:38, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.71G/9.79G [00:11<00:39, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.73G/9.79G [00:11<00:40, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.75G/9.79G [00:11<00:42, 165MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  96%|█████████▋| 9.59G/9.95G [00:36<00:01, 216MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  97%|█████████▋| 9.63G/9.95G [00:36<00:01, 202MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  97%|█████████▋| 9.66G/9.95G [00:36<00:01, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  97%|█████████▋| 9.69G/9.95G [00:36<00:01, 208MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.72G/9.95G [00:36<00:01, 212MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.75G/9.95G [00:36<00:00, 215MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  98%|█████████▊| 9.78G/9.95G [00:36<00:00, 233MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  99%|█████████▊| 9.81G/9.95G [00:37<00:00, 240MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  99%|█████████▉| 9.85G/9.95G [00:37<00:00, 217MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00005.bin\";:  99%|█████████▉| 9.88G/9.95G [00:37<00:00, 219MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";: 100%|█████████▉| 9.91G/9.95G [00:37<00:00, 207MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";: 100%|█████████▉| 9.94G/9.95G [00:37<00:00, 211MB/s]#033[A\n",
      "Downloading (…)00001-of-00005.bin\";: 100%|██████████| 9.95G/9.95G [00:37<00:00, 264MB/s]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.86s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.83s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.87s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.86s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.90s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.87s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.88s/it]\n",
      "Downloading shards:  20%|██        | 1/5 [00:37<02:31, 37.88s/it]\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 0.00/9.79G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 10.5M/9.79G [00:00<03:04, 53.0MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 21.0M/9.79G [00:00<02:20, 69.5MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   0%|          | 41.9M/9.79G [00:00<01:44, 92.9MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 62.9M/9.79G [00:00<01:37, 99.6MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 73.4M/9.79G [00:00<01:37, 100MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 94.4M/9.79G [00:00<01:32, 105MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.77G/9.79G [00:11<00:44, 159MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.79G/9.79G [00:11<00:45, 153MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▊       | 2.81G/9.79G [00:11<00:46, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▉       | 2.83G/9.79G [00:11<00:45, 152MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▉       | 2.85G/9.79G [00:12<00:47, 147MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▉       | 2.87G/9.79G [00:12<00:47, 145MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|          | 115M/9.79G [00:01<01:31, 106MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   1%|▏         | 136M/9.79G [00:01<01:26, 111MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 157M/9.79G [00:01<01:25, 113MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 178M/9.79G [00:01<01:23, 115MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 199M/9.79G [00:01<01:20, 119MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 220M/9.79G [00:02<01:16, 126MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|██▉       | 2.89G/9.79G [00:12<00:48, 141MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|██▉       | 2.92G/9.79G [00:12<00:48, 142MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|██▉       | 2.94G/9.79G [00:12<00:48, 141MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|███       | 2.96G/9.79G [00:12<00:48, 141MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|███       | 2.98G/9.79G [00:13<00:48, 139MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.00G/9.79G [00:13<00:48, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.02G/9.79G [00:13<00:49, 136MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   2%|▏         | 241M/9.79G [00:02<01:13, 130MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 262M/9.79G [00:02<01:12, 132MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 283M/9.79G [00:02<01:10, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 304M/9.79G [00:02<01:10, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   3%|▎         | 325M/9.79G [00:02<01:10, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▎         | 346M/9.79G [00:02<01:09, 136MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▎         | 367M/9.79G [00:03<01:08, 138MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.04G/9.79G [00:13<00:48, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███▏      | 3.06G/9.79G [00:13<00:48, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███▏      | 3.08G/9.79G [00:13<00:47, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.10G/9.79G [00:13<00:47, 141MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.12G/9.79G [00:14<00:47, 141MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.15G/9.79G [00:14<00:48, 136MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.17G/9.79G [00:14<00:50, 131MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▍         | 388M/9.79G [00:03<01:07, 139MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▍         | 409M/9.79G [00:03<01:07, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   4%|▍         | 430M/9.79G [00:03<01:07, 139MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▍         | 451M/9.79G [00:03<01:08, 137MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▍         | 472M/9.79G [00:03<01:10, 133MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▌         | 493M/9.79G [00:04<01:10, 131MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.19G/9.79G [00:14<00:50, 131MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.21G/9.79G [00:14<01:03, 103MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.23G/9.79G [00:15<01:06, 98.8MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.25G/9.79G [00:15<01:07, 96.9MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▌         | 514M/9.79G [00:04<01:16, 122MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   5%|▌         | 535M/9.79G [00:04<01:33, 98.6MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 556M/9.79G [00:04<01:40, 91.5MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 566M/9.79G [00:04<01:44, 88.3MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 577M/9.79G [00:05<01:45, 87.5MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.27G/9.79G [00:15<01:05, 99.4MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▎      | 3.28G/9.79G [00:15<01:04, 100MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▎      | 3.29G/9.79G [00:15<01:05, 98.7MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.31G/9.79G [00:15<01:01, 106MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.33G/9.79G [00:16<00:57, 111MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.36G/9.79G [00:16<00:54, 119MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.38G/9.79G [00:16<00:51, 126MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 587M/9.79G [00:05<01:49, 84.3MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 598M/9.79G [00:05<01:46, 86.6MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▌         | 608M/9.79G [00:05<01:42, 89.3MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   6%|▋         | 619M/9.79G [00:05<01:41, 90.4MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 640M/9.79G [00:05<01:31, 100MB/s] #033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 661M/9.79G [00:05<01:25, 107MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 682M/9.79G [00:06<01:19, 114MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▍      | 3.40G/9.79G [00:16<00:49, 130MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▍      | 3.42G/9.79G [00:16<00:45, 139MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▌      | 3.44G/9.79G [00:16<00:43, 145MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▌      | 3.46G/9.79G [00:16<00:41, 152MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.48G/9.79G [00:17<00:38, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.50G/9.79G [00:17<00:37, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.52G/9.79G [00:17<00:36, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.54G/9.79G [00:17<00:35, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 703M/9.79G [00:06<01:12, 125MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   7%|▋         | 724M/9.79G [00:06<01:10, 128MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 744M/9.79G [00:06<01:06, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 765M/9.79G [00:06<01:02, 145MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 786M/9.79G [00:06<01:00, 149MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 807M/9.79G [00:06<00:57, 155MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   8%|▊         | 828M/9.79G [00:06<00:55, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▊         | 849M/9.79G [00:07<00:52, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▋      | 3.57G/9.79G [00:17<00:34, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.59G/9.79G [00:17<00:33, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.61G/9.79G [00:17<00:34, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.63G/9.79G [00:17<00:33, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.65G/9.79G [00:17<00:33, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.67G/9.79G [00:18<00:32, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.69G/9.79G [00:18<00:32, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.71G/9.79G [00:18<00:32, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.73G/9.79G [00:18<00:33, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▉         | 870M/9.79G [00:07<00:51, 173MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▉         | 891M/9.79G [00:07<00:52, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:   9%|▉         | 912M/9.79G [00:07<00:51, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|▉         | 933M/9.79G [00:07<00:48, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|▉         | 954M/9.79G [00:07<00:47, 187MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|▉         | 975M/9.79G [00:07<00:49, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|█         | 996M/9.79G [00:07<00:49, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  10%|█         | 1.02G/9.79G [00:08<00:50, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.04G/9.79G [00:08<00:49, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.75G/9.79G [00:18<00:33, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▊      | 3.77G/9.79G [00:18<00:33, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.80G/9.79G [00:18<00:34, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.82G/9.79G [00:18<00:33, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.84G/9.79G [00:19<00:32, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.86G/9.79G [00:19<00:31, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|███▉      | 3.88G/9.79G [00:19<00:31, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|███▉      | 3.90G/9.79G [00:19<00:32, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.06G/9.79G [00:08<00:50, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.08G/9.79G [00:08<00:49, 175MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█         | 1.10G/9.79G [00:08<00:49, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  11%|█▏        | 1.12G/9.79G [00:08<00:49, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.14G/9.79G [00:08<00:49, 176MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.16G/9.79G [00:08<00:50, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.18G/9.79G [00:08<00:50, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  12%|█▏        | 1.21G/9.79G [00:09<00:49, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|████      | 3.92G/9.79G [00:19<00:33, 176MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|████      | 3.94G/9.79G [00:19<00:32, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|████      | 3.96G/9.79G [00:19<00:32, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 3.98G/9.79G [00:19<00:32, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 4.01G/9.79G [00:19<00:31, 181MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 4.03G/9.79G [00:20<00:34, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████▏     | 4.05G/9.79G [00:20<00:35, 164MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.07G/9.79G [00:20<00:36, 157MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.23G/9.79G [00:09<00:48, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.25G/9.79G [00:09<00:48, 175MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.27G/9.79G [00:09<00:48, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.29G/9.79G [00:09<00:51, 164MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  13%|█▎        | 1.31G/9.79G [00:09<00:54, 156MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▎        | 1.33G/9.79G [00:09<00:55, 153MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.35G/9.79G [00:10<00:56, 149MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.09G/9.79G [00:20<00:36, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.11G/9.79G [00:20<00:37, 152MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.13G/9.79G [00:20<00:40, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.15G/9.79G [00:20<00:40, 139MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.17G/9.79G [00:21<00:40, 138MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.19G/9.79G [00:21<00:41, 136MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.22G/9.79G [00:21<00:40, 137MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.37G/9.79G [00:10<00:56, 150MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.39G/9.79G [00:10<00:56, 149MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  14%|█▍        | 1.42G/9.79G [00:10<00:54, 154MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▍        | 1.44G/9.79G [00:10<00:51, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▍        | 1.46G/9.79G [00:10<00:51, 163MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▌        | 1.48G/9.79G [00:10<00:51, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  15%|█▌        | 1.50G/9.79G [00:10<00:49, 167MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.52G/9.79G [00:11<00:50, 164MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.54G/9.79G [00:11<00:48, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.56G/9.79G [00:11<00:51, 158MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▌        | 1.58G/9.79G [00:11<00:52, 157MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  16%|█▋        | 1.60G/9.79G [00:11<00:52, 155MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.63G/9.79G [00:11<00:53, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.65G/9.79G [00:11<00:52, 154MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.67G/9.79G [00:12<00:52, 155MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.69G/9.79G [00:12<00:53, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  17%|█▋        | 1.71G/9.79G [00:12<00:52, 153MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.73G/9.79G [00:12<00:51, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.75G/9.79G [00:12<00:50, 159MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.77G/9.79G [00:12<00:47, 167MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  18%|█▊        | 1.80G/9.79G [00:12<00:42, 187MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▊        | 1.84G/9.79G [00:12<00:40, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▉        | 1.86G/9.79G [00:13<00:40, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.24G/9.79G [00:21<00:41, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.26G/9.79G [00:21<00:41, 132MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▎     | 4.28G/9.79G [00:21<00:42, 129MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▍     | 4.30G/9.79G [00:22<00:43, 126MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▍     | 4.32G/9.79G [00:22<00:43, 127MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▍     | 4.34G/9.79G [00:22<00:42, 128MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▍     | 4.36G/9.79G [00:22<00:42, 127MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▍     | 4.38G/9.79G [00:22<00:42, 128MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▍     | 4.40G/9.79G [00:22<00:41, 131MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▌     | 4.42G/9.79G [00:23<00:40, 133MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▌     | 4.45G/9.79G [00:23<00:37, 144MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.47G/9.79G [00:23<00:34, 153MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.49G/9.79G [00:23<00:32, 164MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.51G/9.79G [00:23<00:30, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▋     | 4.53G/9.79G [00:23<00:29, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▋     | 4.55G/9.79G [00:23<00:28, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.57G/9.79G [00:23<00:28, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.59G/9.79G [00:23<00:27, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.61G/9.79G [00:24<00:27, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.63G/9.79G [00:24<00:27, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.66G/9.79G [00:24<00:28, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.68G/9.79G [00:24<00:28, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  19%|█▉        | 1.89G/9.79G [00:13<00:38, 207MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|█▉        | 1.92G/9.79G [00:13<00:37, 210MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|█▉        | 1.94G/9.79G [00:13<00:37, 207MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|██        | 1.96G/9.79G [00:13<00:38, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|██        | 1.98G/9.79G [00:13<00:38, 201MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  20%|██        | 2.00G/9.79G [00:13<00:39, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██        | 2.02G/9.79G [00:13<00:39, 199MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██        | 2.04G/9.79G [00:13<00:40, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██        | 2.08G/9.79G [00:14<00:37, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.70G/9.79G [00:24<00:30, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.72G/9.79G [00:24<00:31, 163MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.74G/9.79G [00:24<00:32, 157MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▊     | 4.76G/9.79G [00:24<00:31, 161MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.78G/9.79G [00:25<00:31, 161MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.80G/9.79G [00:25<00:30, 165MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.82G/9.79G [00:25<00:30, 163MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  21%|██▏       | 2.10G/9.79G [00:14<00:37, 204MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.13G/9.79G [00:14<00:36, 211MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.16G/9.79G [00:14<00:35, 217MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  22%|██▏       | 2.19G/9.79G [00:14<00:34, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.22G/9.79G [00:14<00:33, 223MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.25G/9.79G [00:14<00:33, 226MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  23%|██▎       | 2.29G/9.79G [00:15<00:33, 224MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.84G/9.79G [00:25<00:29, 165MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|████▉     | 4.87G/9.79G [00:25<00:29, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|████▉     | 4.89G/9.79G [00:25<00:29, 167MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|█████     | 4.91G/9.79G [00:25<00:29, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|█████     | 4.93G/9.79G [00:25<00:28, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 4.95G/9.79G [00:26<00:28, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 4.97G/9.79G [00:26<00:28, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 4.99G/9.79G [00:26<00:27, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 5.01G/9.79G [00:26<00:26, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▎       | 2.32G/9.79G [00:15<00:33, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▍       | 2.35G/9.79G [00:15<00:32, 226MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  24%|██▍       | 2.38G/9.79G [00:15<00:32, 225MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▍       | 2.41G/9.79G [00:15<00:33, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▍       | 2.44G/9.79G [00:15<00:32, 223MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  25%|██▌       | 2.47G/9.79G [00:15<00:33, 221MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.51G/9.79G [00:16<00:32, 225MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████▏    | 5.03G/9.79G [00:26<00:26, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.05G/9.79G [00:26<00:25, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.08G/9.79G [00:26<00:26, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.10G/9.79G [00:26<00:25, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.12G/9.79G [00:27<00:25, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.14G/9.79G [00:27<00:25, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.17G/9.79G [00:27<00:24, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.19G/9.79G [00:27<00:26, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.54G/9.79G [00:16<00:33, 219MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  26%|██▌       | 2.57G/9.79G [00:16<00:31, 226MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.60G/9.79G [00:16<00:32, 223MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.63G/9.79G [00:16<00:34, 205MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.65G/9.79G [00:16<00:34, 206MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  27%|██▋       | 2.68G/9.79G [00:16<00:34, 206MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.72G/9.79G [00:17<00:33, 214MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.21G/9.79G [00:27<00:26, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.23G/9.79G [00:27<00:25, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▎    | 5.25G/9.79G [00:27<00:25, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▍    | 5.27G/9.79G [00:27<00:24, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▍    | 5.30G/9.79G [00:28<00:25, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▍    | 5.32G/9.79G [00:28<00:24, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▍    | 5.34G/9.79G [00:28<00:23, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▍    | 5.36G/9.79G [00:28<00:23, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▍    | 5.38G/9.79G [00:28<00:23, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.75G/9.79G [00:17<00:32, 216MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  28%|██▊       | 2.78G/9.79G [00:17<00:31, 220MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▊       | 2.81G/9.79G [00:17<00:32, 214MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▉       | 2.84G/9.79G [00:17<00:31, 221MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  29%|██▉       | 2.87G/9.79G [00:17<00:31, 219MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|██▉       | 2.90G/9.79G [00:17<00:31, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|██▉       | 2.94G/9.79G [00:18<00:32, 212MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▌    | 5.40G/9.79G [00:28<00:23, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▌    | 5.42G/9.79G [00:28<00:25, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.44G/9.79G [00:28<00:25, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.46G/9.79G [00:28<00:24, 175MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.48G/9.79G [00:29<00:23, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.51G/9.79G [00:29<00:23, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▋    | 5.53G/9.79G [00:29<00:23, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.55G/9.79G [00:29<00:23, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  30%|███       | 2.97G/9.79G [00:18<00:34, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 2.99G/9.79G [00:18<00:35, 193MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.01G/9.79G [00:18<00:35, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.03G/9.79G [00:18<00:35, 192MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███       | 3.05G/9.79G [00:18<00:35, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  31%|███▏      | 3.07G/9.79G [00:18<00:36, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.09G/9.79G [00:18<00:36, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.11G/9.79G [00:19<00:36, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.57G/9.79G [00:29<00:23, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.59G/9.79G [00:29<00:24, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.61G/9.79G [00:29<00:25, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.63G/9.79G [00:29<00:26, 159MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.65G/9.79G [00:30<00:26, 158MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.67G/9.79G [00:30<00:27, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.69G/9.79G [00:30<00:25, 158MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.14G/9.79G [00:19<00:36, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.16G/9.79G [00:19<00:37, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  32%|███▏      | 3.18G/9.79G [00:19<00:38, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.20G/9.79G [00:19<00:39, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.22G/9.79G [00:19<00:40, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.24G/9.79G [00:19<00:38, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  33%|███▎      | 3.26G/9.79G [00:19<00:38, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▎      | 3.28G/9.79G [00:20<00:38, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.71G/9.79G [00:30<00:26, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▊    | 5.74G/9.79G [00:30<00:27, 150MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.76G/9.79G [00:30<00:26, 150MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.78G/9.79G [00:30<00:26, 152MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.80G/9.79G [00:31<00:25, 157MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.82G/9.79G [00:31<00:24, 163MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|█████▉    | 5.84G/9.79G [00:31<00:23, 165MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|█████▉    | 5.86G/9.79G [00:31<00:23, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▎      | 3.30G/9.79G [00:20<00:39, 164MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.32G/9.79G [00:20<00:39, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.34G/9.79G [00:20<00:40, 160MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  34%|███▍      | 3.37G/9.79G [00:20<00:39, 165MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▍      | 3.39G/9.79G [00:20<00:37, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▍      | 3.41G/9.79G [00:20<00:37, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▌      | 3.43G/9.79G [00:20<00:37, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▌      | 3.45G/9.79G [00:21<00:36, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  35%|███▌      | 3.47G/9.79G [00:21<00:35, 177MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.49G/9.79G [00:21<00:34, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.51G/9.79G [00:21<00:34, 183MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▌      | 3.53G/9.79G [00:21<00:33, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  36%|███▋      | 3.55G/9.79G [00:21<00:34, 179MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.58G/9.79G [00:21<00:34, 182MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.60G/9.79G [00:21<00:34, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.62G/9.79G [00:21<00:33, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.64G/9.79G [00:22<00:33, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  37%|███▋      | 3.66G/9.79G [00:22<00:32, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.68G/9.79G [00:22<00:32, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.70G/9.79G [00:22<00:31, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.72G/9.79G [00:22<00:31, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.74G/9.79G [00:22<00:31, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  38%|███▊      | 3.76G/9.79G [00:22<00:30, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▊      | 3.79G/9.79G [00:22<00:30, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.81G/9.79G [00:22<00:30, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.83G/9.79G [00:23<00:30, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  39%|███▉      | 3.85G/9.79G [00:23<00:30, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|██████    | 5.88G/9.79G [00:31<00:22, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|██████    | 5.90G/9.79G [00:31<00:22, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.92G/9.79G [00:31<00:21, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.95G/9.79G [00:31<00:21, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.97G/9.79G [00:31<00:20, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.99G/9.79G [00:32<00:20, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████▏   | 6.01G/9.79G [00:32<00:20, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.03G/9.79G [00:32<00:20, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.05G/9.79G [00:32<00:20, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.07G/9.79G [00:32<00:20, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.09G/9.79G [00:32<00:19, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.11G/9.79G [00:32<00:19, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.13G/9.79G [00:32<00:19, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.16G/9.79G [00:32<00:18, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.18G/9.79G [00:33<00:18, 193MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.20G/9.79G [00:33<00:18, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▎   | 6.22G/9.79G [00:33<00:17, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▎   | 6.24G/9.79G [00:33<00:18, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▍   | 6.26G/9.79G [00:33<00:18, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▍   | 6.28G/9.79G [00:33<00:18, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▍   | 6.30G/9.79G [00:33<00:18, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▍   | 6.32G/9.79G [00:33<00:17, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▍   | 6.34G/9.79G [00:33<00:17, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▌   | 6.38G/9.79G [00:34<00:17, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▌   | 6.40G/9.79G [00:34<00:16, 202MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.43G/9.79G [00:34<00:16, 205MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.45G/9.79G [00:34<00:16, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|███▉      | 3.87G/9.79G [00:23<00:30, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|███▉      | 3.89G/9.79G [00:23<00:30, 193MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|███▉      | 3.91G/9.79G [00:23<00:29, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|████      | 3.94G/9.79G [00:23<00:28, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  40%|████      | 3.96G/9.79G [00:23<00:28, 204MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 4.00G/9.79G [00:23<00:28, 202MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 4.02G/9.79G [00:24<00:29, 193MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████      | 4.04G/9.79G [00:24<00:30, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.47G/9.79G [00:34<00:17, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▋   | 6.49G/9.79G [00:34<00:17, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.51G/9.79G [00:34<00:17, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.53G/9.79G [00:34<00:17, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.55G/9.79G [00:35<00:16, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.57G/9.79G [00:35<00:16, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.60G/9.79G [00:35<00:19, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.62G/9.79G [00:35<00:19, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  41%|████▏     | 4.06G/9.79G [00:24<00:30, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.08G/9.79G [00:24<00:30, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.10G/9.79G [00:24<00:29, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.12G/9.79G [00:24<00:29, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  42%|████▏     | 4.14G/9.79G [00:24<00:29, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.17G/9.79G [00:24<00:28, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.20G/9.79G [00:24<00:26, 209MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  43%|████▎     | 4.24G/9.79G [00:25<00:25, 219MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.64G/9.79G [00:35<00:19, 165MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.66G/9.79G [00:35<00:18, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.68G/9.79G [00:35<00:18, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.70G/9.79G [00:35<00:18, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▊   | 6.72G/9.79G [00:36<00:17, 175MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.74G/9.79G [00:36<00:17, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.76G/9.79G [00:36<00:16, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.78G/9.79G [00:36<00:16, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▎     | 4.27G/9.79G [00:25<00:24, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▍     | 4.30G/9.79G [00:25<00:25, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  44%|████▍     | 4.33G/9.79G [00:25<00:24, 225MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▍     | 4.36G/9.79G [00:25<00:24, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▍     | 4.39G/9.79G [00:25<00:25, 214MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  45%|████▌     | 4.42G/9.79G [00:25<00:25, 212MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.46G/9.79G [00:26<00:25, 212MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|██████▉   | 6.81G/9.79G [00:36<00:15, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|██████▉   | 6.83G/9.79G [00:36<00:15, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|██████▉   | 6.85G/9.79G [00:36<00:15, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|███████   | 6.87G/9.79G [00:36<00:15, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|███████   | 6.89G/9.79G [00:36<00:14, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.91G/9.79G [00:36<00:14, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.93G/9.79G [00:37<00:14, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.95G/9.79G [00:37<00:14, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.97G/9.79G [00:37<00:14, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████▏  | 6.99G/9.79G [00:37<00:14, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.49G/9.79G [00:26<00:25, 204MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▌     | 4.51G/9.79G [00:26<00:25, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▋     | 4.53G/9.79G [00:26<00:26, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  46%|████▋     | 4.55G/9.79G [00:26<00:26, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.57G/9.79G [00:26<00:26, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.59G/9.79G [00:26<00:27, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.61G/9.79G [00:26<00:26, 193MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  47%|████▋     | 4.63G/9.79G [00:27<00:26, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.01G/9.79G [00:37<00:14, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.04G/9.79G [00:37<00:13, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.07G/9.79G [00:37<00:13, 202MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.10G/9.79G [00:37<00:14, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.12G/9.79G [00:38<00:14, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.14G/9.79G [00:38<00:14, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.16G/9.79G [00:38<00:15, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.67G/9.79G [00:27<00:25, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.70G/9.79G [00:27<00:24, 205MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  48%|████▊     | 4.72G/9.79G [00:27<00:24, 204MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▊     | 4.75G/9.79G [00:27<00:23, 218MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.78G/9.79G [00:27<00:23, 211MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.81G/9.79G [00:27<00:24, 206MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  49%|████▉     | 4.84G/9.79G [00:28<00:23, 210MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.18G/9.79G [00:38<00:15, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▎  | 7.20G/9.79G [00:38<00:15, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.22G/9.79G [00:38<00:15, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.25G/9.79G [00:38<00:14, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.27G/9.79G [00:38<00:14, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.29G/9.79G [00:39<00:14, 175MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▍  | 7.31G/9.79G [00:39<00:14, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▍  | 7.33G/9.79G [00:39<00:13, 178MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▌  | 7.35G/9.79G [00:39<00:13, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|████▉     | 4.88G/9.79G [00:28<00:23, 209MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|█████     | 4.91G/9.79G [00:28<00:22, 213MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  50%|█████     | 4.94G/9.79G [00:28<00:22, 215MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 4.97G/9.79G [00:28<00:22, 216MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████     | 5.00G/9.79G [00:28<00:22, 214MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  51%|█████▏    | 5.03G/9.79G [00:28<00:21, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.06G/9.79G [00:29<00:21, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▌  | 7.37G/9.79G [00:39<00:13, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.39G/9.79G [00:39<00:12, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.41G/9.79G [00:39<00:12, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.43G/9.79G [00:39<00:12, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.46G/9.79G [00:39<00:12, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▋  | 7.48G/9.79G [00:40<00:12, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.50G/9.79G [00:40<00:13, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.52G/9.79G [00:40<00:13, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.10G/9.79G [00:29<00:21, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  52%|█████▏    | 5.13G/9.79G [00:29<00:21, 218MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.16G/9.79G [00:29<00:20, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.19G/9.79G [00:29<00:20, 222MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  53%|█████▎    | 5.22G/9.79G [00:29<00:20, 228MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▎    | 5.25G/9.79G [00:29<00:19, 230MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▍    | 5.28G/9.79G [00:29<00:18, 238MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  54%|█████▍    | 5.32G/9.79G [00:30<00:17, 250MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.54G/9.79G [00:40<00:14, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.56G/9.79G [00:40<00:14, 152MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.58G/9.79G [00:40<00:16, 138MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.60G/9.79G [00:41<00:16, 131MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.62G/9.79G [00:41<00:18, 118MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.64G/9.79G [00:41<00:19, 113MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▍    | 5.35G/9.79G [00:30<00:18, 241MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▍    | 5.38G/9.79G [00:30<00:19, 232MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  55%|█████▌    | 5.41G/9.79G [00:30<00:19, 226MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.44G/9.79G [00:30<00:20, 211MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.47G/9.79G [00:30<00:20, 207MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▌    | 5.49G/9.79G [00:30<00:20, 206MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  56%|█████▋    | 5.52G/9.79G [00:31<00:21, 203MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.67G/9.79G [00:41<00:18, 112MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▊  | 7.69G/9.79G [00:41<00:18, 113MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▊  | 7.71G/9.79G [00:41<00:17, 117MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▉  | 7.73G/9.79G [00:42<00:17, 115MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▉  | 7.75G/9.79G [00:42<00:17, 115MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.54G/9.79G [00:31<00:21, 201MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.56G/9.79G [00:31<00:21, 200MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.59G/9.79G [00:31<00:20, 201MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  57%|█████▋    | 5.61G/9.79G [00:31<00:20, 201MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.63G/9.79G [00:31<00:20, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.65G/9.79G [00:31<00:20, 199MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.67G/9.79G [00:31<00:21, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.69G/9.79G [00:31<00:21, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  58%|█████▊    | 5.73G/9.79G [00:32<00:20, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▉  | 7.77G/9.79G [00:42<00:19, 104MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|███████▉  | 7.79G/9.79G [00:42<00:18, 108MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|███████▉  | 7.81G/9.79G [00:42<00:17, 115MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|████████  | 7.83G/9.79G [00:43<00:16, 117MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|████████  | 7.85G/9.79G [00:43<00:16, 121MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|████████  | 7.87G/9.79G [00:43<00:15, 122MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▊    | 5.75G/9.79G [00:32<00:21, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.77G/9.79G [00:32<00:21, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.79G/9.79G [00:32<00:20, 192MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  59%|█████▉    | 5.81G/9.79G [00:32<00:20, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|█████▉    | 5.83G/9.79G [00:32<00:20, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|█████▉    | 5.85G/9.79G [00:32<00:20, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|█████▉    | 5.87G/9.79G [00:32<00:20, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|██████    | 5.89G/9.79G [00:33<00:20, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  60%|██████    | 5.91G/9.79G [00:33<00:20, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.90G/9.79G [00:43<00:15, 124MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.92G/9.79G [00:43<00:13, 135MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.94G/9.79G [00:43<00:12, 146MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████▏ | 7.96G/9.79G [00:43<00:11, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 7.98G/9.79G [00:44<00:11, 161MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.00G/9.79G [00:44<00:10, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.02G/9.79G [00:44<00:10, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.04G/9.79G [00:44<00:10, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.06G/9.79G [00:44<00:11, 157MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.08G/9.79G [00:44<00:10, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.11G/9.79G [00:44<00:10, 159MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.13G/9.79G [00:44<00:10, 155MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.15G/9.79G [00:45<00:10, 151MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.17G/9.79G [00:45<00:11, 145MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▎ | 8.19G/9.79G [00:45<00:11, 135MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.21G/9.79G [00:45<00:11, 132MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.23G/9.79G [00:45<00:11, 133MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.25G/9.79G [00:45<00:11, 134MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▍ | 8.27G/9.79G [00:46<00:10, 140MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▍ | 8.29G/9.79G [00:46<00:10, 148MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▍ | 8.32G/9.79G [00:46<00:09, 157MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▌ | 8.34G/9.79G [00:46<00:08, 162MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.95G/9.79G [00:33<00:18, 209MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████    | 5.98G/9.79G [00:33<00:17, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  61%|██████▏   | 6.01G/9.79G [00:33<00:16, 224MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.04G/9.79G [00:33<00:16, 230MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.07G/9.79G [00:33<00:16, 232MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  62%|██████▏   | 6.10G/9.79G [00:33<00:16, 230MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.13G/9.79G [00:34<00:15, 235MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.17G/9.79G [00:34<00:16, 218MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  63%|██████▎   | 6.20G/9.79G [00:34<00:16, 215MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▎   | 6.23G/9.79G [00:34<00:16, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▍   | 6.26G/9.79G [00:34<00:16, 218MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  64%|██████▍   | 6.29G/9.79G [00:34<00:15, 226MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▍   | 6.32G/9.79G [00:34<00:16, 210MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▍   | 6.35G/9.79G [00:35<00:17, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▌   | 6.38G/9.79G [00:35<00:18, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  65%|██████▌   | 6.40G/9.79G [00:35<00:18, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.42G/9.79G [00:35<00:18, 182MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.44G/9.79G [00:35<00:17, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.46G/9.79G [00:35<00:17, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▌   | 6.48G/9.79G [00:35<00:17, 190MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  66%|██████▋   | 6.50G/9.79G [00:35<00:17, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.52G/9.79G [00:36<00:16, 194MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▌ | 8.36G/9.79G [00:46<00:08, 167MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.38G/9.79G [00:46<00:08, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.40G/9.79G [00:46<00:07, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.42G/9.79G [00:46<00:07, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.44G/9.79G [00:46<00:07, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▋ | 8.46G/9.79G [00:47<00:07, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.48G/9.79G [00:47<00:06, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.50G/9.79G [00:47<00:06, 192MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.52G/9.79G [00:47<00:06, 195MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.55G/9.79G [00:36<00:16, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.57G/9.79G [00:36<00:16, 199MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  67%|██████▋   | 6.60G/9.79G [00:36<00:15, 200MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.62G/9.79G [00:36<00:15, 201MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.64G/9.79G [00:36<00:15, 197MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.66G/9.79G [00:36<00:15, 198MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.68G/9.79G [00:36<00:15, 199MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  68%|██████▊   | 6.70G/9.79G [00:36<00:15, 197MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▊   | 6.72G/9.79G [00:37<00:15, 192MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.55G/9.79G [00:47<00:06, 191MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.57G/9.79G [00:47<00:06, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.59G/9.79G [00:47<00:06, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.61G/9.79G [00:47<00:06, 186MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.63G/9.79G [00:47<00:06, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.65G/9.79G [00:48<00:06, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▊ | 8.67G/9.79G [00:48<00:06, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.69G/9.79G [00:48<00:06, 168MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.74G/9.79G [00:37<00:16, 189MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.76G/9.79G [00:37<00:16, 188MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  69%|██████▉   | 6.78G/9.79G [00:37<00:16, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|██████▉   | 6.81G/9.79G [00:37<00:16, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|██████▉   | 6.84G/9.79G [00:37<00:15, 196MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|███████   | 6.87G/9.79G [00:37<00:14, 205MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  70%|███████   | 6.89G/9.79G [00:37<00:14, 205MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.92G/9.79G [00:38<00:13, 211MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.71G/9.79G [00:48<00:06, 172MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.73G/9.79G [00:48<00:06, 173MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.76G/9.79G [00:48<00:05, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|████████▉ | 8.78G/9.79G [00:48<00:05, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|████████▉ | 8.80G/9.79G [00:48<00:05, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|█████████ | 8.82G/9.79G [00:49<00:05, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|█████████ | 8.84G/9.79G [00:49<00:05, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.86G/9.79G [00:49<00:05, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.88G/9.79G [00:49<00:04, 184MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████   | 6.95G/9.79G [00:38<00:13, 212MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  71%|███████▏  | 6.98G/9.79G [00:38<00:12, 217MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.01G/9.79G [00:38<00:12, 217MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.05G/9.79G [00:38<00:12, 215MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  72%|███████▏  | 7.08G/9.79G [00:38<00:12, 214MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.11G/9.79G [00:38<00:12, 219MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.14G/9.79G [00:39<00:12, 216MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.90G/9.79G [00:49<00:04, 183MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.92G/9.79G [00:49<00:04, 185MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████▏| 8.94G/9.79G [00:49<00:04, 187MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 8.97G/9.79G [00:49<00:04, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 8.99G/9.79G [00:50<00:04, 175MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 9.01G/9.79G [00:50<00:04, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 9.03G/9.79G [00:50<00:04, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 9.05G/9.79G [00:50<00:04, 179MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  73%|███████▎  | 7.17G/9.79G [00:39<00:11, 221MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▎  | 7.20G/9.79G [00:39<00:12, 211MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.24G/9.79G [00:39<00:12, 209MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.27G/9.79G [00:39<00:12, 204MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  74%|███████▍  | 7.29G/9.79G [00:39<00:12, 202MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▍  | 7.31G/9.79G [00:39<00:12, 201MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▍  | 7.33G/9.79G [00:40<00:12, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▌  | 7.35G/9.79G [00:40<00:11, 203MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.07G/9.79G [00:50<00:03, 180MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.09G/9.79G [00:50<00:03, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.11G/9.79G [00:50<00:03, 181MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.13G/9.79G [00:50<00:03, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▎| 9.15G/9.79G [00:50<00:03, 173MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▎| 9.18G/9.79G [00:51<00:03, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▍| 9.20G/9.79G [00:51<00:03, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▍| 9.22G/9.79G [00:51<00:03, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  75%|███████▌  | 7.38G/9.79G [00:40<00:11, 205MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.41G/9.79G [00:40<00:11, 216MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▌  | 7.44G/9.79G [00:40<00:10, 227MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  76%|███████▋  | 7.48G/9.79G [00:40<00:10, 229MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.51G/9.79G [00:40<00:09, 233MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.54G/9.79G [00:40<00:09, 237MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  77%|███████▋  | 7.57G/9.79G [00:41<00:09, 241MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▍| 9.24G/9.79G [00:51<00:03, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▍| 9.26G/9.79G [00:51<00:03, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▍| 9.28G/9.79G [00:51<00:03, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▌| 9.30G/9.79G [00:51<00:02, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▌| 9.32G/9.79G [00:51<00:02, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▌| 9.34G/9.79G [00:52<00:02, 158MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▌| 9.36G/9.79G [00:52<00:02, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▌| 9.38G/9.79G [00:52<00:02, 153MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.60G/9.79G [00:41<00:09, 236MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.63G/9.79G [00:41<00:09, 234MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  78%|███████▊  | 7.67G/9.79G [00:41<00:09, 231MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▊  | 7.70G/9.79G [00:41<00:09, 225MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▉  | 7.73G/9.79G [00:41<00:09, 223MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  79%|███████▉  | 7.76G/9.79G [00:41<00:09, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|███████▉  | 7.79G/9.79G [00:42<00:08, 223MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|███████▉  | 7.82G/9.79G [00:42<00:08, 226MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▌| 9.41G/9.79G [00:52<00:02, 156MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▋| 9.43G/9.79G [00:52<00:02, 160MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.45G/9.79G [00:52<00:02, 159MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.47G/9.79G [00:52<00:02, 158MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.49G/9.79G [00:53<00:01, 163MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.51G/9.79G [00:53<00:01, 166MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.53G/9.79G [00:53<00:01, 169MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.55G/9.79G [00:53<00:01, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  80%|████████  | 7.85G/9.79G [00:42<00:08, 224MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.89G/9.79G [00:42<00:08, 227MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.92G/9.79G [00:42<00:08, 228MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  81%|████████  | 7.95G/9.79G [00:42<00:07, 233MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 7.98G/9.79G [00:42<00:07, 234MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.01G/9.79G [00:42<00:07, 233MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.04G/9.79G [00:43<00:07, 232MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.57G/9.79G [00:53<00:01, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.59G/9.79G [00:53<00:01, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.62G/9.79G [00:53<00:00, 175MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.64G/9.79G [00:53<00:00, 176MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▊| 9.66G/9.79G [00:54<00:00, 171MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▉| 9.68G/9.79G [00:54<00:00, 173MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▉| 9.70G/9.79G [00:54<00:00, 174MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▉| 9.72G/9.79G [00:54<00:00, 177MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  82%|████████▏ | 8.07G/9.79G [00:43<00:07, 233MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.11G/9.79G [00:43<00:07, 230MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.14G/9.79G [00:43<00:07, 225MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  83%|████████▎ | 8.17G/9.79G [00:43<00:07, 223MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.20G/9.79G [00:43<00:07, 227MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.23G/9.79G [00:43<00:07, 219MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  84%|████████▍ | 8.26G/9.79G [00:44<00:06, 220MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|█████████▉| 9.74G/9.79G [00:54<00:00, 172MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|█████████▉| 9.76G/9.79G [00:54<00:00, 170MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|█████████▉| 9.78G/9.79G [00:54<00:00, 168MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|██████████| 9.79G/9.79G [00:54<00:00, 179MB/s]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.54s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.53s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.54s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.54s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.54s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.55s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.55s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:22<02:10, 43.56s/it]\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 0.00/9.71G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 10.5M/9.71G [00:00<01:44, 93.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 21.0M/9.71G [00:00<01:51, 87.1MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▍ | 8.29G/9.79G [00:44<00:06, 218MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  85%|████████▌ | 8.34G/9.79G [00:44<00:05, 261MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.38G/9.79G [00:44<00:04, 294MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▌ | 8.42G/9.79G [00:44<00:04, 312MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  86%|████████▋ | 8.46G/9.79G [00:44<00:03, 332MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.50G/9.79G [00:44<00:03, 342MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  87%|████████▋ | 8.55G/9.79G [00:44<00:03, 359MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.59G/9.79G [00:45<00:03, 370MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  88%|████████▊ | 8.63G/9.79G [00:45<00:03, 372MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 31.5M/9.71G [00:00<02:34, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 41.9M/9.71G [00:00<03:35, 44.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 52.4M/9.71G [00:00<03:07, 51.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 73.4M/9.71G [00:01<03:02, 52.9MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▊ | 8.67G/9.79G [00:45<00:02, 378MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.71G/9.79G [00:45<00:02, 375MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  89%|████████▉ | 8.76G/9.79G [00:45<00:02, 379MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|████████▉ | 8.80G/9.79G [00:45<00:02, 349MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  90%|█████████ | 8.84G/9.79G [00:45<00:02, 365MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.88G/9.79G [00:45<00:02, 373MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  91%|█████████ | 8.92G/9.79G [00:45<00:02, 371MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 8.97G/9.79G [00:46<00:02, 381MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 9.01G/9.79G [00:46<00:02, 383MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  92%|█████████▏| 9.05G/9.79G [00:46<00:01, 382MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.10G/9.79G [00:46<00:01, 394MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  93%|█████████▎| 9.14G/9.79G [00:46<00:01, 333MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▍| 9.19G/9.79G [00:46<00:01, 332MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  94%|█████████▍| 9.23G/9.79G [00:46<00:01, 336MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▍| 9.27G/9.79G [00:46<00:01, 353MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  95%|█████████▌| 9.31G/9.79G [00:47<00:01, 357MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▌| 9.35G/9.79G [00:47<00:01, 356MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▌| 9.40G/9.79G [00:47<00:01, 347MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  96%|█████████▋| 9.44G/9.79G [00:47<00:01, 335MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.48G/9.79G [00:47<00:00, 344MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  97%|█████████▋| 9.52G/9.79G [00:47<00:00, 342MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.56G/9.79G [00:47<00:00, 352MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  98%|█████████▊| 9.60G/9.79G [00:47<00:00, 350MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▊| 9.65G/9.79G [00:47<00:00, 365MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▉| 9.69G/9.79G [00:48<00:00, 363MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 83.9M/9.71G [00:01<03:11, 50.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 105M/9.71G [00:01<02:53, 55.2MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 115M/9.71G [00:02<02:49, 56.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|▏         | 136M/9.71G [00:02<02:13, 71.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 147M/9.71G [00:02<02:46, 57.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 168M/9.71G [00:02<02:21, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 178M/9.71G [00:02<02:21, 67.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 199M/9.71G [00:03<02:22, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 210M/9.71G [00:03<02:42, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 220M/9.71G [00:03<02:31, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 231M/9.71G [00:03<02:40, 59.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 241M/9.71G [00:03<02:28, 63.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 262M/9.71G [00:04<02:11, 71.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 273M/9.71G [00:04<02:15, 69.6MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";:  99%|█████████▉| 9.73G/9.79G [00:48<00:00, 363MB/s]#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|█████████▉| 9.77G/9.79G [00:48<00:00, 375MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00005.bin\";: 100%|██████████| 9.79G/9.79G [00:48<00:00, 202MB/s]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.10s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.08s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.09s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.07s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.09s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.10s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.10s/it]\n",
      "Downloading shards:  40%|████      | 2/5 [01:26<02:12, 44.12s/it]\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 0.00/9.71G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 21.0M/9.71G [00:00<01:17, 125MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   0%|          | 41.9M/9.71G [00:00<01:12, 133MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 62.9M/9.71G [00:00<01:43, 93.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 294M/9.71G [00:04<02:25, 64.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 304M/9.71G [00:04<02:29, 62.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 325M/9.71G [00:05<02:15, 69.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 336M/9.71G [00:05<02:22, 65.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 73.4M/9.71G [00:00<01:48, 88.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 94.4M/9.71G [00:00<01:43, 93.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|          | 115M/9.71G [00:01<01:40, 95.8MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   1%|▏         | 136M/9.71G [00:01<01:23, 114MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 157M/9.71G [00:01<01:22, 115MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▎         | 346M/9.71G [00:05<02:22, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▎         | 357M/9.71G [00:05<02:24, 64.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 377M/9.71G [00:05<01:56, 80.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 388M/9.71G [00:06<01:59, 77.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 398M/9.71G [00:06<02:18, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 178M/9.71G [00:01<01:43, 92.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 199M/9.71G [00:01<01:30, 105MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 220M/9.71G [00:02<01:36, 98.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   2%|▏         | 241M/9.71G [00:02<01:54, 82.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 409M/9.71G [00:06<03:15, 47.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 419M/9.71G [00:07<04:01, 38.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 440M/9.71G [00:07<03:09, 48.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 262M/9.71G [00:02<01:49, 86.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 273M/9.71G [00:02<01:49, 86.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 294M/9.71G [00:03<01:48, 86.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 315M/9.71G [00:03<01:37, 96.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   3%|▎         | 325M/9.71G [00:03<01:38, 95.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▎         | 346M/9.71G [00:03<01:37, 95.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 451M/9.71G [00:07<03:26, 44.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 472M/9.71G [00:07<02:43, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 482M/9.71G [00:08<02:49, 54.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▌         | 503M/9.71G [00:08<02:37, 58.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▎         | 357M/9.71G [00:03<01:41, 91.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 377M/9.71G [00:03<01:40, 93.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 388M/9.71G [00:04<01:39, 93.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 409M/9.71G [00:04<01:34, 98.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 419M/9.71G [00:04<01:42, 90.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   4%|▍         | 430M/9.71G [00:04<01:39, 93.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▌         | 514M/9.71G [00:08<02:37, 58.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 535M/9.71G [00:08<02:30, 61.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 545M/9.71G [00:09<02:34, 59.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 566M/9.71G [00:09<02:22, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 451M/9.71G [00:04<01:30, 102MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▍         | 472M/9.71G [00:04<01:23, 111MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▌         | 493M/9.71G [00:04<01:17, 118MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   5%|▌         | 514M/9.71G [00:05<01:22, 112MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 535M/9.71G [00:05<01:26, 106MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 577M/9.71G [00:09<02:26, 62.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 598M/9.71G [00:09<02:22, 64.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▋         | 608M/9.71G [00:10<02:27, 61.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▋         | 629M/9.71G [00:10<02:22, 63.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 556M/9.71G [00:05<01:37, 93.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 577M/9.71G [00:05<01:31, 100MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▌         | 598M/9.71G [00:06<01:26, 105MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▋         | 619M/9.71G [00:06<01:29, 101MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   6%|▋         | 629M/9.71G [00:06<01:33, 97.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 640M/9.71G [00:10<03:03, 49.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 650M/9.71G [00:10<02:51, 52.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 671M/9.71G [00:11<02:10, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 682M/9.71G [00:11<02:20, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 640M/9.71G [00:06<02:15, 66.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 650M/9.71G [00:06<02:15, 66.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 661M/9.71G [00:07<02:11, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 682M/9.71G [00:07<01:56, 77.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 692M/9.71G [00:07<02:15, 66.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 713M/9.71G [00:07<01:53, 79.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 724M/9.71G [00:08<02:38, 56.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 744M/9.71G [00:08<02:23, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 755M/9.71G [00:08<02:43, 54.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 776M/9.71G [00:08<02:31, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 786M/9.71G [00:09<02:34, 57.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 807M/9.71G [00:09<02:08, 69.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 818M/9.71G [00:09<02:32, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 692M/9.71G [00:11<02:33, 58.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 713M/9.71G [00:11<02:04, 72.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   7%|▋         | 724M/9.71G [00:12<02:34, 58.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 744M/9.71G [00:12<02:30, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 755M/9.71G [00:12<02:58, 50.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 776M/9.71G [00:12<02:26, 60.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 786M/9.71G [00:13<02:37, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 807M/9.71G [00:13<02:07, 70.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   8%|▊         | 818M/9.71G [00:13<02:29, 59.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▊         | 839M/9.71G [00:13<02:09, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▊         | 849M/9.71G [00:13<02:10, 68.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 870M/9.71G [00:14<02:05, 70.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▊         | 839M/9.71G [00:09<02:04, 71.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▊         | 849M/9.71G [00:09<02:09, 68.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 860M/9.71G [00:10<02:24, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 870M/9.71G [00:10<02:53, 50.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 881M/9.71G [00:14<02:13, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 902M/9.71G [00:14<01:49, 80.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 912M/9.71G [00:14<02:06, 69.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 933M/9.71G [00:15<01:53, 77.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 944M/9.71G [00:15<01:53, 77.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 881M/9.71G [00:10<02:31, 58.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 902M/9.71G [00:10<02:19, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:   9%|▉         | 912M/9.71G [00:11<03:03, 47.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 923M/9.71G [00:11<02:54, 50.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 944M/9.71G [00:11<02:00, 72.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 954M/9.71G [00:15<02:08, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 965M/9.71G [00:15<02:15, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 986M/9.71G [00:15<01:58, 73.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 996M/9.71G [00:16<02:32, 57.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 1.02G/9.71G [00:16<02:02, 71.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 954M/9.71G [00:11<02:08, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|▉         | 965M/9.71G [00:11<02:00, 72.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 986M/9.71G [00:12<01:41, 85.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 996M/9.71G [00:12<01:46, 81.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 1.01G/9.71G [00:12<02:05, 69.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.03G/9.71G [00:16<02:07, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.05G/9.71G [00:16<01:50, 78.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.06G/9.71G [00:17<02:17, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.08G/9.71G [00:17<01:54, 75.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  10%|█         | 1.02G/9.71G [00:12<02:15, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.03G/9.71G [00:12<02:08, 67.4MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.05G/9.71G [00:13<02:30, 57.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.06G/9.71G [00:13<02:14, 64.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.07G/9.71G [00:13<02:01, 70.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.08G/9.71G [00:13<01:54, 75.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.09G/9.71G [00:17<02:11, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█▏        | 1.10G/9.71G [00:17<02:02, 70.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█▏        | 1.11G/9.71G [00:17<02:14, 64.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.12G/9.71G [00:17<02:26, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.14G/9.71G [00:18<02:04, 68.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█         | 1.09G/9.71G [00:13<01:47, 80.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  11%|█▏        | 1.11G/9.71G [00:13<01:29, 96.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.12G/9.71G [00:14<01:59, 71.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.13G/9.71G [00:14<01:53, 75.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.14G/9.71G [00:14<01:49, 78.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.15G/9.71G [00:14<02:10, 65.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.15G/9.71G [00:18<02:47, 50.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.17G/9.71G [00:18<02:20, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.18G/9.71G [00:18<02:10, 65.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.20G/9.71G [00:19<02:11, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.21G/9.71G [00:19<02:31, 56.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.17G/9.71G [00:14<02:03, 68.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.18G/9.71G [00:14<01:55, 73.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.20G/9.71G [00:15<02:01, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  12%|█▏        | 1.21G/9.71G [00:15<02:25, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.22G/9.71G [00:15<02:35, 54.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.22G/9.71G [00:19<02:37, 54.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.23G/9.71G [00:19<02:26, 58.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.24G/9.71G [00:19<02:41, 52.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.25G/9.71G [00:20<02:23, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.26G/9.71G [00:20<02:24, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.23G/9.71G [00:15<02:19, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.24G/9.71G [00:15<02:31, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.25G/9.71G [00:16<02:19, 60.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.26G/9.71G [00:16<02:17, 61.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.27G/9.71G [00:16<02:19, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.27G/9.71G [00:20<02:23, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.29G/9.71G [00:20<01:55, 72.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.30G/9.71G [00:20<02:26, 57.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▎        | 1.32G/9.71G [00:21<02:19, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.29G/9.71G [00:16<02:11, 64.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  13%|█▎        | 1.30G/9.71G [00:17<02:32, 55.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▎        | 1.31G/9.71G [00:17<02:15, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▎        | 1.32G/9.71G [00:17<02:17, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▎        | 1.33G/9.71G [00:17<02:18, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▎        | 1.33G/9.71G [00:21<02:17, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.34G/9.71G [00:21<02:23, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.35G/9.71G [00:21<02:17, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.36G/9.71G [00:21<02:16, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.38G/9.71G [00:22<01:54, 72.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.34G/9.71G [00:17<02:22, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.35G/9.71G [00:17<02:31, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.36G/9.71G [00:18<02:30, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.38G/9.71G [00:18<01:53, 73.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.39G/9.71G [00:18<02:34, 53.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.39G/9.71G [00:22<02:19, 59.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.42G/9.71G [00:22<02:07, 65.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.43G/9.71G [00:22<02:10, 63.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.45G/9.71G [00:23<02:09, 63.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  14%|█▍        | 1.41G/9.71G [00:18<02:17, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.42G/9.71G [00:18<02:08, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.43G/9.71G [00:19<02:09, 63.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▍        | 1.45G/9.71G [00:19<02:02, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.46G/9.71G [00:19<02:01, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.46G/9.71G [00:23<02:14, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.48G/9.71G [00:23<02:00, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.49G/9.71G [00:24<02:27, 55.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.51G/9.71G [00:24<02:13, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.52G/9.71G [00:24<02:16, 59.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.54G/9.71G [00:24<01:58, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.55G/9.71G [00:24<02:05, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.56G/9.71G [00:25<02:09, 62.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.57G/9.71G [00:25<02:21, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▋        | 1.59G/9.71G [00:25<01:56, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.60G/9.71G [00:25<01:58, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.63G/9.71G [00:25<01:51, 72.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.64G/9.71G [00:26<02:09, 62.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.48G/9.71G [00:19<01:58, 69.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  15%|█▌        | 1.49G/9.71G [00:19<02:08, 63.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.51G/9.71G [00:20<02:18, 59.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.52G/9.71G [00:20<02:13, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.54G/9.71G [00:20<01:55, 70.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.56G/9.71G [00:21<02:00, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▌        | 1.57G/9.71G [00:21<02:04, 65.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  16%|█▋        | 1.59G/9.71G [00:21<02:05, 64.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.60G/9.71G [00:21<02:02, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.63G/9.71G [00:21<01:53, 70.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.64G/9.71G [00:22<01:59, 67.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.65G/9.71G [00:22<01:57, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.66G/9.71G [00:22<02:25, 55.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.66G/9.71G [00:26<02:10, 61.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.67G/9.71G [00:26<02:10, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.69G/9.71G [00:26<01:52, 71.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.70G/9.71G [00:27<02:04, 64.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.68G/9.71G [00:22<01:49, 73.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.69G/9.71G [00:23<02:10, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  17%|█▋        | 1.70G/9.71G [00:23<02:07, 62.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.72G/9.71G [00:23<02:11, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.72G/9.71G [00:27<02:08, 62.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.73G/9.71G [00:27<01:57, 68.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.75G/9.71G [00:28<02:22, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.76G/9.71G [00:28<02:20, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.73G/9.71G [00:23<02:08, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.75G/9.71G [00:23<01:50, 72.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.76G/9.71G [00:24<02:30, 53.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.78G/9.71G [00:24<02:02, 64.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.78G/9.71G [00:28<02:00, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.79G/9.71G [00:28<01:59, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▊        | 1.81G/9.71G [00:28<01:45, 74.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.82G/9.71G [00:29<01:49, 72.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.85G/9.71G [00:29<01:37, 80.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  18%|█▊        | 1.79G/9.71G [00:24<02:02, 64.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▊        | 1.81G/9.71G [00:24<01:50, 71.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.82G/9.71G [00:25<01:55, 68.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.84G/9.71G [00:25<01:46, 73.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.85G/9.71G [00:25<01:39, 79.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.87G/9.71G [00:25<01:38, 80.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.87G/9.71G [00:29<01:38, 79.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.88G/9.71G [00:29<01:47, 72.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.90G/9.71G [00:30<02:23, 54.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  19%|█▉        | 1.88G/9.71G [00:25<01:44, 74.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.90G/9.71G [00:26<02:19, 56.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.91G/9.71G [00:26<02:24, 53.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.91G/9.71G [00:30<02:18, 56.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.93G/9.71G [00:30<02:24, 53.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.94G/9.71G [00:31<02:18, 56.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|██        | 1.96G/9.71G [00:31<02:06, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.93G/9.71G [00:26<02:33, 50.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|█▉        | 1.94G/9.71G [00:27<02:18, 56.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|██        | 1.96G/9.71G [00:27<02:06, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|██        | 1.97G/9.71G [00:27<01:56, 66.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  20%|██        | 1.97G/9.71G [00:31<01:57, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 1.99G/9.71G [00:31<01:55, 66.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.00G/9.71G [00:31<01:57, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.02G/9.71G [00:32<01:56, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 1.99G/9.71G [00:27<01:56, 66.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.00G/9.71G [00:27<01:51, 68.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.02G/9.71G [00:28<01:57, 65.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.03G/9.71G [00:28<02:18, 55.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.03G/9.71G [00:32<02:19, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.06G/9.71G [00:32<02:03, 61.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██▏       | 2.07G/9.71G [00:33<02:35, 49.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██▏       | 2.09G/9.71G [00:33<01:57, 65.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██        | 2.06G/9.71G [00:28<02:04, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██▏       | 2.07G/9.71G [00:29<02:20, 54.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  21%|██▏       | 2.09G/9.71G [00:29<01:59, 63.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.10G/9.71G [00:33<02:22, 53.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.12G/9.71G [00:34<02:18, 54.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.13G/9.71G [00:34<02:16, 55.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.15G/9.71G [00:34<02:05, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.16G/9.71G [00:34<02:08, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.18G/9.71G [00:34<01:50, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.19G/9.71G [00:35<01:44, 71.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.20G/9.71G [00:35<02:04, 60.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.21G/9.71G [00:35<02:04, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.23G/9.71G [00:35<01:50, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.24G/9.71G [00:35<02:04, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.26G/9.71G [00:36<02:09, 57.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.10G/9.71G [00:29<02:30, 50.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.12G/9.71G [00:30<02:18, 54.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.13G/9.71G [00:30<02:14, 56.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.15G/9.71G [00:30<01:54, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.16G/9.71G [00:30<02:14, 56.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.17G/9.71G [00:30<02:01, 62.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  22%|██▏       | 2.18G/9.71G [00:31<02:15, 55.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.19G/9.71G [00:31<01:59, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.20G/9.71G [00:31<01:59, 62.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.21G/9.71G [00:31<02:00, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.23G/9.71G [00:31<01:45, 71.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.24G/9.71G [00:31<01:59, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.26G/9.71G [00:32<01:53, 65.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.28G/9.71G [00:32<01:53, 65.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  23%|██▎       | 2.28G/9.71G [00:36<02:00, 61.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▎       | 2.30G/9.71G [00:36<01:50, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.31G/9.71G [00:37<02:08, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.33G/9.71G [00:37<02:06, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▎       | 2.30G/9.71G [00:32<01:54, 64.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.31G/9.71G [00:33<02:18, 53.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.32G/9.71G [00:33<02:02, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.33G/9.71G [00:33<02:09, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.34G/9.71G [00:37<02:08, 57.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.36G/9.71G [00:37<02:02, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.37G/9.71G [00:38<02:15, 54.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.34G/9.71G [00:33<02:18, 53.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.36G/9.71G [00:33<02:00, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  24%|██▍       | 2.37G/9.71G [00:34<02:13, 54.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.38G/9.71G [00:34<02:04, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.39G/9.71G [00:34<01:52, 65.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.39G/9.71G [00:38<01:56, 62.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.40G/9.71G [00:38<02:08, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.41G/9.71G [00:38<01:56, 62.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.43G/9.71G [00:39<01:45, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.44G/9.71G [00:39<01:50, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.45G/9.71G [00:39<01:41, 71.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.40G/9.71G [00:34<02:13, 54.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▍       | 2.42G/9.71G [00:34<01:42, 71.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.43G/9.71G [00:35<01:44, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.45G/9.71G [00:35<01:31, 79.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.46G/9.71G [00:35<01:39, 72.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  25%|██▌       | 2.46G/9.71G [00:39<01:50, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.49G/9.71G [00:39<01:32, 78.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.50G/9.71G [00:39<01:29, 80.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.51G/9.71G [00:39<01:26, 83.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.52G/9.71G [00:40<01:33, 77.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.49G/9.71G [00:35<01:37, 74.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.51G/9.71G [00:35<01:36, 74.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.52G/9.71G [00:36<01:36, 74.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.54G/9.71G [00:36<01:32, 77.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.55G/9.71G [00:36<01:48, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.54G/9.71G [00:40<01:41, 70.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▌       | 2.55G/9.71G [00:40<01:55, 62.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▋       | 2.57G/9.71G [00:40<01:39, 71.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.58G/9.71G [00:41<01:59, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.60G/9.71G [00:41<01:36, 73.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  26%|██▋       | 2.57G/9.71G [00:36<01:46, 67.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.58G/9.71G [00:37<02:00, 59.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.60G/9.71G [00:37<01:41, 70.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.61G/9.71G [00:37<01:36, 73.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.61G/9.71G [00:41<01:35, 74.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.63G/9.71G [00:41<01:26, 81.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.64G/9.71G [00:41<01:30, 78.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.65G/9.71G [00:42<01:52, 62.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.66G/9.71G [00:42<01:52, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.63G/9.71G [00:37<01:31, 77.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.64G/9.71G [00:37<01:27, 80.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.65G/9.71G [00:38<01:51, 63.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  27%|██▋       | 2.66G/9.71G [00:38<01:57, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.67G/9.71G [00:38<01:57, 59.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.67G/9.71G [00:42<01:53, 62.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.69G/9.71G [00:42<01:42, 68.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.71G/9.71G [00:43<02:07, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.73G/9.71G [00:43<02:00, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.69G/9.71G [00:38<01:37, 71.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.71G/9.71G [00:39<02:08, 54.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.73G/9.71G [00:39<01:44, 66.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.74G/9.71G [00:39<01:49, 63.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.74G/9.71G [00:43<02:08, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.76G/9.71G [00:43<01:42, 68.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▊       | 2.77G/9.71G [00:44<01:53, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▊       | 2.79G/9.71G [00:44<01:36, 71.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.80G/9.71G [00:44<01:33, 73.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  28%|██▊       | 2.76G/9.71G [00:39<01:41, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▊       | 2.77G/9.71G [00:40<02:01, 56.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▊       | 2.78G/9.71G [00:40<01:54, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▊       | 2.79G/9.71G [00:40<01:46, 64.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.80G/9.71G [00:40<01:43, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.81G/9.71G [00:40<01:34, 72.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.81G/9.71G [00:44<01:33, 73.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.82G/9.71G [00:44<01:44, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.83G/9.71G [00:44<01:38, 69.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.84G/9.71G [00:45<01:36, 71.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.85G/9.71G [00:45<01:41, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.82G/9.71G [00:40<01:40, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.84G/9.71G [00:41<01:34, 72.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  29%|██▉       | 2.85G/9.71G [00:41<01:37, 70.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.87G/9.71G [00:41<01:28, 77.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.88G/9.71G [00:41<01:33, 72.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.87G/9.71G [00:45<01:31, 74.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.88G/9.71G [00:45<01:47, 63.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.90G/9.71G [00:45<01:38, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.92G/9.71G [00:46<01:33, 72.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.94G/9.71G [00:46<01:27, 77.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|██▉       | 2.90G/9.71G [00:41<01:41, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.92G/9.71G [00:42<01:39, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.94G/9.71G [00:42<01:27, 77.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.95G/9.71G [00:42<01:46, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 2.97G/9.71G [00:42<01:34, 71.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 2.98G/9.71G [00:42<01:35, 70.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.00G/9.71G [00:43<01:39, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.01G/9.71G [00:43<01:36, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.03G/9.71G [00:43<01:38, 68.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███▏      | 3.04G/9.71G [00:44<01:50, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███▏      | 3.05G/9.71G [00:44<01:42, 65.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.06G/9.71G [00:44<01:42, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.07G/9.71G [00:44<01:38, 67.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.09G/9.71G [00:44<01:26, 76.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  30%|███       | 2.95G/9.71G [00:46<01:38, 68.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 2.97G/9.71G [00:46<01:28, 75.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 2.98G/9.71G [00:46<01:34, 71.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.00G/9.71G [00:47<01:42, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.01G/9.71G [00:47<01:41, 66.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███       | 3.03G/9.71G [00:47<01:37, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  31%|███▏      | 3.04G/9.71G [00:48<01:54, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.06G/9.71G [00:48<01:44, 63.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.08G/9.71G [00:48<01:22, 80.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.09G/9.71G [00:48<01:20, 81.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.11G/9.71G [00:48<01:12, 91.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.12G/9.71G [00:48<01:25, 77.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.15G/9.71G [00:49<01:26, 75.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.11G/9.71G [00:44<01:13, 89.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.12G/9.71G [00:45<01:24, 78.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  32%|███▏      | 3.14G/9.71G [00:45<01:20, 81.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.16G/9.71G [00:45<01:33, 70.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.16G/9.71G [00:49<01:41, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.18G/9.71G [00:49<01:34, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.19G/9.71G [00:49<01:32, 70.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.18G/9.71G [00:45<01:30, 72.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.19G/9.71G [00:45<01:29, 73.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.21G/9.71G [00:50<02:37, 41.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.22G/9.71G [00:51<03:11, 33.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.23G/9.71G [00:51<02:44, 39.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.21G/9.71G [00:46<02:33, 42.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.22G/9.71G [00:47<03:26, 31.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.23G/9.71G [00:47<02:53, 37.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.24G/9.71G [00:51<02:31, 42.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.25G/9.71G [00:51<02:18, 46.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▎      | 3.27G/9.71G [00:52<02:04, 51.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.28G/9.71G [00:52<02:08, 49.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.24G/9.71G [00:47<02:39, 40.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  33%|███▎      | 3.25G/9.71G [00:47<02:15, 47.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▎      | 3.27G/9.71G [00:48<01:45, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.28G/9.71G [00:48<02:01, 53.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.30G/9.71G [00:48<01:49, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.30G/9.71G [00:52<01:54, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.31G/9.71G [00:52<01:50, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.33G/9.71G [00:53<01:50, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.34G/9.71G [00:53<01:45, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.31G/9.71G [00:48<01:47, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.33G/9.71G [00:49<01:43, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  34%|███▍      | 3.34G/9.71G [00:49<01:45, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.37G/9.71G [00:49<01:35, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.37G/9.71G [00:53<01:30, 70.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.38G/9.71G [00:53<01:44, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.40G/9.71G [00:54<01:33, 67.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.41G/9.71G [00:54<01:41, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.42G/9.71G [00:54<01:38, 64.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.38G/9.71G [00:49<01:38, 64.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▍      | 3.40G/9.71G [00:50<01:34, 66.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.41G/9.71G [00:50<01:41, 61.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.42G/9.71G [00:50<01:51, 56.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.43G/9.71G [00:54<01:39, 63.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.45G/9.71G [00:55<02:04, 50.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.46G/9.71G [00:55<02:13, 46.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  35%|███▌      | 3.43G/9.71G [00:50<01:56, 53.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.45G/9.71G [00:51<02:04, 50.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.46G/9.71G [00:51<02:04, 50.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.48G/9.71G [00:51<01:36, 64.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.48G/9.71G [00:55<01:40, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.49G/9.71G [00:55<01:56, 53.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.51G/9.71G [00:56<01:38, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▋      | 3.52G/9.71G [00:56<01:35, 64.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.49G/9.71G [00:51<01:44, 59.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▌      | 3.51G/9.71G [00:52<01:30, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  36%|███▋      | 3.52G/9.71G [00:52<01:37, 63.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.54G/9.71G [00:52<01:34, 65.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.54G/9.71G [00:56<01:25, 72.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.55G/9.71G [00:56<01:45, 58.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.58G/9.71G [00:57<01:41, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.59G/9.71G [00:57<01:57, 52.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.55G/9.71G [00:52<01:42, 59.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.58G/9.71G [00:53<01:45, 58.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.59G/9.71G [00:53<01:55, 52.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.61G/9.71G [00:57<02:00, 50.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.62G/9.71G [00:58<01:58, 51.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.64G/9.71G [00:58<01:37, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.65G/9.71G [00:58<01:37, 62.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.61G/9.71G [00:53<02:04, 48.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.62G/9.71G [00:54<01:54, 53.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  37%|███▋      | 3.64G/9.71G [00:54<01:40, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.65G/9.71G [00:54<01:38, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.67G/9.71G [00:58<01:34, 64.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.68G/9.71G [00:58<01:37, 61.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.69G/9.71G [00:59<02:05, 48.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.70G/9.71G [00:59<02:10, 46.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.72G/9.71G [00:59<01:40, 59.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.73G/9.71G [00:59<01:35, 62.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▊      | 3.75G/9.71G [01:00<01:18, 75.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.76G/9.71G [01:00<01:28, 67.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.79G/9.71G [01:00<01:21, 72.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.80G/9.71G [01:00<01:22, 72.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.81G/9.71G [01:00<01:17, 75.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.82G/9.71G [01:00<01:16, 77.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.83G/9.71G [01:01<01:25, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.85G/9.71G [01:01<01:20, 72.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.67G/9.71G [00:54<01:29, 67.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.68G/9.71G [00:54<01:37, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.69G/9.71G [00:55<02:11, 45.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.70G/9.71G [00:55<02:06, 47.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.72G/9.71G [00:55<01:39, 60.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  38%|███▊      | 3.73G/9.71G [00:55<01:33, 63.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▊      | 3.75G/9.71G [00:56<01:17, 76.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.76G/9.71G [00:56<01:31, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.79G/9.71G [00:56<01:22, 72.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.80G/9.71G [00:56<01:22, 71.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.81G/9.71G [00:56<01:19, 74.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.82G/9.71G [00:57<01:24, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  39%|███▉      | 3.83G/9.71G [00:57<01:16, 76.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.85G/9.71G [00:57<01:16, 76.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.86G/9.71G [01:01<01:21, 71.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.88G/9.71G [01:01<01:24, 69.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.89G/9.71G [01:02<01:20, 72.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.91G/9.71G [01:02<01:18, 74.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.86G/9.71G [00:57<01:37, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|███▉      | 3.88G/9.71G [00:57<01:20, 72.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.89G/9.71G [00:58<01:17, 75.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.91G/9.71G [00:58<01:08, 84.1MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.92G/9.71G [00:58<01:24, 68.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  40%|████      | 3.92G/9.71G [01:02<01:21, 71.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.94G/9.71G [01:02<01:14, 77.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.95G/9.71G [01:02<01:17, 73.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.97G/9.71G [01:03<01:16, 74.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.98G/9.71G [01:03<01:22, 69.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.94G/9.71G [00:58<01:14, 77.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.95G/9.71G [00:58<01:21, 70.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.97G/9.71G [00:59<01:13, 77.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 3.98G/9.71G [00:59<01:23, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████▏     | 4.01G/9.71G [00:59<01:18, 72.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████      | 4.00G/9.71G [01:03<01:26, 66.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████▏     | 4.01G/9.71G [01:03<01:23, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████▏     | 4.02G/9.71G [01:03<01:48, 52.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.04G/9.71G [01:04<01:30, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  41%|████▏     | 4.02G/9.71G [00:59<01:42, 55.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.04G/9.71G [01:00<01:29, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.06G/9.71G [01:00<01:19, 71.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.07G/9.71G [01:00<01:21, 69.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.06G/9.71G [01:04<01:23, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.07G/9.71G [01:04<01:17, 72.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.09G/9.71G [01:04<01:20, 70.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.10G/9.71G [01:05<01:32, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.09G/9.71G [01:00<01:19, 70.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.10G/9.71G [01:01<01:33, 59.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.12G/9.71G [01:01<01:28, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.13G/9.71G [01:01<01:31, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  42%|████▏     | 4.12G/9.71G [01:05<01:28, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.13G/9.71G [01:05<01:31, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.15G/9.71G [01:05<01:26, 63.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.16G/9.71G [01:06<01:43, 53.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.15G/9.71G [01:01<01:29, 61.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.16G/9.71G [01:02<01:36, 57.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.18G/9.71G [01:02<01:29, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.18G/9.71G [01:06<01:30, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.19G/9.71G [01:06<01:33, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.22G/9.71G [01:06<01:17, 71.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▎     | 4.23G/9.71G [01:07<01:19, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▎     | 4.25G/9.71G [01:07<01:14, 73.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.19G/9.71G [01:02<01:35, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  43%|████▎     | 4.22G/9.71G [01:02<01:19, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▎     | 4.23G/9.71G [01:03<01:21, 67.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▎     | 4.25G/9.71G [01:03<01:15, 72.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.26G/9.71G [01:03<01:26, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.26G/9.71G [01:07<01:19, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.28G/9.71G [01:07<01:08, 79.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.29G/9.71G [01:07<01:09, 77.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.30G/9.71G [01:08<01:35, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.27G/9.71G [01:03<01:22, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.28G/9.71G [01:03<01:22, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.29G/9.71G [01:04<01:20, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.30G/9.71G [01:04<01:20, 67.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.31G/9.71G [01:04<01:34, 57.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.32G/9.71G [01:04<01:33, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  44%|████▍     | 4.31G/9.71G [01:08<01:36, 56.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.32G/9.71G [01:08<01:39, 54.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.34G/9.71G [01:08<01:18, 68.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.36G/9.71G [01:09<01:13, 72.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▌     | 4.37G/9.71G [01:09<01:22, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.34G/9.71G [01:04<01:17, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.35G/9.71G [01:05<01:30, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▍     | 4.36G/9.71G [01:05<01:34, 56.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▌     | 4.38G/9.71G [01:05<01:10, 75.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▌     | 4.39G/9.71G [01:09<01:12, 73.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▌     | 4.40G/9.71G [01:10<01:42, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.42G/9.71G [01:10<01:19, 66.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.44G/9.71G [01:10<01:17, 67.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  45%|████▌     | 4.40G/9.71G [01:05<01:07, 79.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.42G/9.71G [01:06<01:22, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.44G/9.71G [01:06<01:22, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.45G/9.71G [01:06<01:18, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.46G/9.71G [01:06<01:17, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.46G/9.71G [01:10<01:08, 76.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.47G/9.71G [01:10<01:13, 71.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.49G/9.71G [01:10<00:59, 87.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▋     | 4.50G/9.71G [01:11<01:08, 76.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.52G/9.71G [01:11<01:07, 77.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.48G/9.71G [01:06<01:08, 76.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▌     | 4.49G/9.71G [01:06<01:06, 78.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  46%|████▋     | 4.50G/9.71G [01:07<01:03, 82.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.52G/9.71G [01:07<01:12, 71.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.53G/9.71G [01:07<01:26, 60.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.54G/9.71G [01:07<01:29, 57.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.55G/9.71G [01:08<01:25, 60.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.56G/9.71G [01:08<01:43, 49.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.58G/9.71G [01:08<01:21, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.59G/9.71G [01:08<01:15, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.61G/9.71G [01:08<01:04, 78.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.62G/9.71G [01:09<01:11, 71.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.65G/9.71G [01:09<01:31, 55.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.53G/9.71G [01:11<01:24, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.54G/9.71G [01:11<01:27, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.55G/9.71G [01:12<01:33, 55.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.56G/9.71G [01:12<01:49, 47.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.58G/9.71G [01:12<01:29, 57.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  47%|████▋     | 4.59G/9.71G [01:12<01:24, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.61G/9.71G [01:13<01:15, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.62G/9.71G [01:13<01:23, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.65G/9.71G [01:13<01:24, 60.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.66G/9.71G [01:13<01:20, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.67G/9.71G [01:13<01:22, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.68G/9.71G [01:14<01:17, 64.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.70G/9.71G [01:14<01:15, 66.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.66G/9.71G [01:09<01:22, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.67G/9.71G [01:09<01:30, 55.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.68G/9.71G [01:10<01:27, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  48%|████▊     | 4.70G/9.71G [01:10<01:22, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▊     | 4.71G/9.71G [01:10<01:19, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▊     | 4.71G/9.71G [01:14<01:15, 66.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▊     | 4.73G/9.71G [01:14<01:11, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.74G/9.71G [01:15<01:28, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▊     | 4.73G/9.71G [01:10<01:10, 70.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.74G/9.71G [01:11<01:25, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.76G/9.71G [01:11<01:20, 61.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.77G/9.71G [01:11<01:27, 56.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.76G/9.71G [01:15<01:28, 55.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.77G/9.71G [01:15<01:28, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.79G/9.71G [01:15<01:15, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.80G/9.71G [01:16<01:16, 64.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|████▉     | 4.82G/9.71G [01:16<01:08, 71.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.79G/9.71G [01:11<01:17, 63.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  49%|████▉     | 4.80G/9.71G [01:12<01:13, 66.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|████▉     | 4.82G/9.71G [01:12<01:19, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|████▉     | 4.83G/9.71G [01:16<01:10, 69.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|████▉     | 4.84G/9.71G [01:16<01:07, 71.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.85G/9.71G [01:16<01:06, 73.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.87G/9.71G [01:16<01:06, 72.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.89G/9.71G [01:17<01:03, 75.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.90G/9.71G [01:17<01:13, 65.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|████▉     | 4.84G/9.71G [01:12<01:18, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.85G/9.71G [01:12<01:20, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.87G/9.71G [01:13<01:12, 66.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.89G/9.71G [01:13<00:58, 83.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  50%|█████     | 4.90G/9.71G [01:13<01:02, 77.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.92G/9.71G [01:17<01:27, 54.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.93G/9.71G [01:18<01:39, 48.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.94G/9.71G [01:18<01:36, 49.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.92G/9.71G [01:13<01:21, 58.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.93G/9.71G [01:14<01:19, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.95G/9.71G [01:14<01:26, 55.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.96G/9.71G [01:18<01:29, 53.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.97G/9.71G [01:18<01:25, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████▏    | 4.98G/9.71G [01:19<01:23, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.00G/9.71G [01:19<01:06, 70.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.96G/9.71G [01:14<01:33, 50.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████     | 4.97G/9.71G [01:14<01:27, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  51%|█████▏    | 4.98G/9.71G [01:15<01:28, 53.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.00G/9.71G [01:15<01:06, 71.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.01G/9.71G [01:19<01:30, 51.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.03G/9.71G [01:19<01:13, 64.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.04G/9.71G [01:19<01:08, 68.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.05G/9.71G [01:20<01:27, 53.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.01G/9.71G [01:15<01:37, 48.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.03G/9.71G [01:15<01:18, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.04G/9.71G [01:16<01:14, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.05G/9.71G [01:16<01:23, 56.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.06G/9.71G [01:16<01:19, 58.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.08G/9.71G [01:16<01:14, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.06G/9.71G [01:20<01:24, 55.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.08G/9.71G [01:20<01:27, 52.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.10G/9.71G [01:20<01:13, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.11G/9.71G [01:21<01:10, 65.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.13G/9.71G [01:21<01:05, 69.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.09G/9.71G [01:16<01:10, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  52%|█████▏    | 5.10G/9.71G [01:16<01:14, 62.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.11G/9.71G [01:17<01:15, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.13G/9.71G [01:17<01:05, 69.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.14G/9.71G [01:17<01:07, 67.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.14G/9.71G [01:21<01:14, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.15G/9.71G [01:21<01:15, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.16G/9.71G [01:21<01:14, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.17G/9.71G [01:22<01:11, 63.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.19G/9.71G [01:22<01:01, 73.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.15G/9.71G [01:17<01:17, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.16G/9.71G [01:17<01:16, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.17G/9.71G [01:18<01:10, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  53%|█████▎    | 5.19G/9.71G [01:18<01:02, 71.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▎    | 5.20G/9.71G [01:18<01:17, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▎    | 5.20G/9.71G [01:22<01:20, 55.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.22G/9.71G [01:22<01:06, 67.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.23G/9.71G [01:23<01:18, 57.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.24G/9.71G [01:23<01:13, 61.1MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.22G/9.71G [01:18<01:07, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.23G/9.71G [01:19<01:18, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.24G/9.71G [01:19<01:13, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.25G/9.71G [01:19<01:23, 53.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.25G/9.71G [01:23<01:17, 57.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.27G/9.71G [01:23<01:06, 67.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.28G/9.71G [01:24<01:23, 53.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.31G/9.71G [01:24<01:22, 53.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.32G/9.71G [01:24<01:20, 54.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.33G/9.71G [01:24<01:20, 54.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.34G/9.71G [01:25<01:37, 44.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.35G/9.71G [01:25<01:25, 51.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.36G/9.71G [01:25<01:15, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.37G/9.71G [01:25<01:33, 46.4MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.38G/9.71G [01:25<01:22, 52.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.40G/9.71G [01:26<01:12, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.42G/9.71G [01:26<00:55, 76.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.27G/9.71G [01:19<01:04, 68.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  54%|█████▍    | 5.28G/9.71G [01:19<01:04, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.31G/9.71G [01:20<01:12, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.32G/9.71G [01:20<01:23, 52.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.33G/9.71G [01:20<01:28, 49.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▍    | 5.34G/9.71G [01:21<01:42, 42.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.35G/9.71G [01:21<01:32, 47.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.36G/9.71G [01:21<01:23, 52.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.37G/9.71G [01:21<01:33, 46.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  55%|█████▌    | 5.38G/9.71G [01:21<01:22, 52.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.40G/9.71G [01:22<01:08, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.41G/9.71G [01:22<01:04, 66.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.43G/9.71G [01:22<00:58, 73.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.43G/9.71G [01:26<01:04, 66.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.44G/9.71G [01:26<01:09, 61.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▋    | 5.46G/9.71G [01:27<00:56, 75.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▋    | 5.47G/9.71G [01:27<01:02, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▌    | 5.44G/9.71G [01:22<01:04, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▋    | 5.46G/9.71G [01:22<00:56, 74.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  56%|█████▋    | 5.47G/9.71G [01:23<01:04, 65.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.49G/9.71G [01:23<01:00, 69.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.49G/9.71G [01:27<01:00, 70.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.51G/9.71G [01:27<01:17, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.53G/9.71G [01:28<01:03, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.54G/9.71G [01:28<01:11, 58.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.51G/9.71G [01:23<01:13, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.53G/9.71G [01:24<01:06, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.54G/9.71G [01:24<01:12, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.56G/9.71G [01:28<01:00, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.57G/9.71G [01:28<00:58, 70.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.58G/9.71G [01:28<01:07, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.59G/9.71G [01:29<01:10, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.60G/9.71G [01:29<01:07, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.56G/9.71G [01:24<01:16, 54.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  57%|█████▋    | 5.58G/9.71G [01:24<01:05, 63.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.59G/9.71G [01:25<01:17, 53.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.61G/9.71G [01:25<01:14, 54.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.61G/9.71G [01:29<01:26, 47.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.62G/9.71G [01:29<01:19, 51.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.64G/9.71G [01:30<01:06, 61.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.65G/9.71G [01:30<01:03, 63.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.67G/9.71G [01:30<00:52, 77.1MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.62G/9.71G [01:25<01:09, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.64G/9.71G [01:26<01:09, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.65G/9.71G [01:26<01:08, 59.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  58%|█████▊    | 5.67G/9.71G [01:26<00:54, 74.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▊    | 5.68G/9.71G [01:26<00:55, 72.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▊    | 5.68G/9.71G [01:30<00:54, 73.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.70G/9.71G [01:30<00:49, 81.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.71G/9.71G [01:31<01:05, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.73G/9.71G [01:31<01:04, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.75G/9.71G [01:31<00:52, 75.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.70G/9.71G [01:26<00:47, 84.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.71G/9.71G [01:26<00:49, 81.4MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.74G/9.71G [01:27<00:49, 79.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.75G/9.71G [01:27<00:53, 74.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.77G/9.71G [01:32<01:19, 49.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  59%|█████▉    | 5.77G/9.71G [01:27<00:58, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.78G/9.71G [01:28<01:43, 38.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.78G/9.71G [01:32<01:32, 42.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.80G/9.71G [01:32<01:17, 50.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.81G/9.71G [01:32<01:16, 51.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.83G/9.71G [01:33<01:00, 63.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.80G/9.71G [01:28<01:27, 44.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|█████▉    | 5.81G/9.71G [01:28<01:22, 47.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.83G/9.71G [01:29<01:03, 60.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.84G/9.71G [01:29<01:14, 51.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.84G/9.71G [01:33<01:13, 52.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.86G/9.71G [01:33<01:00, 63.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.87G/9.71G [01:33<01:01, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.89G/9.71G [01:34<00:49, 77.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.91G/9.71G [01:34<00:47, 80.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.92G/9.71G [01:34<00:50, 75.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.95G/9.71G [01:34<00:49, 76.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████▏   | 5.96G/9.71G [01:34<00:55, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████▏   | 5.97G/9.71G [01:35<00:56, 66.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 5.98G/9.71G [01:35<01:00, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 5.99G/9.71G [01:35<01:05, 56.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.01G/9.71G [01:35<00:56, 65.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.02G/9.71G [01:36<01:01, 59.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.04G/9.71G [01:36<00:55, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.05G/9.71G [01:36<00:52, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.86G/9.71G [01:29<01:00, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  60%|██████    | 5.87G/9.71G [01:29<01:01, 61.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.89G/9.71G [01:30<00:49, 76.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.90G/9.71G [01:30<00:46, 81.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.91G/9.71G [01:30<00:49, 77.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.92G/9.71G [01:30<00:52, 72.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████    | 5.95G/9.71G [01:30<00:49, 76.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████▏   | 5.96G/9.71G [01:30<00:55, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  61%|██████▏   | 5.97G/9.71G [01:31<00:57, 65.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 5.98G/9.71G [01:31<00:53, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 5.99G/9.71G [01:31<01:00, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.01G/9.71G [01:31<01:00, 61.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.02G/9.71G [01:32<01:04, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.04G/9.71G [01:32<00:54, 67.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.05G/9.71G [01:32<00:59, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.06G/9.71G [01:37<01:27, 41.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.07G/9.71G [01:37<01:18, 46.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.08G/9.71G [01:37<01:10, 51.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  62%|██████▏   | 6.06G/9.71G [01:32<01:11, 51.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.07G/9.71G [01:33<01:16, 47.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.08G/9.71G [01:33<01:16, 47.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.10G/9.71G [01:37<01:12, 49.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.11G/9.71G [01:37<01:09, 51.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.13G/9.71G [01:38<00:57, 62.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.14G/9.71G [01:38<00:54, 65.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.10G/9.71G [01:33<01:12, 49.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.11G/9.71G [01:33<01:13, 48.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.13G/9.71G [01:34<01:01, 58.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  63%|██████▎   | 6.14G/9.71G [01:34<00:55, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▎   | 6.17G/9.71G [01:34<00:49, 72.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▎   | 6.17G/9.71G [01:38<00:45, 77.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▎   | 6.18G/9.71G [01:38<00:48, 72.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.20G/9.71G [01:38<00:43, 81.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.22G/9.71G [01:39<00:42, 82.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.23G/9.71G [01:39<00:47, 74.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▎   | 6.18G/9.71G [01:34<00:57, 61.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.20G/9.71G [01:34<00:42, 82.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.22G/9.71G [01:35<00:38, 90.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.23G/9.71G [01:35<00:44, 78.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.25G/9.71G [01:35<00:45, 76.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.25G/9.71G [01:39<00:43, 79.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.26G/9.71G [01:39<00:43, 79.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▍   | 6.28G/9.71G [01:39<00:42, 80.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▍   | 6.29G/9.71G [01:40<00:56, 60.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▍   | 6.30G/9.71G [01:40<00:55, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  64%|██████▍   | 6.26G/9.71G [01:35<00:45, 75.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▍   | 6.28G/9.71G [01:35<00:40, 84.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▍   | 6.29G/9.71G [01:36<00:50, 67.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.31G/9.71G [01:40<01:08, 49.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.32G/9.71G [01:40<01:01, 55.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.34G/9.71G [01:41<00:49, 67.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.35G/9.71G [01:41<00:47, 69.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.31G/9.71G [01:36<01:13, 46.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.32G/9.71G [01:37<01:06, 50.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.34G/9.71G [01:37<00:52, 63.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  65%|██████▌   | 6.35G/9.71G [01:37<00:52, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.36G/9.71G [01:37<00:53, 62.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.38G/9.71G [01:41<00:43, 76.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.39G/9.71G [01:41<00:51, 64.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.41G/9.71G [01:41<00:47, 68.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.42G/9.71G [01:42<00:49, 66.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▋   | 6.44G/9.71G [01:42<00:39, 81.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.38G/9.71G [01:37<00:58, 57.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.40G/9.71G [01:37<00:40, 80.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.41G/9.71G [01:37<00:39, 83.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▌   | 6.42G/9.71G [01:38<00:43, 75.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▋   | 6.44G/9.71G [01:38<00:35, 91.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▋   | 6.45G/9.71G [01:38<00:42, 76.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  66%|██████▋   | 6.45G/9.71G [01:42<00:44, 74.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.47G/9.71G [01:42<00:54, 59.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.48G/9.71G [01:43<00:51, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.50G/9.71G [01:43<00:46, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.47G/9.71G [01:38<00:48, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.48G/9.71G [01:39<00:47, 68.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.50G/9.71G [01:39<00:46, 69.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.52G/9.71G [01:39<00:43, 73.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.51G/9.71G [01:43<00:43, 74.0MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.52G/9.71G [01:43<00:42, 74.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.53G/9.71G [01:43<00:46, 68.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.55G/9.71G [01:43<00:38, 82.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.56G/9.71G [01:44<00:48, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  67%|██████▋   | 6.53G/9.71G [01:39<00:46, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.55G/9.71G [01:40<00:47, 67.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.56G/9.71G [01:40<00:43, 72.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.57G/9.71G [01:40<00:53, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.57G/9.71G [01:44<01:00, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.59G/9.71G [01:44<01:00, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.60G/9.71G [01:45<01:03, 49.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.59G/9.71G [01:40<01:06, 46.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.60G/9.71G [01:41<01:01, 50.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.61G/9.71G [01:41<00:55, 56.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.62G/9.71G [01:41<01:03, 48.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.62G/9.71G [01:45<01:04, 47.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.63G/9.71G [01:45<01:04, 47.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.65G/9.71G [01:45<00:49, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▊   | 6.66G/9.71G [01:46<00:56, 54.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.63G/9.71G [01:41<01:14, 41.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  68%|██████▊   | 6.65G/9.71G [01:41<00:52, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▊   | 6.66G/9.71G [01:42<00:53, 56.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.68G/9.71G [01:42<00:46, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.69G/9.71G [01:42<00:47, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.71G/9.71G [01:43<00:56, 53.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.72G/9.71G [01:43<00:53, 55.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.74G/9.71G [01:43<00:52, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.75G/9.71G [01:43<00:53, 55.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.77G/9.71G [01:44<00:44, 66.4MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.78G/9.71G [01:44<01:03, 45.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.68G/9.71G [01:46<00:51, 59.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.69G/9.71G [01:46<00:48, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.71G/9.71G [01:47<01:00, 49.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.72G/9.71G [01:47<00:53, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  69%|██████▉   | 6.74G/9.71G [01:47<00:51, 57.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.75G/9.71G [01:47<00:54, 53.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.77G/9.71G [01:48<00:42, 68.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|██████▉   | 6.78G/9.71G [01:48<01:02, 46.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.81G/9.71G [01:48<00:48, 59.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.83G/9.71G [01:48<00:42, 68.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.84G/9.71G [01:49<00:43, 66.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.86G/9.71G [01:49<00:41, 69.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.81G/9.71G [01:44<00:48, 59.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.83G/9.71G [01:44<00:42, 67.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  70%|███████   | 6.84G/9.71G [01:45<00:43, 65.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.86G/9.71G [01:45<00:41, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.87G/9.71G [01:45<00:39, 71.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.87G/9.71G [01:49<00:39, 72.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.89G/9.71G [01:49<00:39, 71.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.90G/9.71G [01:49<00:38, 72.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████▏  | 6.92G/9.71G [01:50<00:41, 66.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.89G/9.71G [01:45<00:37, 74.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████   | 6.90G/9.71G [01:45<00:36, 76.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████▏  | 6.92G/9.71G [01:46<00:41, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████▏  | 6.93G/9.71G [01:46<00:39, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.95G/9.71G [01:46<00:35, 78.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  71%|███████▏  | 6.93G/9.71G [01:50<00:41, 67.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.95G/9.71G [01:50<00:37, 72.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.96G/9.71G [01:50<00:41, 66.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.98G/9.71G [01:51<00:40, 66.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.99G/9.71G [01:51<00:42, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.96G/9.71G [01:47<00:50, 54.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.98G/9.71G [01:47<00:43, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 6.99G/9.71G [01:47<00:42, 63.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 7.01G/9.71G [01:47<00:37, 72.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 7.01G/9.71G [01:51<00:36, 74.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 7.03G/9.71G [01:51<00:40, 66.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.05G/9.71G [01:52<00:38, 69.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.06G/9.71G [01:52<00:46, 57.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  72%|███████▏  | 7.03G/9.71G [01:47<00:36, 73.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.05G/9.71G [01:48<00:38, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.06G/9.71G [01:48<00:44, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.08G/9.71G [01:48<00:39, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.08G/9.71G [01:52<00:38, 67.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.09G/9.71G [01:52<00:42, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.11G/9.71G [01:53<00:35, 73.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.13G/9.71G [01:53<00:32, 79.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.09G/9.71G [01:48<00:42, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.11G/9.71G [01:49<00:34, 75.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  73%|███████▎  | 7.13G/9.71G [01:49<00:32, 78.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▎  | 7.14G/9.71G [01:49<00:37, 67.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▎  | 7.14G/9.71G [01:53<00:38, 67.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.16G/9.71G [01:53<00:41, 61.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.17G/9.71G [01:54<00:46, 54.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.18G/9.71G [01:54<00:43, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.16G/9.71G [01:49<00:41, 60.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.17G/9.71G [01:50<00:47, 52.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.18G/9.71G [01:50<00:42, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.19G/9.71G [01:50<00:42, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.19G/9.71G [01:54<00:39, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.20G/9.71G [01:54<00:46, 53.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.22G/9.71G [01:54<00:37, 66.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.24G/9.71G [01:55<00:42, 58.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.20G/9.71G [01:50<00:46, 54.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.21G/9.71G [01:50<00:40, 61.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  74%|███████▍  | 7.22G/9.71G [01:51<00:37, 66.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.24G/9.71G [01:51<00:43, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.26G/9.71G [01:51<00:41, 59.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.26G/9.71G [01:55<00:41, 59.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.27G/9.71G [01:55<00:43, 56.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.29G/9.71G [01:56<00:42, 56.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.30G/9.71G [01:56<00:39, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.27G/9.71G [01:51<00:44, 55.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▍  | 7.28G/9.71G [01:51<00:38, 62.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.29G/9.71G [01:52<00:41, 58.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.30G/9.71G [01:52<00:36, 66.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.32G/9.71G [01:52<00:39, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.33G/9.71G [01:52<00:36, 65.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.35G/9.71G [01:52<00:30, 78.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.36G/9.71G [01:53<00:41, 56.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.37G/9.71G [01:53<00:53, 44.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.39G/9.71G [01:54<00:53, 43.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▋  | 7.41G/9.71G [01:54<00:44, 51.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  75%|███████▌  | 7.32G/9.71G [01:56<00:36, 66.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.33G/9.71G [01:56<00:36, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.35G/9.71G [01:57<00:34, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.36G/9.71G [01:57<00:40, 57.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.37G/9.71G [01:57<00:47, 49.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.38G/9.71G [01:57<00:42, 55.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▌  | 7.39G/9.71G [01:58<00:48, 47.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  76%|███████▋  | 7.41G/9.71G [01:58<00:46, 49.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.43G/9.71G [01:58<00:39, 57.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.44G/9.71G [01:58<00:42, 53.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.47G/9.71G [01:59<00:40, 55.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.43G/9.71G [01:54<00:39, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.44G/9.71G [01:54<00:38, 58.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.47G/9.71G [01:55<00:39, 56.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.48G/9.71G [01:55<00:39, 56.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.49G/9.71G [01:55<00:39, 56.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.48G/9.71G [01:59<00:41, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.50G/9.71G [01:59<00:34, 64.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.51G/9.71G [01:59<00:35, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.53G/9.71G [02:00<00:29, 74.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.54G/9.71G [02:00<00:31, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.55G/9.71G [02:00<00:30, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.50G/9.71G [01:55<00:37, 59.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  77%|███████▋  | 7.51G/9.71G [01:55<00:35, 61.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.53G/9.71G [01:56<00:30, 72.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.54G/9.71G [01:56<00:28, 76.1MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.55G/9.71G [01:56<00:28, 75.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.56G/9.71G [02:00<00:36, 59.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.57G/9.71G [02:01<00:42, 50.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.59G/9.71G [02:01<00:33, 63.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.56G/9.71G [01:56<00:34, 61.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.57G/9.71G [01:57<00:41, 51.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.59G/9.71G [01:57<00:34, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.60G/9.71G [01:57<00:39, 53.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  78%|███████▊  | 7.60G/9.71G [02:01<00:41, 50.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▊  | 7.62G/9.71G [02:01<00:34, 59.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▊  | 7.63G/9.71G [02:02<00:34, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.65G/9.71G [02:02<00:29, 70.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.67G/9.71G [02:02<00:29, 69.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▊  | 7.62G/9.71G [01:57<00:35, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▊  | 7.64G/9.71G [01:58<00:29, 69.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.65G/9.71G [01:58<00:29, 68.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.67G/9.71G [01:58<00:29, 69.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.68G/9.71G [01:58<00:35, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.68G/9.71G [02:02<00:34, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.69G/9.71G [02:02<00:32, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.70G/9.71G [02:02<00:30, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.72G/9.71G [02:03<00:31, 62.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.69G/9.71G [01:58<00:32, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  79%|███████▉  | 7.70G/9.71G [01:58<00:31, 64.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.72G/9.71G [01:59<00:32, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.73G/9.71G [01:59<00:41, 48.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.73G/9.71G [02:03<00:40, 48.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.75G/9.71G [02:04<00:40, 48.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.76G/9.71G [02:04<00:37, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.77G/9.71G [02:04<00:33, 58.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|███████▉  | 7.75G/9.71G [02:00<00:40, 48.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.77G/9.71G [02:00<00:32, 60.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.78G/9.71G [02:00<00:35, 54.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.78G/9.71G [02:04<00:37, 51.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.80G/9.71G [02:04<00:28, 66.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.81G/9.71G [02:04<00:27, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.83G/9.71G [02:05<00:23, 79.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.84G/9.71G [02:05<00:29, 63.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.80G/9.71G [02:00<00:31, 61.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  80%|████████  | 7.81G/9.71G [02:00<00:30, 62.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.82G/9.71G [02:01<00:29, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.83G/9.71G [02:01<00:28, 66.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.84G/9.71G [02:01<00:30, 61.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.86G/9.71G [02:05<00:26, 69.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.87G/9.71G [02:05<00:28, 63.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████▏ | 7.90G/9.71G [02:06<00:24, 72.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████▏ | 7.91G/9.71G [02:06<00:30, 60.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.86G/9.71G [02:01<00:26, 69.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████  | 7.87G/9.71G [02:01<00:26, 69.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████▏ | 7.90G/9.71G [02:02<00:23, 75.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  81%|████████▏ | 7.91G/9.71G [02:02<00:31, 56.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.92G/9.71G [02:02<00:30, 59.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.93G/9.71G [02:06<00:27, 64.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.94G/9.71G [02:06<00:32, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.96G/9.71G [02:07<00:23, 74.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.93G/9.71G [02:02<00:31, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.95G/9.71G [02:02<00:22, 78.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.96G/9.71G [02:03<00:24, 70.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.97G/9.71G [02:03<00:33, 52.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.97G/9.71G [02:07<00:32, 52.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.99G/9.71G [02:07<00:27, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 8.00G/9.71G [02:07<00:26, 63.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.02G/9.71G [02:08<00:21, 79.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.03G/9.71G [02:08<00:27, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 7.99G/9.71G [02:03<00:27, 62.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  82%|████████▏ | 8.00G/9.71G [02:03<00:26, 64.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.02G/9.71G [02:04<00:21, 77.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.03G/9.71G [02:04<00:26, 63.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.05G/9.71G [02:08<00:25, 63.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.06G/9.71G [02:08<00:24, 67.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.07G/9.71G [02:08<00:22, 72.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.08G/9.71G [02:09<00:23, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.11G/9.71G [02:09<00:24, 65.3MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▎ | 8.12G/9.71G [02:09<00:28, 55.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.14G/9.71G [02:10<00:25, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.15G/9.71G [02:10<00:29, 53.7MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.17G/9.71G [02:10<00:25, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.18G/9.71G [02:10<00:27, 56.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.20G/9.71G [02:11<00:22, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.21G/9.71G [02:11<00:23, 62.8MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.05G/9.71G [02:04<00:28, 57.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.06G/9.71G [02:04<00:26, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.08G/9.71G [02:05<00:26, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  83%|████████▎ | 8.11G/9.71G [02:05<00:22, 70.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▎ | 8.12G/9.71G [02:05<00:25, 62.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.14G/9.71G [02:06<00:24, 63.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.15G/9.71G [02:06<00:27, 56.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.17G/9.71G [02:06<00:25, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.18G/9.71G [02:06<00:25, 59.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  84%|████████▍ | 8.20G/9.71G [02:07<00:22, 67.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.21G/9.71G [02:07<00:23, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.23G/9.71G [02:07<00:22, 64.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.24G/9.71G [02:07<00:22, 64.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.23G/9.71G [02:11<00:25, 59.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▍ | 8.24G/9.71G [02:11<00:28, 50.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.26G/9.71G [02:12<00:21, 65.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.27G/9.71G [02:12<00:22, 64.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.26G/9.71G [02:08<00:23, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.27G/9.71G [02:08<00:27, 51.9MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.29G/9.71G [02:08<00:24, 58.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  85%|████████▌ | 8.29G/9.71G [02:12<00:21, 66.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.30G/9.71G [02:12<00:22, 62.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.33G/9.71G [02:13<00:22, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.34G/9.71G [02:13<00:24, 57.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.32G/9.71G [02:08<00:18, 74.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.33G/9.71G [02:09<00:23, 59.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.34G/9.71G [02:09<00:25, 54.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.36G/9.71G [02:09<00:20, 64.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▌ | 8.36G/9.71G [02:13<00:20, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▋ | 8.38G/9.71G [02:13<00:17, 74.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▋ | 8.39G/9.71G [02:14<00:21, 62.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.41G/9.71G [02:14<00:16, 76.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▋ | 8.38G/9.71G [02:09<00:19, 67.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  86%|████████▋ | 8.39G/9.71G [02:10<00:19, 66.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.41G/9.71G [02:10<00:17, 73.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.42G/9.71G [02:10<00:21, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.42G/9.71G [02:14<00:21, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.44G/9.71G [02:14<00:17, 70.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.45G/9.71G [02:14<00:19, 65.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.47G/9.71G [02:15<00:21, 58.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.44G/9.71G [02:10<00:17, 72.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.45G/9.71G [02:11<00:20, 62.1MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.47G/9.71G [02:11<00:21, 58.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.48G/9.71G [02:11<00:22, 54.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  87%|████████▋ | 8.48G/9.71G [02:15<00:22, 54.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.50G/9.71G [02:15<00:20, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.51G/9.71G [02:16<00:20, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.54G/9.71G [02:16<00:15, 73.6MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.50G/9.71G [02:11<00:20, 59.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.51G/9.71G [02:12<00:19, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.54G/9.71G [02:12<00:16, 72.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.55G/9.71G [02:12<00:16, 69.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.55G/9.71G [02:16<00:17, 67.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.57G/9.71G [02:16<00:15, 73.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.58G/9.71G [02:17<00:19, 58.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▊ | 8.60G/9.71G [02:17<00:17, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.57G/9.71G [02:12<00:17, 65.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  88%|████████▊ | 8.58G/9.71G [02:13<00:24, 47.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▊ | 8.60G/9.71G [02:13<00:19, 58.2MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▊ | 8.61G/9.71G [02:13<00:18, 58.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▊ | 8.61G/9.71G [02:17<00:20, 54.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.63G/9.71G [02:17<00:18, 58.5MB/s]\n",
      "#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.64G/9.71G [02:18<00:19, 54.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.65G/9.71G [02:18<00:20, 50.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.63G/9.71G [02:14<00:17, 61.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.64G/9.71G [02:14<00:17, 59.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.65G/9.71G [02:14<00:17, 60.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.66G/9.71G [02:14<00:19, 55.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.66G/9.71G [02:18<00:19, 54.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.68G/9.71G [02:18<00:15, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.69G/9.71G [02:19<00:18, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.71G/9.71G [02:19<00:16, 59.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  89%|████████▉ | 8.67G/9.71G [02:14<00:18, 55.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.69G/9.71G [02:15<00:17, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.71G/9.71G [02:15<00:15, 62.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.72G/9.71G [02:15<00:16, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|████████▉ | 8.72G/9.71G [02:19<00:17, 57.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.75G/9.71G [02:20<00:17, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.76G/9.71G [02:20<00:18, 51.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.75G/9.71G [02:16<00:16, 57.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.76G/9.71G [02:16<00:19, 50.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.78G/9.71G [02:16<00:16, 56.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  90%|█████████ | 8.78G/9.71G [02:20<00:16, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.79G/9.71G [02:20<00:16, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.81G/9.71G [02:21<00:13, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.82G/9.71G [02:21<00:13, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.84G/9.71G [02:21<00:12, 66.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.79G/9.71G [02:16<00:16, 57.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.81G/9.71G [02:17<00:13, 68.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.82G/9.71G [02:17<00:12, 70.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.84G/9.71G [02:17<00:12, 67.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.85G/9.71G [02:17<00:14, 58.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.86G/9.71G [02:17<00:15, 55.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.87G/9.71G [02:18<00:14, 59.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.88G/9.71G [02:18<00:13, 59.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.90G/9.71G [02:18<00:11, 71.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.91G/9.71G [02:18<00:11, 70.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.93G/9.71G [02:18<00:09, 80.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.94G/9.71G [02:19<00:13, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.97G/9.71G [02:19<00:10, 68.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.98G/9.71G [02:19<00:10, 71.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████ | 8.85G/9.71G [02:21<00:14, 57.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.86G/9.71G [02:21<00:15, 54.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.87G/9.71G [02:22<00:14, 56.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  91%|█████████▏| 8.88G/9.71G [02:22<00:14, 57.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.90G/9.71G [02:22<00:10, 77.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.91G/9.71G [02:22<00:11, 67.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.93G/9.71G [02:22<00:09, 84.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.94G/9.71G [02:23<00:13, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.97G/9.71G [02:23<00:10, 73.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  92%|█████████▏| 8.98G/9.71G [02:23<00:09, 76.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 8.99G/9.71G [02:23<00:10, 70.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.00G/9.71G [02:24<00:13, 54.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.02G/9.71G [02:24<00:12, 55.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 8.99G/9.71G [02:19<00:10, 66.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.00G/9.71G [02:19<00:10, 68.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.02G/9.71G [02:20<00:10, 64.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.03G/9.71G [02:20<00:14, 46.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.03G/9.71G [02:24<00:15, 45.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.05G/9.71G [02:24<00:11, 57.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.06G/9.71G [02:25<00:10, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▎| 9.08G/9.71G [02:25<00:08, 75.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.05G/9.71G [02:20<00:11, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  93%|█████████▎| 9.06G/9.71G [02:21<00:10, 59.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▎| 9.08G/9.71G [02:21<00:10, 62.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▎| 9.09G/9.71G [02:21<00:09, 64.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▎| 9.09G/9.71G [02:25<00:10, 60.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.11G/9.71G [02:25<00:08, 71.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.12G/9.71G [02:26<00:09, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.14G/9.71G [02:26<00:08, 68.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.11G/9.71G [02:21<00:08, 70.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.12G/9.71G [02:22<00:09, 61.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.14G/9.71G [02:22<00:08, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.15G/9.71G [02:22<00:09, 58.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.15G/9.71G [02:26<00:08, 63.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  94%|█████████▍| 9.16G/9.71G [02:26<00:07, 68.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.18G/9.71G [02:26<00:08, 65.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.19G/9.71G [02:27<00:09, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.18G/9.71G [02:22<00:07, 67.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.19G/9.71G [02:23<00:08, 58.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.21G/9.71G [02:23<00:09, 53.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.22G/9.71G [02:23<00:08, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.21G/9.71G [02:27<00:09, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▍| 9.22G/9.71G [02:27<00:09, 53.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.23G/9.71G [02:27<00:08, 53.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.24G/9.71G [02:28<00:09, 50.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.25G/9.71G [02:28<00:08, 51.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.23G/9.71G [02:23<00:09, 52.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.24G/9.71G [02:24<00:09, 51.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.25G/9.71G [02:24<00:08, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.27G/9.71G [02:24<00:07, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  95%|█████████▌| 9.27G/9.71G [02:28<00:06, 62.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.29G/9.71G [02:28<00:06, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.30G/9.71G [02:29<00:06, 64.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.32G/9.71G [02:29<00:06, 60.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.28G/9.71G [02:24<00:08, 50.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.29G/9.71G [02:25<00:07, 52.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.30G/9.71G [02:25<00:06, 58.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.31G/9.71G [02:25<00:05, 66.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.32G/9.71G [02:25<00:05, 68.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.33G/9.71G [02:29<00:07, 51.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▋| 9.35G/9.71G [02:30<00:06, 56.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▋| 9.36G/9.71G [02:30<00:06, 51.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▌| 9.33G/9.71G [02:25<00:07, 50.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▋| 9.35G/9.71G [02:26<00:06, 57.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  96%|█████████▋| 9.36G/9.71G [02:26<00:06, 55.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.38G/9.71G [02:26<00:05, 61.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.38G/9.71G [02:30<00:05, 58.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.40G/9.71G [02:30<00:05, 59.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.42G/9.71G [02:31<00:04, 58.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.43G/9.71G [02:31<00:05, 53.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.40G/9.71G [02:26<00:04, 64.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.42G/9.71G [02:27<00:04, 60.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.43G/9.71G [02:27<00:04, 61.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.45G/9.71G [02:27<00:03, 71.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.46G/9.71G [02:27<00:03, 64.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.45G/9.71G [02:31<00:03, 65.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  97%|█████████▋| 9.46G/9.71G [02:31<00:04, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.48G/9.71G [02:32<00:03, 72.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.49G/9.71G [02:32<00:03, 63.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.48G/9.71G [02:28<00:03, 61.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.49G/9.71G [02:28<00:03, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.51G/9.71G [02:28<00:03, 63.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.52G/9.71G [02:28<00:02, 66.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.51G/9.71G [02:32<00:03, 63.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.52G/9.71G [02:32<00:02, 66.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.53G/9.71G [02:32<00:02, 60.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.54G/9.71G [02:33<00:02, 61.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.55G/9.71G [02:33<00:02, 62.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.53G/9.71G [02:28<00:03, 57.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.54G/9.71G [02:29<00:02, 61.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  98%|█████████▊| 9.55G/9.71G [02:29<00:02, 63.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▊| 9.57G/9.71G [02:29<00:02, 56.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▊| 9.57G/9.71G [02:33<00:02, 58.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▊| 9.58G/9.71G [02:33<00:02, 57.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.59G/9.71G [02:34<00:02, 53.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.60G/9.71G [02:34<00:02, 48.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.63G/9.71G [02:34<00:01, 53.3MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.64G/9.71G [02:34<00:01, 53.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.66G/9.71G [02:35<00:00, 57.5MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.67G/9.71G [02:35<00:00, 62.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.69G/9.71G [02:35<00:00, 55.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.70G/9.71G [02:35<00:00, 56.7MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|██████████| 9.71G/9.71G [02:36<00:00, 62.2MB/s]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.04s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.04s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.04s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.05s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.05s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.04s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.05s/it]\n",
      "Downloading (…)00004-of-00005.bin\";:   0%|          | 0.00/9.71G [00:00<?, ?B/s]#033[A\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:10, 95.07s/it]\n",
      "Downloading (…)00004-of-00005.bin\";:   0%|          | 31.5M/9.71G [00:00<00:33, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   1%|          | 73.4M/9.71G [00:00<00:26, 363MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   1%|▏         | 126M/9.71G [00:00<00:24, 395MB/s] #033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▊| 9.58G/9.71G [02:29<00:02, 59.9MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.59G/9.71G [02:30<00:02, 48.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.60G/9.71G [02:30<00:01, 53.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.63G/9.71G [02:30<00:01, 55.8MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.64G/9.71G [02:30<00:01, 54.0MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";:  99%|█████████▉| 9.66G/9.71G [02:31<00:00, 59.1MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.67G/9.71G [02:31<00:00, 62.2MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.69G/9.71G [02:31<00:00, 55.4MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|█████████▉| 9.70G/9.71G [02:31<00:00, 55.6MB/s]#033[A\n",
      "Downloading (…)00003-of-00005.bin\";: 100%|██████████| 9.71G/9.71G [02:32<00:00, 63.9MB/s]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.47s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.45s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.46s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.47s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.46s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.47s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:06, 93.47s/it]\n",
      "Downloading shards:  60%|██████    | 3/5 [03:58<03:07, 93.51s/it]\n",
      "Downloading (…)00004-of-00005.bin\";:   0%|          | 0.00/9.71G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   0%|          | 21.0M/9.71G [00:00<01:14, 131MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   0%|          | 41.9M/9.71G [00:00<00:57, 167MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   1%|          | 73.4M/9.71G [00:00<00:46, 208MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   1%|          | 105M/9.71G [00:00<00:40, 237MB/s] #033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   2%|▏         | 178M/9.71G [00:00<00:23, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   2%|▏         | 231M/9.71G [00:00<00:22, 420MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   3%|▎         | 283M/9.71G [00:00<00:21, 429MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   3%|▎         | 336M/9.71G [00:00<00:21, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   4%|▍         | 388M/9.71G [00:00<00:21, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   5%|▍         | 440M/9.71G [00:01<00:21, 436MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   5%|▌         | 493M/9.71G [00:01<00:21, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   6%|▌         | 545M/9.71G [00:01<00:20, 438MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   1%|▏         | 136M/9.71G [00:00<00:37, 253MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   2%|▏         | 168M/9.71G [00:00<00:38, 247MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   2%|▏         | 199M/9.71G [00:00<00:39, 240MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   2%|▏         | 231M/9.71G [00:01<00:39, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   3%|▎         | 262M/9.71G [00:01<00:38, 245MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   3%|▎         | 294M/9.71G [00:01<00:40, 233MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   3%|▎         | 325M/9.71G [00:01<00:39, 235MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   4%|▎         | 357M/9.71G [00:01<00:38, 243MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   6%|▌         | 598M/9.71G [00:01<00:20, 438MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   7%|▋         | 650M/9.71G [00:01<00:20, 433MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   7%|▋         | 703M/9.71G [00:01<00:20, 434MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   8%|▊         | 755M/9.71G [00:01<00:20, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   8%|▊         | 807M/9.71G [00:01<00:20, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   9%|▉         | 860M/9.71G [00:02<00:21, 419MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   9%|▉         | 902M/9.71G [00:02<00:21, 412MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|▉         | 944M/9.71G [00:02<00:21, 413MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|█         | 986M/9.71G [00:02<00:21, 412MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   4%|▍         | 388M/9.71G [00:01<00:37, 249MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   4%|▍         | 419M/9.71G [00:01<00:35, 258MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   5%|▍         | 451M/9.71G [00:01<00:35, 260MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   5%|▍         | 482M/9.71G [00:02<00:35, 259MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   5%|▌         | 514M/9.71G [00:02<00:35, 257MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   6%|▌         | 545M/9.71G [00:02<00:36, 254MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   6%|▌         | 577M/9.71G [00:02<00:35, 257MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   6%|▋         | 608M/9.71G [00:02<00:34, 261MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  11%|█         | 1.03G/9.71G [00:02<00:21, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  11%|█         | 1.08G/9.71G [00:02<00:20, 423MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  12%|█▏        | 1.13G/9.71G [00:02<00:20, 429MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  12%|█▏        | 1.18G/9.71G [00:02<00:19, 431MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  13%|█▎        | 1.24G/9.71G [00:02<00:19, 425MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  13%|█▎        | 1.29G/9.71G [00:03<00:21, 383MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  14%|█▍        | 1.34G/9.71G [00:03<00:20, 402MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  14%|█▍        | 1.39G/9.71G [00:03<00:20, 410MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   7%|▋         | 640M/9.71G [00:02<00:34, 263MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   7%|▋         | 671M/9.71G [00:02<00:33, 268MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   7%|▋         | 703M/9.71G [00:02<00:32, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   8%|▊         | 734M/9.71G [00:02<00:32, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   8%|▊         | 765M/9.71G [00:03<00:32, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   8%|▊         | 797M/9.71G [00:03<00:32, 278MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   9%|▊         | 828M/9.71G [00:03<00:33, 269MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   9%|▉         | 860M/9.71G [00:03<00:32, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:   9%|▉         | 891M/9.71G [00:03<00:33, 262MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  15%|█▍        | 1.45G/9.71G [00:03<00:19, 415MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  15%|█▌        | 1.49G/9.71G [00:03<00:21, 381MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  16%|█▌        | 1.53G/9.71G [00:03<00:21, 374MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  16%|█▋        | 1.58G/9.71G [00:03<00:20, 394MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  17%|█▋        | 1.64G/9.71G [00:03<00:20, 404MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  17%|█▋        | 1.69G/9.71G [00:04<00:19, 413MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  18%|█▊        | 1.74G/9.71G [00:04<00:19, 418MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  18%|█▊        | 1.78G/9.71G [00:04<00:22, 350MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|▉         | 923M/9.71G [00:03<00:34, 256MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|▉         | 954M/9.71G [00:03<00:34, 251MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|█         | 986M/9.71G [00:03<00:34, 255MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  10%|█         | 1.02G/9.71G [00:04<00:33, 257MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  11%|█         | 1.05G/9.71G [00:04<00:33, 261MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  11%|█         | 1.08G/9.71G [00:04<00:33, 258MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  11%|█▏        | 1.11G/9.71G [00:04<00:32, 265MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  12%|█▏        | 1.14G/9.71G [00:04<00:31, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  19%|█▉        | 1.82G/9.71G [00:04<00:22, 357MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  19%|█▉        | 1.88G/9.71G [00:04<00:20, 378MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  20%|█▉        | 1.93G/9.71G [00:04<00:19, 397MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  20%|██        | 1.98G/9.71G [00:04<00:18, 410MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██        | 2.02G/9.71G [00:04<00:18, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██▏       | 2.08G/9.71G [00:05<00:18, 422MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  22%|██▏       | 2.13G/9.71G [00:05<00:17, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  22%|██▏       | 2.18G/9.71G [00:05<00:17, 434MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  12%|█▏        | 1.17G/9.71G [00:04<00:31, 270MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  12%|█▏        | 1.21G/9.71G [00:04<00:30, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  13%|█▎        | 1.24G/9.71G [00:04<00:30, 277MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  13%|█▎        | 1.27G/9.71G [00:04<00:30, 277MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  13%|█▎        | 1.30G/9.71G [00:05<00:30, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  14%|█▎        | 1.33G/9.71G [00:05<00:30, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  14%|█▍        | 1.36G/9.71G [00:05<00:31, 267MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  14%|█▍        | 1.39G/9.71G [00:05<00:30, 269MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  15%|█▍        | 1.43G/9.71G [00:05<00:31, 267MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  23%|██▎       | 2.23G/9.71G [00:05<00:17, 439MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  24%|██▎       | 2.29G/9.71G [00:05<00:17, 434MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  24%|██▍       | 2.34G/9.71G [00:05<00:17, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  25%|██▍       | 2.39G/9.71G [00:05<00:17, 426MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  25%|██▌       | 2.44G/9.71G [00:05<00:17, 426MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  26%|██▌       | 2.50G/9.71G [00:06<00:17, 418MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  26%|██▌       | 2.54G/9.71G [00:06<00:17, 418MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  27%|██▋       | 2.58G/9.71G [00:06<00:17, 417MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  27%|██▋       | 2.63G/9.71G [00:06<00:16, 417MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  15%|█▌        | 1.46G/9.71G [00:05<00:31, 262MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  15%|█▌        | 1.49G/9.71G [00:05<00:30, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  16%|█▌        | 1.52G/9.71G [00:05<00:30, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  16%|█▌        | 1.55G/9.71G [00:05<00:29, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  16%|█▋        | 1.58G/9.71G [00:06<00:30, 270MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  17%|█▋        | 1.61G/9.71G [00:06<00:29, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  17%|█▋        | 1.65G/9.71G [00:06<00:30, 268MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  17%|█▋        | 1.68G/9.71G [00:06<00:30, 260MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  28%|██▊       | 2.68G/9.71G [00:06<00:16, 421MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  28%|██▊       | 2.74G/9.71G [00:06<00:16, 422MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  29%|██▊       | 2.79G/9.71G [00:06<00:16, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  29%|██▉       | 2.84G/9.71G [00:06<00:16, 422MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  30%|██▉       | 2.89G/9.71G [00:06<00:16, 419MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  30%|███       | 2.95G/9.71G [00:07<00:16, 418MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  31%|███       | 2.99G/9.71G [00:07<00:16, 412MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  31%|███       | 3.03G/9.71G [00:07<00:16, 409MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  18%|█▊        | 1.71G/9.71G [00:06<00:31, 251MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  18%|█▊        | 1.74G/9.71G [00:06<00:32, 248MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  18%|█▊        | 1.77G/9.71G [00:06<00:31, 251MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  19%|█▊        | 1.80G/9.71G [00:06<00:31, 253MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  19%|█▉        | 1.84G/9.71G [00:07<00:30, 257MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  19%|█▉        | 1.87G/9.71G [00:07<00:30, 260MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  20%|█▉        | 1.90G/9.71G [00:07<00:29, 265MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  20%|█▉        | 1.93G/9.71G [00:07<00:29, 268MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  20%|██        | 1.96G/9.71G [00:07<00:29, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.07G/9.71G [00:07<00:16, 406MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.11G/9.71G [00:07<00:16, 395MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.16G/9.71G [00:07<00:16, 400MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  33%|███▎      | 3.20G/9.71G [00:07<00:16, 404MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  33%|███▎      | 3.24G/9.71G [00:07<00:15, 408MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  34%|███▍      | 3.28G/9.71G [00:07<00:15, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  34%|███▍      | 3.32G/9.71G [00:08<00:15, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  35%|███▍      | 3.37G/9.71G [00:08<00:15, 412MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  35%|███▌      | 3.41G/9.71G [00:08<00:15, 414MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  36%|███▌      | 3.46G/9.71G [00:08<00:14, 417MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██        | 1.99G/9.71G [00:07<00:28, 269MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██        | 2.02G/9.71G [00:07<00:28, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██        | 2.06G/9.71G [00:07<00:28, 272MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  21%|██▏       | 2.09G/9.71G [00:08<00:27, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  22%|██▏       | 2.12G/9.71G [00:08<00:27, 272MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  22%|██▏       | 2.15G/9.71G [00:08<00:27, 272MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  22%|██▏       | 2.18G/9.71G [00:08<00:27, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  23%|██▎       | 2.21G/9.71G [00:08<00:26, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  23%|██▎       | 2.24G/9.71G [00:08<00:25, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  36%|███▌      | 3.51G/9.71G [00:08<00:14, 421MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  37%|███▋      | 3.57G/9.71G [00:08<00:14, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  37%|███▋      | 3.62G/9.71G [00:08<00:14, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  38%|███▊      | 3.67G/9.71G [00:08<00:14, 429MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  38%|███▊      | 3.72G/9.71G [00:08<00:13, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  39%|███▉      | 3.77G/9.71G [00:09<00:13, 425MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  39%|███▉      | 3.83G/9.71G [00:09<00:13, 424MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|███▉      | 3.88G/9.71G [00:09<00:13, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  23%|██▎       | 2.28G/9.71G [00:08<00:25, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  24%|██▍       | 2.31G/9.71G [00:08<00:25, 295MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  24%|██▍       | 2.34G/9.71G [00:08<00:25, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  24%|██▍       | 2.37G/9.71G [00:09<00:24, 294MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  25%|██▍       | 2.40G/9.71G [00:09<00:25, 286MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  25%|██▌       | 2.43G/9.71G [00:09<00:25, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  25%|██▌       | 2.46G/9.71G [00:09<00:25, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  26%|██▌       | 2.50G/9.71G [00:09<00:25, 282MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  26%|██▌       | 2.53G/9.71G [00:09<00:25, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|████      | 3.93G/9.71G [00:09<00:13, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  41%|████      | 3.98G/9.71G [00:09<00:13, 435MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  42%|████▏     | 4.04G/9.71G [00:09<00:13, 436MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  42%|████▏     | 4.09G/9.71G [00:09<00:12, 437MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  43%|████▎     | 4.14G/9.71G [00:09<00:12, 438MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  43%|████▎     | 4.19G/9.71G [00:10<00:12, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▎     | 4.25G/9.71G [00:10<00:12, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▍     | 4.30G/9.71G [00:10<00:12, 429MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  26%|██▋       | 2.56G/9.71G [00:09<00:25, 283MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  27%|██▋       | 2.59G/9.71G [00:09<00:24, 291MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  27%|██▋       | 2.62G/9.71G [00:09<00:24, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  27%|██▋       | 2.65G/9.71G [00:10<00:24, 291MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  28%|██▊       | 2.68G/9.71G [00:10<00:24, 286MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  28%|██▊       | 2.72G/9.71G [00:10<00:37, 186MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  28%|██▊       | 2.75G/9.71G [00:10<00:34, 204MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  29%|██▊       | 2.78G/9.71G [00:10<00:30, 227MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  29%|██▉       | 2.81G/9.71G [00:10<00:28, 245MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  29%|██▉       | 2.84G/9.71G [00:10<00:26, 255MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  30%|██▉       | 2.87G/9.71G [00:10<00:25, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  30%|██▉       | 2.90G/9.71G [00:11<00:24, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  30%|███       | 2.94G/9.71G [00:11<00:24, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  31%|███       | 2.97G/9.71G [00:11<00:23, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  31%|███       | 3.01G/9.71G [00:11<00:22, 292MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  31%|███▏      | 3.05G/9.71G [00:11<00:22, 299MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.08G/9.71G [00:11<00:22, 300MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.11G/9.71G [00:11<00:21, 304MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  32%|███▏      | 3.16G/9.71G [00:11<00:21, 311MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  33%|███▎      | 3.20G/9.71G [00:12<00:20, 312MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  33%|███▎      | 3.23G/9.71G [00:12<00:20, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  34%|███▎      | 3.26G/9.71G [00:12<00:21, 300MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  34%|███▍      | 3.29G/9.71G [00:12<00:21, 297MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  34%|███▍      | 3.32G/9.71G [00:12<00:21, 297MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  35%|███▍      | 3.36G/9.71G [00:12<00:21, 295MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  45%|████▍     | 4.35G/9.71G [00:10<00:12, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  45%|████▌     | 4.40G/9.71G [00:10<00:12, 431MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  46%|████▌     | 4.46G/9.71G [00:10<00:12, 426MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  46%|████▋     | 4.51G/9.71G [00:10<00:12, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  47%|████▋     | 4.56G/9.71G [00:10<00:12, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  48%|████▊     | 4.61G/9.71G [00:11<00:11, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  48%|████▊     | 4.67G/9.71G [00:11<00:11, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  49%|████▊     | 4.72G/9.71G [00:11<00:11, 430MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  49%|████▉     | 4.77G/9.71G [00:11<00:11, 421MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  50%|████▉     | 4.82G/9.71G [00:11<00:11, 420MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  50%|█████     | 4.88G/9.71G [00:11<00:11, 419MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  51%|█████     | 4.93G/9.71G [00:11<00:11, 420MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  51%|█████▏    | 4.98G/9.71G [00:11<00:11, 417MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  52%|█████▏    | 5.02G/9.71G [00:12<00:11, 414MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  52%|█████▏    | 5.06G/9.71G [00:12<00:11, 415MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  53%|█████▎    | 5.12G/9.71G [00:12<00:10, 424MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  53%|█████▎    | 5.17G/9.71G [00:12<00:10, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  54%|█████▍    | 5.22G/9.71G [00:12<00:10, 429MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  54%|█████▍    | 5.27G/9.71G [00:12<00:10, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  55%|█████▍    | 5.33G/9.71G [00:12<00:10, 433MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  55%|█████▌    | 5.38G/9.71G [00:12<00:09, 437MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  56%|█████▌    | 5.43G/9.71G [00:12<00:09, 441MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  56%|█████▋    | 5.48G/9.71G [00:13<00:09, 441MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  57%|█████▋    | 5.54G/9.71G [00:13<00:09, 441MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  35%|███▍      | 3.39G/9.71G [00:12<00:21, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  35%|███▌      | 3.42G/9.71G [00:12<00:21, 294MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  36%|███▌      | 3.45G/9.71G [00:12<00:21, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  36%|███▌      | 3.48G/9.71G [00:13<00:21, 289MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  36%|███▋      | 3.52G/9.71G [00:13<00:20, 296MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  37%|███▋      | 3.55G/9.71G [00:13<00:21, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  37%|███▋      | 3.59G/9.71G [00:13<00:21, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  37%|███▋      | 3.62G/9.71G [00:13<00:21, 289MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  38%|███▊      | 3.65G/9.71G [00:13<00:21, 286MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  58%|█████▊    | 5.59G/9.71G [00:13<00:14, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  58%|█████▊    | 5.64G/9.71G [00:13<00:12, 320MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  59%|█████▊    | 5.69G/9.71G [00:13<00:11, 349MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  59%|█████▉    | 5.75G/9.71G [00:13<00:10, 372MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  60%|█████▉    | 5.80G/9.71G [00:14<00:10, 390MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  60%|██████    | 5.85G/9.71G [00:14<00:09, 404MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  61%|██████    | 5.90G/9.71G [00:14<00:09, 416MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  61%|██████▏   | 5.96G/9.71G [00:14<00:09, 403MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  38%|███▊      | 3.68G/9.71G [00:13<00:21, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  38%|███▊      | 3.71G/9.71G [00:13<00:21, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  39%|███▊      | 3.74G/9.71G [00:13<00:22, 263MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  39%|███▉      | 3.77G/9.71G [00:14<00:22, 268MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  39%|███▉      | 3.81G/9.71G [00:14<00:21, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|███▉      | 3.84G/9.71G [00:14<00:21, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|███▉      | 3.87G/9.71G [00:14<00:21, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|████      | 3.90G/9.71G [00:14<00:20, 277MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  62%|██████▏   | 6.00G/9.71G [00:14<00:09, 404MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  62%|██████▏   | 6.05G/9.71G [00:14<00:08, 410MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  63%|██████▎   | 6.10G/9.71G [00:14<00:08, 420MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  63%|██████▎   | 6.16G/9.71G [00:14<00:08, 425MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  64%|██████▍   | 6.21G/9.71G [00:14<00:08, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  64%|██████▍   | 6.26G/9.71G [00:15<00:08, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  65%|██████▍   | 6.31G/9.71G [00:15<00:10, 339MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  40%|████      | 3.93G/9.71G [00:14<00:20, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  41%|████      | 3.96G/9.71G [00:14<00:20, 283MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  41%|████      | 4.00G/9.71G [00:14<00:20, 276MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  41%|████▏     | 4.03G/9.71G [00:14<00:20, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  42%|████▏     | 4.06G/9.71G [00:15<00:20, 282MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  42%|████▏     | 4.09G/9.71G [00:15<00:19, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  42%|████▏     | 4.12G/9.71G [00:15<00:19, 292MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  43%|████▎     | 4.16G/9.71G [00:15<00:18, 302MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  43%|████▎     | 4.19G/9.71G [00:15<00:18, 304MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  65%|██████▌   | 6.35G/9.71G [00:15<00:09, 352MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  66%|██████▌   | 6.41G/9.71G [00:15<00:08, 375MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.46G/9.71G [00:15<00:08, 391MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.51G/9.71G [00:15<00:07, 403MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  68%|██████▊   | 6.56G/9.71G [00:15<00:07, 410MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  68%|██████▊   | 6.61G/9.71G [00:16<00:07, 411MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  69%|██████▊   | 6.66G/9.71G [00:16<00:07, 414MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  69%|██████▉   | 6.71G/9.71G [00:16<00:07, 419MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  70%|██████▉   | 6.76G/9.71G [00:16<00:07, 421MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▎     | 4.23G/9.71G [00:15<00:18, 298MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▍     | 4.26G/9.71G [00:15<00:18, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▍     | 4.29G/9.71G [00:15<00:18, 299MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  44%|████▍     | 4.32G/9.71G [00:15<00:17, 300MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  45%|████▍     | 4.35G/9.71G [00:16<00:17, 301MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  45%|████▌     | 4.38G/9.71G [00:16<00:17, 302MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  45%|████▌     | 4.41G/9.71G [00:16<00:18, 294MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  46%|████▌     | 4.45G/9.71G [00:16<00:17, 296MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  46%|████▌     | 4.48G/9.71G [00:16<00:17, 296MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  46%|████▋     | 4.51G/9.71G [00:16<00:17, 300MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  70%|███████   | 6.82G/9.71G [00:16<00:06, 423MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  71%|███████   | 6.87G/9.71G [00:16<00:06, 425MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  71%|███████▏  | 6.92G/9.71G [00:16<00:06, 427MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  72%|███████▏  | 6.97G/9.71G [00:16<00:06, 428MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  72%|███████▏  | 7.03G/9.71G [00:16<00:06, 432MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  73%|███████▎  | 7.08G/9.71G [00:17<00:06, 436MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  73%|███████▎  | 7.13G/9.71G [00:17<00:05, 441MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  74%|███████▍  | 7.18G/9.71G [00:17<00:05, 444MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  47%|████▋     | 4.54G/9.71G [00:16<00:17, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  47%|████▋     | 4.57G/9.71G [00:16<00:16, 305MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  47%|████▋     | 4.60G/9.71G [00:16<00:16, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  48%|████▊     | 4.63G/9.71G [00:16<00:16, 304MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  48%|████▊     | 4.68G/9.71G [00:17<00:16, 314MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  48%|████▊     | 4.71G/9.71G [00:17<00:16, 311MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  49%|████▉     | 4.75G/9.71G [00:17<00:15, 314MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  49%|████▉     | 4.78G/9.71G [00:17<00:16, 304MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  50%|████▉     | 4.81G/9.71G [00:17<00:16, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  75%|███████▍  | 7.24G/9.71G [00:17<00:05, 446MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  75%|███████▌  | 7.29G/9.71G [00:17<00:05, 447MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  76%|███████▌  | 7.34G/9.71G [00:17<00:05, 450MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  76%|███████▌  | 7.39G/9.71G [00:17<00:05, 449MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  77%|███████▋  | 7.44G/9.71G [00:17<00:05, 448MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  77%|███████▋  | 7.50G/9.71G [00:18<00:04, 446MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.55G/9.71G [00:18<00:04, 444MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.60G/9.71G [00:18<00:04, 449MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  79%|███████▉  | 7.65G/9.71G [00:18<00:04, 459MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  50%|████▉     | 4.84G/9.71G [00:17<00:16, 289MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  50%|█████     | 4.88G/9.71G [00:17<00:17, 284MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  51%|█████     | 4.91G/9.71G [00:17<00:16, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  51%|█████     | 4.94G/9.71G [00:18<00:16, 292MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  51%|█████▏    | 4.98G/9.71G [00:18<00:15, 302MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  52%|█████▏    | 5.01G/9.71G [00:18<00:15, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  52%|█████▏    | 5.05G/9.71G [00:18<00:15, 307MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  52%|█████▏    | 5.09G/9.71G [00:18<00:15, 302MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  53%|█████▎    | 5.12G/9.71G [00:18<00:15, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  79%|███████▉  | 7.71G/9.71G [00:18<00:04, 469MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  80%|███████▉  | 7.76G/9.71G [00:18<00:04, 473MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  80%|████████  | 7.81G/9.71G [00:18<00:03, 477MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  81%|████████  | 7.86G/9.71G [00:18<00:03, 480MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  82%|████████▏ | 7.92G/9.71G [00:18<00:03, 480MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  82%|████████▏ | 7.97G/9.71G [00:19<00:03, 483MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  83%|████████▎ | 8.02G/9.71G [00:19<00:03, 481MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  83%|████████▎ | 8.07G/9.71G [00:19<00:03, 483MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  84%|████████▎ | 8.13G/9.71G [00:19<00:03, 487MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  53%|█████▎    | 5.16G/9.71G [00:18<00:14, 309MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  53%|█████▎    | 5.19G/9.71G [00:18<00:14, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  54%|█████▍    | 5.22G/9.71G [00:18<00:14, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  54%|█████▍    | 5.26G/9.71G [00:19<00:14, 313MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  55%|█████▍    | 5.30G/9.71G [00:19<00:14, 312MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  55%|█████▍    | 5.34G/9.71G [00:19<00:13, 315MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  55%|█████▌    | 5.37G/9.71G [00:19<00:13, 312MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  56%|█████▌    | 5.40G/9.71G [00:19<00:13, 308MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  56%|█████▌    | 5.43G/9.71G [00:19<00:14, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  84%|████████▍ | 8.18G/9.71G [00:19<00:03, 492MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  85%|████████▍ | 8.23G/9.71G [00:19<00:03, 489MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  85%|████████▌ | 8.28G/9.71G [00:19<00:02, 491MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  86%|████████▌ | 8.34G/9.71G [00:19<00:02, 495MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  86%|████████▋ | 8.39G/9.71G [00:19<00:02, 493MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  87%|████████▋ | 8.44G/9.71G [00:19<00:02, 491MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  87%|████████▋ | 8.49G/9.71G [00:20<00:02, 489MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  88%|████████▊ | 8.55G/9.71G [00:20<00:02, 490MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  89%|████████▊ | 8.60G/9.71G [00:20<00:02, 489MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  56%|█████▋    | 5.46G/9.71G [00:19<00:14, 303MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  57%|█████▋    | 5.51G/9.71G [00:19<00:13, 309MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  57%|█████▋    | 5.54G/9.71G [00:19<00:13, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  57%|█████▋    | 5.57G/9.71G [00:20<00:13, 311MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  58%|█████▊    | 5.60G/9.71G [00:20<00:13, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  58%|█████▊    | 5.63G/9.71G [00:20<00:13, 308MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  58%|█████▊    | 5.67G/9.71G [00:20<00:13, 310MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  59%|█████▊    | 5.70G/9.71G [00:20<00:20, 195MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  59%|█████▉    | 5.74G/9.71G [00:20<00:18, 214MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  59%|█████▉    | 5.77G/9.71G [00:20<00:17, 231MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  60%|█████▉    | 5.80G/9.71G [00:21<00:15, 246MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  60%|██████    | 5.83G/9.71G [00:21<00:14, 261MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  60%|██████    | 5.86G/9.71G [00:21<00:14, 269MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  61%|██████    | 5.89G/9.71G [00:21<00:13, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  61%|██████    | 5.92G/9.71G [00:21<00:13, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  61%|██████▏   | 5.96G/9.71G [00:21<00:12, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  62%|██████▏   | 5.99G/9.71G [00:21<00:12, 291MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  62%|██████▏   | 6.02G/9.71G [00:21<00:13, 283MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  62%|██████▏   | 6.05G/9.71G [00:21<00:12, 284MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  63%|██████▎   | 6.08G/9.71G [00:21<00:12, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  63%|██████▎   | 6.11G/9.71G [00:22<00:13, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  63%|██████▎   | 6.14G/9.71G [00:22<00:13, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  64%|██████▎   | 6.18G/9.71G [00:22<00:13, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  64%|██████▍   | 6.21G/9.71G [00:22<00:13, 267MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  64%|██████▍   | 6.24G/9.71G [00:22<00:13, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  89%|████████▉ | 8.65G/9.71G [00:20<00:02, 489MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  90%|████████▉ | 8.70G/9.71G [00:20<00:02, 489MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  90%|█████████ | 8.76G/9.71G [00:20<00:01, 484MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  91%|█████████ | 8.81G/9.71G [00:20<00:01, 480MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  91%|█████████ | 8.86G/9.71G [00:20<00:01, 473MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  92%|█████████▏| 8.91G/9.71G [00:20<00:01, 471MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  92%|█████████▏| 8.97G/9.71G [00:21<00:01, 467MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  93%|█████████▎| 9.02G/9.71G [00:21<00:01, 468MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  93%|█████████▎| 9.07G/9.71G [00:21<00:01, 467MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▍| 9.12G/9.71G [00:21<00:01, 470MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▍| 9.18G/9.71G [00:21<00:01, 474MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  95%|█████████▌| 9.23G/9.71G [00:21<00:01, 481MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  96%|█████████▌| 9.28G/9.71G [00:21<00:00, 481MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  96%|█████████▌| 9.33G/9.71G [00:21<00:00, 485MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  97%|█████████▋| 9.38G/9.71G [00:21<00:00, 488MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  97%|█████████▋| 9.44G/9.71G [00:22<00:00, 486MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  98%|█████████▊| 9.49G/9.71G [00:22<00:00, 487MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  98%|█████████▊| 9.54G/9.71G [00:22<00:00, 492MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  99%|█████████▉| 9.59G/9.71G [00:22<00:00, 492MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  99%|█████████▉| 9.65G/9.71G [00:22<00:00, 493MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";: 100%|█████████▉| 9.70G/9.71G [00:22<00:00, 494MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";: 100%|██████████| 9.71G/9.71G [00:22<00:00, 429MB/s]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.49s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.48s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.48s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.48s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.48s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.49s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.49s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:21<01:06, 66.49s/it]\n",
      "Downloading (…)00005-of-00005.bin\";:   0%|          | 0.00/2.13G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   2%|▏         | 41.9M/2.13G [00:00<00:05, 368MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   4%|▍         | 83.9M/2.13G [00:00<00:05, 392MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   6%|▌         | 126M/2.13G [00:00<00:05, 396MB/s] #033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   8%|▊         | 168M/2.13G [00:00<00:05, 382MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  10%|▉         | 210M/2.13G [00:00<00:05, 379MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  12%|█▏        | 252M/2.13G [00:00<00:05, 357MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  65%|██████▍   | 6.27G/9.71G [00:22<00:14, 236MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  65%|██████▍   | 6.30G/9.71G [00:22<00:14, 232MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  65%|██████▌   | 6.33G/9.71G [00:23<00:14, 240MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  66%|██████▌   | 6.36G/9.71G [00:23<00:14, 235MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  66%|██████▌   | 6.40G/9.71G [00:23<00:13, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  66%|██████▌   | 6.43G/9.71G [00:23<00:13, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.46G/9.71G [00:23<00:14, 231MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  14%|█▍        | 294M/2.13G [00:00<00:05, 350MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  16%|█▌        | 336M/2.13G [00:00<00:05, 355MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  18%|█▊        | 377M/2.13G [00:01<00:05, 351MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  20%|█▉        | 419M/2.13G [00:01<00:05, 341MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  22%|██▏       | 461M/2.13G [00:01<00:04, 353MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  24%|██▎       | 503M/2.13G [00:01<00:04, 353MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  26%|██▌       | 545M/2.13G [00:01<00:04, 363MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  28%|██▊       | 587M/2.13G [00:01<00:04, 378MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.49G/9.71G [00:23<00:13, 231MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.52G/9.71G [00:23<00:13, 232MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  67%|██████▋   | 6.55G/9.71G [00:23<00:13, 237MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  68%|██████▊   | 6.59G/9.71G [00:24<00:12, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  68%|██████▊   | 6.62G/9.71G [00:24<00:12, 243MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  68%|██████▊   | 6.65G/9.71G [00:24<00:12, 246MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  69%|██████▉   | 6.68G/9.71G [00:24<00:12, 249MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  69%|██████▉   | 6.71G/9.71G [00:24<00:12, 249MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  29%|██▉       | 629M/2.13G [00:01<00:04, 369MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  31%|███▏      | 671M/2.13G [00:01<00:04, 357MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  33%|███▎      | 713M/2.13G [00:01<00:04, 345MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  36%|███▌      | 765M/2.13G [00:02<00:03, 370MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  38%|███▊      | 807M/2.13G [00:02<00:03, 377MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  40%|███▉      | 849M/2.13G [00:02<00:03, 381MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  42%|████▏     | 891M/2.13G [00:02<00:03, 382MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  44%|████▎     | 933M/2.13G [00:02<00:03, 386MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  46%|████▌     | 975M/2.13G [00:02<00:03, 367MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  69%|██████▉   | 6.74G/9.71G [00:24<00:12, 241MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  70%|██████▉   | 6.77G/9.71G [00:24<00:12, 244MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  70%|███████   | 6.81G/9.71G [00:24<00:11, 249MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  70%|███████   | 6.84G/9.71G [00:25<00:11, 255MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  71%|███████   | 6.87G/9.71G [00:25<00:11, 252MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  71%|███████   | 6.90G/9.71G [00:25<00:11, 250MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  71%|███████▏  | 6.93G/9.71G [00:25<00:11, 252MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  72%|███████▏  | 6.96G/9.71G [00:25<00:11, 242MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  48%|████▊     | 1.02G/2.13G [00:02<00:03, 360MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  50%|████▉     | 1.06G/2.13G [00:02<00:03, 345MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  52%|█████▏    | 1.10G/2.13G [00:03<00:03, 341MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  54%|█████▎    | 1.14G/2.13G [00:03<00:02, 348MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  56%|█████▌    | 1.18G/2.13G [00:03<00:02, 362MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  57%|█████▋    | 1.23G/2.13G [00:03<00:02, 354MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  59%|█████▉    | 1.27G/2.13G [00:03<00:02, 356MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  61%|██████▏   | 1.31G/2.13G [00:03<00:02, 352MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  72%|███████▏  | 6.99G/9.71G [00:25<00:11, 238MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  72%|███████▏  | 7.03G/9.71G [00:25<00:11, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  73%|███████▎  | 7.06G/9.71G [00:26<00:10, 247MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  73%|███████▎  | 7.09G/9.71G [00:26<00:10, 243MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  73%|███████▎  | 7.12G/9.71G [00:26<00:10, 241MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  74%|███████▎  | 7.15G/9.71G [00:26<00:10, 240MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  74%|███████▍  | 7.18G/9.71G [00:26<00:10, 235MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  63%|██████▎   | 1.35G/2.13G [00:03<00:02, 352MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  65%|██████▌   | 1.39G/2.13G [00:03<00:02, 357MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  67%|██████▋   | 1.44G/2.13G [00:03<00:01, 370MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  70%|██████▉   | 1.49G/2.13G [00:04<00:01, 394MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  72%|███████▏  | 1.53G/2.13G [00:04<00:01, 400MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  74%|███████▎  | 1.57G/2.13G [00:04<00:01, 396MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  76%|███████▌  | 1.61G/2.13G [00:04<00:01, 390MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  78%|███████▊  | 1.66G/2.13G [00:04<00:01, 374MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  74%|███████▍  | 7.21G/9.71G [00:26<00:10, 243MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  75%|███████▍  | 7.25G/9.71G [00:26<00:09, 256MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  75%|███████▍  | 7.28G/9.71G [00:26<00:08, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  75%|███████▌  | 7.31G/9.71G [00:26<00:08, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  76%|███████▌  | 7.34G/9.71G [00:27<00:08, 266MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  76%|███████▌  | 7.37G/9.71G [00:27<00:08, 268MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  76%|███████▌  | 7.40G/9.71G [00:27<00:08, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  77%|███████▋  | 7.43G/9.71G [00:27<00:08, 277MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  77%|███████▋  | 7.47G/9.71G [00:27<00:08, 270MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  80%|███████▉  | 1.70G/2.13G [00:04<00:01, 303MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  82%|████████▏ | 1.74G/2.13G [00:04<00:01, 312MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  84%|████████▎ | 1.78G/2.13G [00:04<00:01, 313MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  85%|████████▌ | 1.82G/2.13G [00:05<00:00, 317MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  87%|████████▋ | 1.87G/2.13G [00:05<00:00, 294MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  89%|████████▉ | 1.91G/2.13G [00:05<00:00, 304MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  91%|█████████ | 1.94G/2.13G [00:05<00:00, 305MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  93%|█████████▎| 1.98G/2.13G [00:05<00:00, 323MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  77%|███████▋  | 7.50G/9.71G [00:27<00:08, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.53G/9.71G [00:27<00:07, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.56G/9.71G [00:27<00:07, 283MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.59G/9.71G [00:27<00:07, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  78%|███████▊  | 7.62G/9.71G [00:28<00:07, 292MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  79%|███████▉  | 7.65G/9.71G [00:28<00:07, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  79%|███████▉  | 7.69G/9.71G [00:28<00:07, 279MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  79%|███████▉  | 7.72G/9.71G [00:28<00:07, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  80%|███████▉  | 7.75G/9.71G [00:28<00:07, 250MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  95%|█████████▍| 2.02G/2.13G [00:05<00:00, 324MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  97%|█████████▋| 2.07G/2.13G [00:05<00:00, 332MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  99%|█████████▉| 2.11G/2.13G [00:05<00:00, 332MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";: 100%|██████████| 2.13G/2.13G [00:06<00:00, 351MB/s]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.73s/it]#015Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.73s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.73s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.73s/it]#015Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.73s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.48s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.74s/it]#015Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.49s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.74s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.49s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 44.74s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:27<00:00, 53.49s/it]\n",
      "Downloading (…)00004-of-00005.bin\";:  80%|████████  | 7.78G/9.71G [00:28<00:07, 244MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  80%|████████  | 7.81G/9.71G [00:28<00:07, 241MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  81%|████████  | 7.84G/9.71G [00:29<00:07, 242MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  81%|████████  | 7.87G/9.71G [00:29<00:07, 250MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  81%|████████▏ | 7.91G/9.71G [00:29<00:07, 254MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  82%|████████▏ | 7.94G/9.71G [00:29<00:06, 255MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  82%|████████▏ | 7.97G/9.71G [00:29<00:06, 259MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  82%|████████▏ | 8.00G/9.71G [00:29<00:06, 259MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  83%|████████▎ | 8.03G/9.71G [00:29<00:06, 255MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  83%|████████▎ | 8.06G/9.71G [00:29<00:06, 252MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  83%|████████▎ | 8.10G/9.71G [00:29<00:06, 254MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  84%|████████▎ | 8.13G/9.71G [00:30<00:06, 249MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  84%|████████▍ | 8.16G/9.71G [00:30<00:05, 264MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  84%|████████▍ | 8.19G/9.71G [00:30<00:05, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  85%|████████▍ | 8.22G/9.71G [00:30<00:05, 280MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  85%|████████▍ | 8.25G/9.71G [00:30<00:05, 276MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  85%|████████▌ | 8.28G/9.71G [00:30<00:07, 183MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  86%|████████▌ | 8.32G/9.71G [00:30<00:06, 204MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  86%|████████▌ | 8.35G/9.71G [00:31<00:06, 226MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  86%|████████▋ | 8.39G/9.71G [00:31<00:05, 251MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  87%|████████▋ | 8.43G/9.71G [00:31<00:04, 271MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  87%|████████▋ | 8.46G/9.71G [00:31<00:04, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  88%|████████▊ | 8.50G/9.71G [00:31<00:04, 295MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  88%|████████▊ | 8.54G/9.71G [00:31<00:03, 295MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  88%|████████▊ | 8.58G/9.71G [00:31<00:03, 302MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  89%|████████▉ | 8.62G/9.71G [00:31<00:03, 310MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  89%|████████▉ | 8.65G/9.71G [00:32<00:03, 309MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  90%|████████▉ | 8.69G/9.71G [00:32<00:03, 309MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  90%|████████▉ | 8.73G/9.71G [00:32<00:03, 312MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  90%|█████████ | 8.77G/9.71G [00:32<00:03, 310MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  91%|█████████ | 8.81G/9.71G [00:32<00:02, 318MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  91%|█████████ | 8.85G/9.71G [00:32<00:02, 316MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  92%|█████████▏| 8.89G/9.71G [00:32<00:02, 312MB/s]\n",
      "#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  92%|█████████▏| 8.92G/9.71G [00:32<00:02, 289MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  92%|█████████▏| 8.95G/9.71G [00:33<00:02, 280MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  93%|█████████▎| 8.99G/9.71G [00:33<00:02, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  93%|█████████▎| 9.02G/9.71G [00:33<00:02, 275MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  93%|█████████▎| 9.05G/9.71G [00:33<00:02, 274MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▎| 9.08G/9.71G [00:33<00:02, 282MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▍| 9.11G/9.71G [00:33<00:02, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▍| 9.14G/9.71G [00:33<00:02, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  94%|█████████▍| 9.18G/9.71G [00:33<00:01, 276MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  95%|█████████▍| 9.21G/9.71G [00:33<00:01, 284MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  95%|█████████▌| 9.24G/9.71G [00:34<00:01, 290MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  95%|█████████▌| 9.27G/9.71G [00:34<00:01, 286MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  96%|█████████▌| 9.30G/9.71G [00:34<00:01, 281MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  96%|█████████▌| 9.33G/9.71G [00:34<00:01, 282MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  96%|█████████▋| 9.36G/9.71G [00:34<00:01, 287MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  97%|█████████▋| 9.40G/9.71G [00:34<00:01, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  97%|█████████▋| 9.43G/9.71G [00:34<00:00, 294MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  97%|█████████▋| 9.46G/9.71G [00:34<00:00, 293MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  98%|█████████▊| 9.49G/9.71G [00:34<00:00, 285MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  98%|█████████▊| 9.52G/9.71G [00:35<00:00, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  98%|█████████▊| 9.55G/9.71G [00:35<00:00, 286MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  99%|█████████▊| 9.58G/9.71G [00:35<00:00, 291MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  99%|█████████▉| 9.62G/9.71G [00:35<00:00, 295MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";:  99%|█████████▉| 9.65G/9.71G [00:35<00:00, 296MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";: 100%|█████████▉| 9.68G/9.71G [00:35<00:00, 288MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";: 100%|█████████▉| 9.71G/9.71G [00:35<00:00, 273MB/s]#033[A\n",
      "Downloading (…)00004-of-00005.bin\";: 100%|██████████| 9.71G/9.71G [00:35<00:00, 272MB/s]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.71s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.70s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.71s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.70s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.71s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.71s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.71s/it]\n",
      "Downloading shards:  80%|████████  | 4/5 [04:34<01:10, 70.72s/it]\n",
      "Downloading (…)00005-of-00005.bin\";:   0%|          | 0.00/2.13G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   2%|▏         | 41.9M/2.13G [00:00<00:05, 375MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   4%|▍         | 94.4M/2.13G [00:00<00:04, 431MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   7%|▋         | 147M/2.13G [00:00<00:04, 456MB/s] #033[A\n",
      "Downloading (…)00005-of-00005.bin\";:   9%|▉         | 199M/2.13G [00:00<00:04, 469MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  12%|█▏        | 252M/2.13G [00:00<00:03, 474MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  14%|█▍        | 304M/2.13G [00:00<00:03, 464MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  17%|█▋        | 357M/2.13G [00:00<00:03, 469MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  19%|█▉        | 409M/2.13G [00:00<00:03, 471MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  22%|██▏       | 461M/2.13G [00:00<00:03, 474MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  24%|██▍       | 514M/2.13G [00:01<00:03, 478MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  27%|██▋       | 566M/2.13G [00:01<00:03, 481MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  29%|██▉       | 619M/2.13G [00:01<00:03, 482MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  31%|███▏      | 671M/2.13G [00:01<00:03, 481MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  34%|███▍      | 724M/2.13G [00:01<00:02, 482MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  36%|███▋      | 776M/2.13G [00:01<00:02, 475MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  39%|███▉      | 828M/2.13G [00:01<00:02, 475MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  41%|████▏     | 881M/2.13G [00:01<00:02, 476MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  44%|████▎     | 933M/2.13G [00:01<00:02, 476MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  46%|████▌     | 986M/2.13G [00:02<00:02, 467MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  49%|████▊     | 1.04G/2.13G [00:02<00:02, 465MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  51%|█████     | 1.09G/2.13G [00:02<00:02, 467MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  54%|█████▎    | 1.14G/2.13G [00:02<00:02, 412MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  56%|█████▌    | 1.20G/2.13G [00:02<00:02, 425MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  58%|█████▊    | 1.25G/2.13G [00:02<00:02, 439MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  61%|██████    | 1.30G/2.13G [00:02<00:01, 431MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  63%|██████▎   | 1.35G/2.13G [00:02<00:01, 441MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  66%|██████▌   | 1.41G/2.13G [00:03<00:01, 451MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  68%|██████▊   | 1.46G/2.13G [00:03<00:01, 460MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  71%|███████   | 1.51G/2.13G [00:03<00:01, 462MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  73%|███████▎  | 1.56G/2.13G [00:03<00:01, 467MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  76%|███████▌  | 1.61G/2.13G [00:03<00:01, 456MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  78%|███████▊  | 1.67G/2.13G [00:03<00:01, 452MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  81%|████████  | 1.72G/2.13G [00:03<00:00, 458MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  83%|████████▎ | 1.77G/2.13G [00:03<00:00, 461MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  85%|████████▌ | 1.82G/2.13G [00:03<00:00, 459MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  88%|████████▊ | 1.88G/2.13G [00:04<00:00, 455MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  90%|█████████ | 1.93G/2.13G [00:04<00:00, 457MB/s]\n",
      "#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  93%|█████████▎| 1.98G/2.13G [00:04<00:00, 453MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  95%|█████████▌| 2.03G/2.13G [00:04<00:00, 455MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";:  98%|█████████▊| 2.09G/2.13G [00:04<00:00, 455MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";: 100%|██████████| 2.13G/2.13G [00:04<00:00, 441MB/s]#033[A\n",
      "Downloading (…)00005-of-00005.bin\";: 100%|██████████| 2.13G/2.13G [00:04<00:00, 458MB/s]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.92s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.91s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.83s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]#015Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.83s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 46.93s/it]\n",
      "Downloading shards: 100%|██████████| 5/5 [04:39<00:00, 55.82s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:36,  9.10s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:38,  9.59s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:38,  9.62s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:37,  9.26s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:36,  9.20s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:38,  9.66s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:36,  9.01s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:18<00:27,  9.08s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:41, 10.37s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:19<00:28,  9.62s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:19<00:29,  9.67s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:18<00:28,  9.52s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:18<00:28,  9.49s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:19<00:29,  9.68s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:37,  9.45s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.96s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:43, 10.84s/it]#015Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:43, 10.86s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:42, 10.70s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:42, 10.61s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:42, 10.60s/it]\n",
      "Loading checkpoint shards:  20%|██        | 1/5 [00:10<00:41, 10.42s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:17<00:26,  8.96s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:27<00:18,  9.10s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:28<00:19,  9.62s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:20<00:30, 10.13s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:29<00:19,  9.68s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:28<00:19,  9.59s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:28<00:19,  9.56s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:29<00:19,  9.67s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:19<00:28,  9.52s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:20<00:30, 10.02s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:26<00:17,  8.88s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:32, 10.86s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:32, 10.85s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:31, 10.64s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:32, 10.75s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:32, 10.79s/it]\n",
      "Loading checkpoint shards:  40%|████      | 2/5 [00:21<00:32, 10.75s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:36<00:09,  9.16s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:29<00:19,  9.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:38<00:00,  6.64s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:38<00:00,  7.74s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.66s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.66s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.58s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.60s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.97s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.15s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:28<00:19,  9.61s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.08s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.98s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.18s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.97s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.17s/it]\n",
      "#015Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.94s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:35<00:08,  8.75s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:29<00:20, 10.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:37<00:00,  6.25s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:37<00:00,  7.42s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.88s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.90s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.79s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.86s/it]\n",
      "Loading checkpoint shards:  60%|██████    | 3/5 [00:32<00:21, 10.84s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.32s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.66s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.03s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:38<00:09,  9.69s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:40<00:10, 10.05s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  6.98s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:40<00:00,  8.15s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:42<00:00,  7.21s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:42<00:00,  8.46s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.85s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.91s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.83s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.88s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.87s/it]\n",
      "Loading checkpoint shards:  80%|████████  | 4/5 [00:43<00:10, 10.87s/it]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.73s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.73s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.14s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.76s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.10s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.75s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.10s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  7.78s/it]#015Loading checkpoint shards: 100%|██████████| 5/5 [00:45<00:00,  9.09s/it]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/training_args.py:1388: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead \n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:32.743: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:32.775 algo-1:120 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:32.805 algo-1:120 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:32.805 algo-1:120 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:32.806 algo-1:120 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:32.806 algo-1:120 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:32.806 algo-1:120 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "0%|          | 0/29 [00:00<?, ?it/s]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:39.625: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:39.655 algo-1:118 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:39.683 algo-1:118 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:39.683 algo-1:118 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:39.684 algo-1:118 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:39.684 algo-1:118 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:39.684 algo-1:118 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:39.762: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:39.793 algo-1:114 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:39.821 algo-1:114 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:39.822 algo-1:114 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:39.822 algo-1:114 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:39.822 algo-1:114 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:39.822 algo-1:114 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "0%|          | 0/29 [00:00<?, ?it/s]\n",
      "[2023-04-28 20:26:39.836: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:39.867 algo-1:116 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:39.895 algo-1:116 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:39.896 algo-1:116 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:39.896 algo-1:116 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:39.897 algo-1:116 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:39.897 algo-1:116 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:39.910: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:39.942 algo-1:113 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:39.972 algo-1:113 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:39.972 algo-1:113 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:39.973 algo-1:113 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:39.973 algo-1:113 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:39.973 algo-1:113 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:40.105: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:40.136 algo-1:115 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.164 algo-1:115 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.165 algo-1:115 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.165 algo-1:115 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.165 algo-1:115 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.166 algo-1:115 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:40.030: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:40.064 algo-2:112 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.095 algo-2:112 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.095 algo-2:112 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.096 algo-2:112 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.096 algo-2:112 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.096 algo-2:112 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:40.341: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 20:26:40.345: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:40.373 algo-2:116 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.375 algo-2:113 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.401 algo-2:116 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.402 algo-2:116 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.402 algo-2:113 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.402 algo-2:116 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.402 algo-2:113 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.402 algo-2:116 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.403 algo-2:113 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.403 algo-2:116 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:40.403 algo-2:113 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.403 algo-2:113 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:40.404: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 20:26:40.405: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:40.435 algo-2:114 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.436 algo-2:115 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:40.463 algo-2:115 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.463 algo-2:114 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:40.464 algo-2:115 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.464 algo-2:114 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:40.464 algo-2:115 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.464 algo-2:114 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:40.464 algo-2:115 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.464 algo-2:114 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:40.464 algo-2:115 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:40.465 algo-2:114 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "NCCL version 2.14.3+cuda11.7\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:41.740: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:41.769 algo-1:117 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:41.776: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:41.796 algo-1:117 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:41.796 algo-1:117 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:41.796 algo-1:117 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:41.797 algo-1:117 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:41.797 algo-1:117 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:41.805 algo-1:119 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:41.831 algo-1:119 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:41.831 algo-1:119 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:41.831 algo-1:119 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:41.832 algo-1:119 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:41.832 algo-1:119 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:42.025: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:42.054 algo-2:118 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:42.081 algo-2:118 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:42.081 algo-2:118 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[2023-04-28 20:26:42.082 algo-2:118 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:42.082 algo-2:118 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:42.082 algo-2:118 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:42.152: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:42.166: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 20:26:42.182 algo-2:119 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:42.195 algo-2:117 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 20:26:42.209 algo-2:119 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:42.210 algo-2:119 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:42.210 algo-2:119 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:42.210 algo-2:119 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:42.210 algo-2:119 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 20:26:42.222 algo-2:117 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 20:26:42.223 algo-2:117 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 20:26:42.223 algo-2:117 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 20:26:42.223 algo-2:117 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 20:26:42.224 algo-2:117 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\n",
      "3%|▎         | 1/29 [00:17<08:17, 17.78s/it]\n",
      "3%|▎         | 1/29 [00:17<08:21, 17.90s/it]\n",
      "7%|▋         | 2/29 [00:43<10:04, 22.37s/it]\n",
      "7%|▋         | 2/29 [00:43<10:05, 22.42s/it]\n",
      "10%|█         | 3/29 [01:10<10:42, 24.72s/it]\n",
      "10%|█         | 3/29 [01:10<10:43, 24.75s/it]\n",
      "14%|█▍        | 4/29 [01:36<10:31, 25.27s/it]\n",
      "14%|█▍        | 4/29 [01:37<10:32, 25.28s/it]\n",
      "17%|█▋        | 5/29 [02:02<10:11, 25.47s/it]\n",
      "17%|█▋        | 5/29 [02:02<10:11, 25.48s/it]\n",
      "21%|██        | 6/29 [02:28<09:46, 25.51s/it]\n",
      "21%|██        | 6/29 [02:28<09:46, 25.51s/it]\n",
      "24%|██▍       | 7/29 [02:54<09:22, 25.57s/it]\n",
      "24%|██▍       | 7/29 [02:54<09:22, 25.57s/it]\n",
      "28%|██▊       | 8/29 [03:21<09:06, 26.02s/it]\n",
      "28%|██▊       | 8/29 [03:21<09:06, 26.02s/it]\n",
      "31%|███       | 9/29 [03:48<08:49, 26.47s/it]\n",
      "31%|███       | 9/29 [03:48<08:49, 26.47s/it]\n",
      "34%|███▍      | 10/29 [04:16<08:34, 27.08s/it]\n",
      "{'loss': 2.5555, 'learning_rate': 3.275862068965517e-05, 'epoch': 0.34}\n",
      "34%|███▍      | 10/29 [04:17<08:34, 27.08s/it]\n",
      "34%|███▍      | 10/29 [04:17<08:34, 27.08s/it]\n",
      "{'loss': 2.5555, 'learning_rate': 3.275862068965517e-05, 'epoch': 0.34}\n",
      "34%|███▍      | 10/29 [04:17<08:34, 27.08s/it]\n",
      "38%|███▊      | 11/29 [04:44<08:10, 27.23s/it]\n",
      "38%|███▊      | 11/29 [04:44<08:10, 27.23s/it]\n",
      "41%|████▏     | 12/29 [05:10<07:38, 26.98s/it]\n",
      "41%|████▏     | 12/29 [05:11<07:38, 26.98s/it]\n",
      "45%|████▍     | 13/29 [05:39<07:19, 27.45s/it]\n",
      "45%|████▍     | 13/29 [05:39<07:19, 27.45s/it]\n",
      "48%|████▊     | 14/29 [06:06<06:50, 27.38s/it]\n",
      "48%|████▊     | 14/29 [06:06<06:50, 27.38s/it]\n",
      "52%|█████▏    | 15/29 [06:33<06:21, 27.23s/it]\n",
      "52%|█████▏    | 15/29 [06:33<06:21, 27.23s/it]\n",
      "55%|█████▌    | 16/29 [06:59<05:49, 26.86s/it]\n",
      "55%|█████▌    | 16/29 [06:59<05:49, 26.86s/it]\n",
      "59%|█████▊    | 17/29 [07:24<05:16, 26.39s/it]\n",
      "59%|█████▊    | 17/29 [07:24<05:16, 26.39s/it]\n",
      "62%|██████▏   | 18/29 [07:52<04:54, 26.79s/it]\n",
      "62%|██████▏   | 18/29 [07:52<04:54, 26.79s/it]\n",
      "66%|██████▌   | 19/29 [08:17<04:21, 26.19s/it]\n",
      "66%|██████▌   | 19/29 [08:17<04:21, 26.19s/it]\n",
      "69%|██████▉   | 20/29 [08:43<03:56, 26.24s/it]\n",
      "{'loss': 2.542, 'learning_rate': 1.5517241379310346e-05, 'epoch': 0.69}\n",
      "69%|██████▉   | 20/29 [08:43<03:56, 26.24s/it]\n",
      "69%|██████▉   | 20/29 [08:43<03:56, 26.24s/it]\n",
      "{'loss': 2.542, 'learning_rate': 1.5517241379310346e-05, 'epoch': 0.69}\n",
      "69%|██████▉   | 20/29 [08:43<03:56, 26.24s/it]\n",
      "72%|███████▏  | 21/29 [09:12<03:36, 27.08s/it]\n",
      "72%|███████▏  | 21/29 [09:12<03:36, 27.08s/it]\n",
      "76%|███████▌  | 22/29 [09:39<03:09, 27.02s/it]\n",
      "76%|███████▌  | 22/29 [09:39<03:09, 27.02s/it]\n",
      "79%|███████▉  | 23/29 [10:04<02:38, 26.50s/it]\n",
      "79%|███████▉  | 23/29 [10:05<02:38, 26.50s/it]\n",
      "83%|████████▎ | 24/29 [10:32<02:14, 26.91s/it]\n",
      "83%|████████▎ | 24/29 [10:32<02:14, 26.91s/it]\n",
      "86%|████████▌ | 25/29 [11:01<01:49, 27.38s/it]\n",
      "86%|████████▌ | 25/29 [11:01<01:49, 27.38s/it]\n",
      "90%|████████▉ | 26/29 [11:27<01:21, 27.04s/it]\n",
      "90%|████████▉ | 26/29 [11:27<01:21, 27.04s/it]\n",
      "93%|█████████▎| 27/29 [11:54<00:53, 26.95s/it]\n",
      "93%|█████████▎| 27/29 [11:54<00:53, 26.95s/it]\n",
      "97%|█████████▋| 28/29 [12:21<00:27, 27.03s/it]\n",
      "97%|█████████▋| 28/29 [12:21<00:27, 27.03s/it]\n",
      "100%|██████████| 29/29 [12:29<00:00, 21.44s/it]\n",
      "{'train_runtime': 749.9432, 'train_samples_per_second': 1.203, 'train_steps_per_second': 0.039, 'train_loss': 2.576796827645137, 'epoch': 1.0}\n",
      "100%|██████████| 29/29 [12:29<00:00, 21.44s/it]\n",
      "100%|██████████| 29/29 [12:29<00:00, 25.86s/it]\n",
      "100%|██████████| 29/29 [12:29<00:00, 21.44s/it]\n",
      "{'train_runtime': 750.0622, 'train_samples_per_second': 1.203, 'train_steps_per_second': 0.039, 'train_loss': 2.589408611429149, 'epoch': 1.0}\n",
      "100%|██████████| 29/29 [12:30<00:00, 21.44s/it]\n",
      "100%|██████████| 29/29 [12:30<00:00, 25.86s/it]\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.19 GiB already allocated; 326.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.19 GiB already allocated; 326.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.19 GiB already allocated; 350.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.19 GiB already allocated; 350.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.19 GiB already allocated; 326.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.19 GiB already allocated; 326.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.19 GiB already allocated; 350.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.19 GiB already allocated; 350.56 MiB free; 37.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.61 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.9.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.9.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.61 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.33 GiB already allocated; 206.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.33 GiB already allocated; 182.56 MiB free; 38.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.10.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.10.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.47 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.47 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.11.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.11.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 62.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 38.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 70.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.12.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.12.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 46.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.13.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.13.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.14.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.14.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.15.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.15.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.16.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.16.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.17.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.17.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.18.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.18.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 68.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 60.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 44.56 MiB free; 38.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 36.56 MiB free; 38.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.19.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.19.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.20.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.20.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.21.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.21.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.22.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.22.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.23.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.23.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.24.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.24.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 42.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 34.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 66.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.25.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.25.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 58.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.26.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.26.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.27.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.27.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.28.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.28.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.29.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.29.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.30.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.30.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.31.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.31.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 40.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 64.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 32.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 56.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.32.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.32.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.33.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.33.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.34.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.34.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.35.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.35.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.36.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.36.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.37.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.37.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 38.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 54.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 62.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.38.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.38.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 30.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.39.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.39.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.40.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.40.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.41.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.41.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.42.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.42.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 3; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 7; 39.56 GiB total capacity; 36.62 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.query_key_value.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.query_key_value.weight. Error: CUDA out of memory. Tried to allocate 432.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 1; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.attention.dense.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.attention.dense.weight. Error: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 5; 39.56 GiB total capacity; 36.62 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 6; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 3; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 7; 39.56 GiB total capacity; 36.63 GiB already allocated; 36.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 2; 39.56 GiB total capacity; 36.48 GiB already allocated; 52.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_h_to_4h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_h_to_4h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 5; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 4; 39.56 GiB total capacity; 36.48 GiB already allocated; 28.56 MiB free; 38.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:2224: UserWarning: Failed to clone() tensor with name gpt_neox.layers.43.mlp.dense_4h_to_h.weight. This may mean that this state_dict entry could point to invalid memory regions after returning from state_dict() call if this parameter is managed by FSDP. Please check clone implementation of gpt_neox.layers.43.mlp.dense_4h_to_h.weight. Error: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 39.56 GiB total capacity; 36.63 GiB already allocated; 60.56 MiB free; 38.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  warnings.warn(\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 414/414 [00:00<00:00, 78.5kB/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 100MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 31.6kB/s]\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 414/414 [00:00<00:00, 83.2kB/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 76.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 35.6kB/s]\n",
      "2023-04-28 20:40:35,873 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 20:40:35,873 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-28 20:40:35,874 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "2023-04-28 20:40:35,879 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 20:40:35,879 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-28 20:40:35,880 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-04-28 20:41:23 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainign took `20632` seconds, which is about `5.7` hours. The `ml.g5.2xlarge` instance we used costs `$1.515` per hour. So the total cost for training BLOOMZ 7B was is `$8.63`. We could reduce the cost by using a spot instance, but the training time could increase, by waiting or restarts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
