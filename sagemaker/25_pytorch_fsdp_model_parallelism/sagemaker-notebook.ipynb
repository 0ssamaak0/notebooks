{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this tutorial, we are going to fine-tune the new [GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B) on the [ELI5](https://huggingface.co/datasets/eli5) dataset to improve the explanation and question-answering skills of the agent. The [ELI5](https://huggingface.co/datasets/eli5) dataset is an English-language dataset of questions and answers gathered from three subreddits where users ask factual questions requiring paragraph-length or longer answers. We are going to use Hugging Face Transformers and DeepSpeed ZeRO to fine-tune our model.\n",
    "\n",
    "\n",
    "\n",
    "https://engineering.fb.com/2021/07/15/open-source/fsdp/\n",
    "https://pytorch.org/blog/efficient-large-scale-training-with-pytorch/\n",
    "https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" \"sagemaker>=2.150.0\" --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "As the base dataset, we will use the [ELI5](https://huggingface.co/datasets/eli5) dataset, but before fine-tuning the model, we need to preprocess the data. We will create a \"chat\" version of the dataset by adding `<user>` and `<bot>`tokens and add an end-of-sequence `<|endoftext|>` token to help the model learn to distinguish consecutive examples. Additionally, we create chunks of `2048` tokens ([model max length](https://huggingface.co/EleutherAI/gpt-neox-20b)) to avoid unnecessary padding and computing. \n",
    "\n",
    "The first step is to load our dataset from Hugging Face. The dataset contains `272634` samples for `eli5`. We will downsample the dataset to `10 000` to make it more realistic for real-world use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 07:11:29.998014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-27 07:11:30.094125: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-27 07:11:30.096937: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-27 07:11:30.096949: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-27 07:11:30.605369: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 07:11:30.605420: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-27 07:11:30.605424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Found cached dataset eli5 (/home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-ff13b89bd5550ed9.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "# Load Tokenizer \n",
    "model_id = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load dataset from huggingface.co\n",
    "dataset_id = \"eli5\"\n",
    "dataset = load_dataset(dataset_id, split=\"train_eli5\")\n",
    "\n",
    "# downsample dataset to 10k\n",
    "dataset = dataset.shuffle(42).select(range(10_000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [ELI5](https://huggingface.co/datasets/eli5) sample can include multiple answers to a “question”. We will select the answer with the highest user score for our explanation. \n",
    "\n",
    "*Note: This dataset is a good example of using reinforcement learning for training transformers learning to generate answers with higher scores. Let me know if you are interested in an example of that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-8abed608d38c178b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<human>: Explain like I am five: If electricity is made from mostly fossil fuels, how is it considered a clean energy source? (e.g. Electric cars)\\n<bot>: Three main reasons. First, you can get electricity from things like nuclear power, wind, hydroelectric, and solar power. It might come from fossil fuel or it might not.\\n\\nSecond, converting fuel into energy with low emissions is performed better with more specialized equipment. A coal power plant can scrub exhaust air with mechanisms that are impractical to carry around in every car.\\n\\nFinally the third and one of the most important reasons is *where* the emissions are released. If you are dumping car exhaust downtown then air quality suffers locally, but if you dump it from smokestacks on the edge of town it is much less of a problem because people aren't breathing it in as much.<|endoftext|>\"}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "# dataset template for chat conversation\n",
    "template=f'''<human>: Explain like I am five: {{question}}\n",
    "<bot>: {{answer}}{{eos_token}}'''\n",
    "\n",
    "eos_token = tokenizer.eos_token \n",
    "\n",
    "def template_dataset(sample):\n",
    "\tsample[\"text\"] = template.format(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tquestion=sample[\"title\"], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tanswer=sample[\"answers\"][\"text\"][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\teos_token=eos_token\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\treturn sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "# print random sample\n",
    "print(dataset[randint(0, 10_000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the data preparation is to tokenize and chunk our dataset. We convert our inputs (text) to token IDs by tokenizing, which the model can understand. Additionally, we concatenate our dataset samples into chunks of `2048` to avoid unnecessary padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-c4de07e830066b12.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/eli5/LFQA_reddit/1.0.0/17574e5502a10f41bbd17beba83e22475b499fa62caa1384a3d093fc856fe6fa/cache-276409c2387343d5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 902\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a5759905d4f508e3fe4118092eee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/902 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-558105141721/processed/eli-5/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/eli-5/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = \"s3://sagemaker-us-east-1-558105141721/processed/eli-5/train\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the GPT model using PyTorch FSDP \n",
    "\n",
    "In addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for BLOOMZ ~4x.  \n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements uses PEFT to train our model. If you are interested in how this works check-out [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft) blog, where we explain the training script in detail. T\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    # 'model_id': 'togethercomputer/GPT-NeoXT-Chat-Base-20B',\n",
    "    # 'model_id': 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
    "    # 'model_id': 'EleutherAI/pythia-12b',\n",
    "    'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "    'gradient_checkpointing': True,\n",
    "    'bf16': True,\n",
    "    # 'optimizer': \"adafactor\",\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'epochs': 1,\n",
    "    'fsdp': '\"full_shard auto_wrap\"',\n",
    "    'fsdp_transformer_layer_cls_to_wrap': \"GPTNeoXLayer\",\n",
    "}\n",
    "\n",
    "hyperparameters={\n",
    "    # 'model_id': 'togethercomputer/GPT-NeoXT-Chat-Base-20B',\n",
    "    # 'model_id': 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
    "    # 'model_id': 'EleutherAI/pythia-12b',\n",
    "    'model_id': 'EleutherAI/pythia-6.9b',\n",
    "    'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "    'gradient_checkpointing': True,\n",
    "    'bf16': True,\n",
    "    # 'optimizer': \"adafactor\",\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'epochs': 1,\n",
    "    'ds_config': 'ds_offload.json',\n",
    "}\n",
    "\n",
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_clm.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=200,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    transformers_version='4.26.0',\n",
    "    pytorch_version='1.13.1',\n",
    "    py_version=\"py39\",\n",
    "    hyperparameters = hyperparameters,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}}\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "# huggingface_estimator.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2023-04-28-08-32-57-582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-28 08:32:59 Starting - Starting the training job......\n",
      "2023-04-28 08:33:40 Starting - Preparing the instances for training.........\n",
      "2023-04-28 08:35:27 Downloading - Downloading input data\n",
      "2023-04-28 08:35:27 Training - Downloading the training image..................\n",
      "2023-04-28 08:38:28 Training - Training image download completed. Training in progress.......bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-04-28 08:39:28,745 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-04-28 08:39:28,807 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 08:39:28,816 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-04-28 08:39:28,818 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-04-28 08:39:28,818 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-04-28 08:39:29,221 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.28.1\n",
      "Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 95.5 MB/s eta 0:00:00\n",
      "Collecting accelerate==0.18.0\n",
      "Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.3/215.3 kB 44.1 MB/s eta 0:00:00\n",
      "Collecting deepspeed==0.8.3\n",
      "Downloading deepspeed-0.8.3.tar.gz (765 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.4/765.4 kB 89.5 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.28.1->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.18.0->-r requirements.txt (line 2)) (1.13.1+cu117)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.18.0->-r requirements.txt (line 2)) (5.9.4)\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.3->-r requirements.txt (line 3)) (1.10.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 1)) (2.1.1)\n",
      "Building wheels for collected packages: deepspeed\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776406 sha256=0a36554458c87bea91783f3db25e938b94d78c69ff089992aae6d93e0f433e37\n",
      "Stored in directory: /root/.cache/pip/wheels/f8/ea/8f/0768328ba436ed66f602d8d3b809624448c9eb627434176d04\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed, accelerate, transformers\n",
      "Attempting uninstall: deepspeed\n",
      "Found existing installation: deepspeed 0.6.1+06f2048\n",
      "Uninstalling deepspeed-0.6.1+06f2048:\n",
      "Successfully uninstalled deepspeed-0.6.1+06f2048\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.16.0\n",
      "Uninstalling accelerate-0.16.0:\n",
      "Successfully uninstalled accelerate-0.16.0\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.26.0\n",
      "Uninstalling transformers-4.26.0:\n",
      "Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed accelerate-0.18.0 deepspeed-0.8.3 transformers-4.28.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-04-28 08:39:45,029 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 08:39:45,030 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-28 08:39:45,096 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 08:39:45,167 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 08:39:45,176 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-04-28 08:39:45,237 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-28 08:39:45,247 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"ds_config\": \"ds_offload.json\",\n",
      "        \"epochs\": 1,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"model_id\": \"EleutherAI/pythia-6.9b\",\n",
      "        \"per_device_train_batch_size\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2023-04-28-08-32-57-582\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-08-32-57-582/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"ds_config\":\"ds_offload.json\",\"epochs\":1,\"gradient_checkpointing\":true,\"model_id\":\"EleutherAI/pythia-6.9b\",\"per_device_train_batch_size\":1}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=96\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-08-32-57-582/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"dataset_path\":\"/opt/ml/input/data/training\",\"ds_config\":\"ds_offload.json\",\"epochs\":1,\"gradient_checkpointing\":true,\"model_id\":\"EleutherAI/pythia-6.9b\",\"per_device_train_batch_size\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-pytorch-training-2023-04-28-08-32-57-582\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-2023-04-28-08-32-57-582/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--bf16\",\"True\",\"--dataset_path\",\"/opt/ml/input/data/training\",\"--ds_config\",\"ds_offload.json\",\"--epochs\",\"1\",\"--gradient_checkpointing\",\"True\",\"--model_id\",\"EleutherAI/pythia-6.9b\",\"--per_device_train_batch_size\",\"1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_BF16=true\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_DS_CONFIG=ds_offload.json\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_MODEL_ID=EleutherAI/pythia-6.9b\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 8 run_clm.py --bf16 True --dataset_path /opt/ml/input/data/training --ds_config ds_offload.json --epochs 1 --gradient_checkpointing True --model_id EleutherAI/pythia-6.9b --per_device_train_batch_size 1\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 571/571 [00:00<00:00, 92.2kB/s]\n",
      "Downloading (…)model.bin.index.json:   0%|          | 0.00/42.0k [00:00<?, ?B/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 42.0k/42.0k [00:00<00:00, 7.18MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)00001-of-00002.bin\";:   0%|          | 0.00/9.91G [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   0%|          | 31.5M/9.91G [00:00<00:41, 240MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   1%|          | 62.9M/9.91G [00:00<00:38, 259MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   1%|          | 94.4M/9.91G [00:00<00:36, 268MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   1%|▏         | 126M/9.91G [00:00<00:35, 278MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   2%|▏         | 157M/9.91G [00:00<00:33, 288MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   2%|▏         | 199M/9.91G [00:00<00:31, 305MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   2%|▏         | 241M/9.91G [00:00<00:30, 314MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   3%|▎         | 283M/9.91G [00:00<00:30, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   3%|▎         | 325M/9.91G [00:01<00:29, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   4%|▎         | 367M/9.91G [00:01<00:29, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   4%|▍         | 409M/9.91G [00:01<00:28, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   5%|▍         | 451M/9.91G [00:01<00:28, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   5%|▍         | 493M/9.91G [00:01<00:27, 341MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   5%|▌         | 535M/9.91G [00:01<00:26, 351MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   6%|▌         | 577M/9.91G [00:01<00:26, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   6%|▌         | 619M/9.91G [00:01<00:25, 360MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   7%|▋         | 661M/9.91G [00:02<00:25, 361MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   7%|▋         | 703M/9.91G [00:02<00:25, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   8%|▊         | 744M/9.91G [00:02<00:25, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   8%|▊         | 786M/9.91G [00:02<00:24, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   8%|▊         | 828M/9.91G [00:02<00:24, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   9%|▉         | 870M/9.91G [00:02<00:24, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:   9%|▉         | 912M/9.91G [00:02<00:24, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  10%|▉         | 954M/9.91G [00:02<00:24, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  10%|█         | 996M/9.91G [00:02<00:24, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  10%|█         | 1.04G/9.91G [00:03<00:23, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  11%|█         | 1.08G/9.91G [00:03<00:23, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  11%|█▏        | 1.12G/9.91G [00:03<00:23, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  12%|█▏        | 1.16G/9.91G [00:03<00:23, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  12%|█▏        | 1.21G/9.91G [00:03<00:23, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  13%|█▎        | 1.25G/9.91G [00:03<00:23, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  13%|█▎        | 1.29G/9.91G [00:03<00:23, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  13%|█▎        | 1.33G/9.91G [00:03<00:22, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  14%|█▍        | 1.37G/9.91G [00:03<00:22, 376MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  14%|█▍        | 1.42G/9.91G [00:04<00:22, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  15%|█▍        | 1.46G/9.91G [00:04<00:22, 375MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  15%|█▌        | 1.50G/9.91G [00:04<00:22, 377MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  16%|█▌        | 1.54G/9.91G [00:04<00:22, 370MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  16%|█▌        | 1.58G/9.91G [00:04<00:22, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  16%|█▋        | 1.63G/9.91G [00:04<00:22, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  17%|█▋        | 1.67G/9.91G [00:04<00:22, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  17%|█▋        | 1.71G/9.91G [00:04<00:22, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  18%|█▊        | 1.75G/9.91G [00:04<00:21, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  18%|█▊        | 1.79G/9.91G [00:05<00:21, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  19%|█▊        | 1.84G/9.91G [00:05<00:21, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  19%|█▉        | 1.88G/9.91G [00:05<00:21, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  19%|█▉        | 1.92G/9.91G [00:05<00:21, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  20%|█▉        | 1.96G/9.91G [00:05<00:21, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  20%|██        | 2.00G/9.91G [00:05<00:21, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  21%|██        | 2.04G/9.91G [00:05<00:21, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  21%|██        | 2.09G/9.91G [00:05<00:20, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  21%|██▏       | 2.13G/9.91G [00:05<00:20, 374MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  22%|██▏       | 2.17G/9.91G [00:06<00:20, 373MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  22%|██▏       | 2.21G/9.91G [00:06<00:20, 371MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  23%|██▎       | 2.25G/9.91G [00:06<00:20, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  23%|██▎       | 2.30G/9.91G [00:06<00:20, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  24%|██▎       | 2.34G/9.91G [00:06<00:20, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  24%|██▍       | 2.38G/9.91G [00:06<00:20, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  24%|██▍       | 2.42G/9.91G [00:06<00:20, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  25%|██▍       | 2.46G/9.91G [00:06<00:20, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  25%|██▌       | 2.51G/9.91G [00:07<00:20, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  26%|██▌       | 2.55G/9.91G [00:07<00:20, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  26%|██▌       | 2.59G/9.91G [00:07<00:20, 364MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  27%|██▋       | 2.63G/9.91G [00:07<00:19, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  27%|██▋       | 2.67G/9.91G [00:07<00:19, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  27%|██▋       | 2.72G/9.91G [00:07<00:19, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  28%|██▊       | 2.76G/9.91G [00:07<00:19, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  28%|██▊       | 2.80G/9.91G [00:07<00:20, 343MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  29%|██▊       | 2.84G/9.91G [00:07<00:20, 348MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  29%|██▉       | 2.88G/9.91G [00:08<00:19, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  30%|██▉       | 2.93G/9.91G [00:08<00:19, 359MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  30%|██▉       | 2.97G/9.91G [00:08<00:19, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  30%|███       | 3.01G/9.91G [00:08<00:18, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  31%|███       | 3.05G/9.91G [00:08<00:18, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  31%|███       | 3.09G/9.91G [00:08<00:18, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  32%|███▏      | 3.14G/9.91G [00:08<00:18, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  32%|███▏      | 3.18G/9.91G [00:08<00:19, 351MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  32%|███▏      | 3.22G/9.91G [00:08<00:19, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  33%|███▎      | 3.26G/9.91G [00:09<00:19, 340MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  33%|███▎      | 3.30G/9.91G [00:09<00:19, 336MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  34%|███▎      | 3.34G/9.91G [00:09<00:19, 334MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  34%|███▍      | 3.39G/9.91G [00:09<00:19, 332MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  35%|███▍      | 3.43G/9.91G [00:09<00:19, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  35%|███▌      | 3.47G/9.91G [00:09<00:19, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  35%|███▌      | 3.51G/9.91G [00:09<00:19, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  36%|███▌      | 3.55G/9.91G [00:10<00:28, 226MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  36%|███▌      | 3.59G/9.91G [00:10<00:26, 236MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  36%|███▋      | 3.62G/9.91G [00:10<00:25, 247MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  37%|███▋      | 3.65G/9.91G [00:10<00:24, 255MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  37%|███▋      | 3.68G/9.91G [00:10<00:23, 267MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  38%|███▊      | 3.72G/9.91G [00:10<00:21, 285MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  38%|███▊      | 3.76G/9.91G [00:10<00:20, 297MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  38%|███▊      | 3.81G/9.91G [00:11<00:19, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  39%|███▉      | 3.85G/9.91G [00:11<00:19, 313MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  39%|███▉      | 3.89G/9.91G [00:11<00:18, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  40%|███▉      | 3.93G/9.91G [00:11<00:18, 323MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  40%|████      | 3.97G/9.91G [00:11<00:18, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  41%|████      | 4.02G/9.91G [00:11<00:18, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  41%|████      | 4.06G/9.91G [00:11<00:17, 326MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  41%|████▏     | 4.10G/9.91G [00:11<00:17, 327MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  42%|████▏     | 4.14G/9.91G [00:12<00:19, 300MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  42%|████▏     | 4.17G/9.91G [00:12<00:19, 297MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  42%|████▏     | 4.20G/9.91G [00:12<00:19, 298MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  43%|████▎     | 4.25G/9.91G [00:12<00:18, 306MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  43%|████▎     | 4.28G/9.91G [00:12<00:18, 308MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  44%|████▎     | 4.32G/9.91G [00:12<00:17, 316MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  44%|████▍     | 4.36G/9.91G [00:12<00:17, 323MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  44%|████▍     | 4.40G/9.91G [00:12<00:16, 329MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  45%|████▍     | 4.45G/9.91G [00:13<00:16, 332MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  45%|████▌     | 4.49G/9.91G [00:13<00:16, 335MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  46%|████▌     | 4.53G/9.91G [00:13<00:16, 333MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  46%|████▌     | 4.57G/9.91G [00:13<00:15, 337MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  47%|████▋     | 4.61G/9.91G [00:13<00:15, 338MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  47%|████▋     | 4.66G/9.91G [00:13<00:15, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  47%|████▋     | 4.70G/9.91G [00:13<00:14, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  48%|████▊     | 4.74G/9.91G [00:13<00:14, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  48%|████▊     | 4.78G/9.91G [00:13<00:14, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  49%|████▊     | 4.82G/9.91G [00:14<00:13, 364MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  49%|████▉     | 4.87G/9.91G [00:14<00:13, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  50%|████▉     | 4.91G/9.91G [00:14<00:13, 365MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  50%|████▉     | 4.95G/9.91G [00:14<00:13, 370MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  50%|█████     | 4.99G/9.91G [00:14<00:14, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  51%|█████     | 5.03G/9.91G [00:14<00:13, 349MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  51%|█████     | 5.08G/9.91G [00:14<00:13, 353MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  52%|█████▏    | 5.12G/9.91G [00:14<00:13, 359MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  52%|█████▏    | 5.16G/9.91G [00:15<00:13, 362MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  52%|█████▏    | 5.20G/9.91G [00:15<00:12, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  53%|█████▎    | 5.24G/9.91G [00:15<00:12, 366MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  53%|█████▎    | 5.28G/9.91G [00:15<00:12, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  54%|█████▎    | 5.33G/9.91G [00:15<00:12, 367MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  54%|█████▍    | 5.37G/9.91G [00:15<00:12, 368MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  55%|█████▍    | 5.41G/9.91G [00:15<00:12, 369MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  55%|█████▌    | 5.45G/9.91G [00:15<00:12, 359MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  55%|█████▌    | 5.49G/9.91G [00:15<00:12, 357MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  56%|█████▌    | 5.54G/9.91G [00:16<00:12, 358MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  56%|█████▋    | 5.58G/9.91G [00:16<00:12, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  57%|█████▋    | 5.62G/9.91G [00:16<00:12, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  57%|█████▋    | 5.66G/9.91G [00:16<00:11, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  58%|█████▊    | 5.70G/9.91G [00:16<00:11, 353MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  58%|█████▊    | 5.75G/9.91G [00:16<00:11, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  58%|█████▊    | 5.79G/9.91G [00:16<00:11, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  59%|█████▉    | 5.83G/9.91G [00:16<00:11, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  59%|█████▉    | 5.87G/9.91G [00:17<00:11, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  60%|█████▉    | 5.91G/9.91G [00:17<00:11, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  60%|██████    | 5.96G/9.91G [00:17<00:11, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  61%|██████    | 6.00G/9.91G [00:17<00:11, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  61%|██████    | 6.04G/9.91G [00:17<00:10, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  61%|██████▏   | 6.08G/9.91G [00:17<00:10, 356MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  62%|██████▏   | 6.12G/9.91G [00:17<00:10, 359MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  62%|██████▏   | 6.17G/9.91G [00:17<00:10, 358MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  63%|██████▎   | 6.21G/9.91G [00:17<00:10, 358MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  63%|██████▎   | 6.25G/9.91G [00:18<00:10, 360MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  63%|██████▎   | 6.29G/9.91G [00:18<00:10, 360MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  64%|██████▍   | 6.33G/9.91G [00:18<00:10, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  64%|██████▍   | 6.38G/9.91G [00:18<00:09, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  65%|██████▍   | 6.42G/9.91G [00:18<00:10, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  65%|██████▌   | 6.46G/9.91G [00:18<00:10, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  66%|██████▌   | 6.50G/9.91G [00:18<00:10, 321MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  66%|██████▌   | 6.54G/9.91G [00:18<00:10, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  66%|██████▋   | 6.59G/9.91G [00:19<00:09, 339MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  67%|██████▋   | 6.63G/9.91G [00:19<00:09, 343MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  67%|██████▋   | 6.67G/9.91G [00:19<00:09, 346MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  68%|██████▊   | 6.71G/9.91G [00:19<00:09, 347MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  68%|██████▊   | 6.75G/9.91G [00:19<00:09, 348MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  69%|██████▊   | 6.79G/9.91G [00:19<00:08, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  69%|██████▉   | 6.84G/9.91G [00:19<00:08, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  69%|██████▉   | 6.88G/9.91G [00:19<00:08, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  70%|██████▉   | 6.92G/9.91G [00:20<00:08, 354MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  70%|███████   | 6.96G/9.91G [00:20<00:08, 355MB/s]\n",
      "#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  71%|███████   | 7.00G/9.91G [00:20<00:12, 233MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  71%|███████   | 7.04G/9.91G [00:20<00:11, 243MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  71%|███████▏  | 7.08G/9.91G [00:20<00:10, 264MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  72%|███████▏  | 7.11G/9.91G [00:20<00:10, 265MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  72%|███████▏  | 7.15G/9.91G [00:20<00:09, 279MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  72%|███████▏  | 7.18G/9.91G [00:21<00:09, 283MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  73%|███████▎  | 7.22G/9.91G [00:21<00:09, 296MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  73%|███████▎  | 7.27G/9.91G [00:21<00:08, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  74%|███████▎  | 7.31G/9.91G [00:21<00:08, 320MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  74%|███████▍  | 7.35G/9.91G [00:21<00:08, 286MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  74%|███████▍  | 7.38G/9.91G [00:21<00:08, 288MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  75%|███████▍  | 7.41G/9.91G [00:21<00:08, 290MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  75%|███████▌  | 7.44G/9.91G [00:21<00:08, 295MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  76%|███████▌  | 7.49G/9.91G [00:22<00:07, 308MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  76%|███████▌  | 7.53G/9.91G [00:22<00:07, 319MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  76%|███████▋  | 7.57G/9.91G [00:22<00:07, 324MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  77%|███████▋  | 7.61G/9.91G [00:22<00:07, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  77%|███████▋  | 7.65G/9.91G [00:22<00:07, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  78%|███████▊  | 7.70G/9.91G [00:22<00:06, 317MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  78%|███████▊  | 7.74G/9.91G [00:22<00:06, 325MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  78%|███████▊  | 7.78G/9.91G [00:22<00:06, 329MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  79%|███████▉  | 7.82G/9.91G [00:23<00:06, 335MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  79%|███████▉  | 7.86G/9.91G [00:23<00:06, 338MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  80%|███████▉  | 7.91G/9.91G [00:23<00:05, 337MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  80%|████████  | 7.95G/9.91G [00:23<00:06, 300MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  81%|████████  | 7.98G/9.91G [00:23<00:06, 299MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  81%|████████  | 8.02G/9.91G [00:23<00:06, 312MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  81%|████████▏ | 8.06G/9.91G [00:23<00:05, 328MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  82%|████████▏ | 8.11G/9.91G [00:23<00:05, 342MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  82%|████████▏ | 8.15G/9.91G [00:24<00:05, 352MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  83%|████████▎ | 8.19G/9.91G [00:24<00:04, 363MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  83%|████████▎ | 8.23G/9.91G [00:24<00:04, 372MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  83%|████████▎ | 8.27G/9.91G [00:24<00:04, 375MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  84%|████████▍ | 8.32G/9.91G [00:24<00:04, 381MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  84%|████████▍ | 8.36G/9.91G [00:24<00:04, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  85%|████████▍ | 8.40G/9.91G [00:24<00:03, 385MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  85%|████████▌ | 8.44G/9.91G [00:24<00:03, 385MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  86%|████████▌ | 8.48G/9.91G [00:24<00:03, 385MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  86%|████████▌ | 8.52G/9.91G [00:25<00:03, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  86%|████████▋ | 8.57G/9.91G [00:25<00:03, 391MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  87%|████████▋ | 8.61G/9.91G [00:25<00:03, 388MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  87%|████████▋ | 8.65G/9.91G [00:25<00:03, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  88%|████████▊ | 8.69G/9.91G [00:25<00:03, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  88%|████████▊ | 8.73G/9.91G [00:25<00:03, 389MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  89%|████████▊ | 8.78G/9.91G [00:25<00:02, 391MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  89%|████████▉ | 8.82G/9.91G [00:25<00:02, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  89%|████████▉ | 8.86G/9.91G [00:25<00:02, 380MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  90%|████████▉ | 8.90G/9.91G [00:26<00:02, 379MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  90%|█████████ | 8.94G/9.91G [00:26<00:02, 381MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  91%|█████████ | 8.99G/9.91G [00:26<00:02, 385MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  91%|█████████ | 9.03G/9.91G [00:26<00:02, 387MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  92%|█████████▏| 9.07G/9.91G [00:26<00:02, 386MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  92%|█████████▏| 9.11G/9.91G [00:26<00:02, 383MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  92%|█████████▏| 9.15G/9.91G [00:26<00:01, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  93%|█████████▎| 9.20G/9.91G [00:26<00:01, 390MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  93%|█████████▎| 9.24G/9.91G [00:26<00:01, 380MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  94%|█████████▎| 9.28G/9.91G [00:27<00:01, 336MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  94%|█████████▍| 9.32G/9.91G [00:27<00:01, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  94%|█████████▍| 9.36G/9.91G [00:27<00:01, 287MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  95%|█████████▍| 9.40G/9.91G [00:27<00:01, 274MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  95%|█████████▌| 9.43G/9.91G [00:27<00:01, 278MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  96%|█████████▌| 9.47G/9.91G [00:27<00:01, 292MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  96%|█████████▌| 9.51G/9.91G [00:27<00:01, 299MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  96%|█████████▋| 9.55G/9.91G [00:28<00:01, 311MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  97%|█████████▋| 9.59G/9.91G [00:28<00:00, 321MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  97%|█████████▋| 9.64G/9.91G [00:28<00:00, 329MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  98%|█████████▊| 9.68G/9.91G [00:28<00:00, 330MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  98%|█████████▊| 9.72G/9.91G [00:28<00:00, 333MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  98%|█████████▊| 9.76G/9.91G [00:28<00:00, 331MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  99%|█████████▉| 9.80G/9.91G [00:28<00:00, 332MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";:  99%|█████████▉| 9.85G/9.91G [00:28<00:00, 342MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";: 100%|█████████▉| 9.89G/9.91G [00:28<00:00, 355MB/s]#033[A\n",
      "Downloading (…)00001-of-00002.bin\";: 100%|██████████| 9.91G/9.91G [00:29<00:00, 341MB/s]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.13s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.10s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.10s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.15s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.15s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.15s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.15s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:29<00:29, 29.15s/it]\n",
      "Downloading (…)00002-of-00002.bin\";:   0%|          | 0.00/3.94G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   1%|          | 41.9M/3.94G [00:00<00:11, 342MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   2%|▏         | 94.4M/3.94G [00:00<00:09, 413MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   4%|▎         | 147M/3.94G [00:00<00:08, 437MB/s] #033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   5%|▌         | 199M/3.94G [00:00<00:09, 376MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   6%|▋         | 252M/3.94G [00:00<00:09, 394MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   7%|▋         | 294M/3.94G [00:00<00:09, 400MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:   9%|▊         | 336M/3.94G [00:00<00:09, 398MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  10%|▉         | 388M/3.94G [00:00<00:08, 411MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  11%|█         | 440M/3.94G [00:01<00:08, 427MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  13%|█▎        | 493M/3.94G [00:01<00:09, 363MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  14%|█▍        | 545M/3.94G [00:01<00:08, 389MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  15%|█▌        | 598M/3.94G [00:01<00:08, 409MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  17%|█▋        | 650M/3.94G [00:01<00:07, 421MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  18%|█▊        | 703M/3.94G [00:01<00:07, 428MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  19%|█▉        | 755M/3.94G [00:01<00:07, 442MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  21%|██        | 807M/3.94G [00:01<00:06, 453MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  22%|██▏       | 860M/3.94G [00:02<00:06, 460MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  23%|██▎       | 912M/3.94G [00:02<00:06, 464MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  25%|██▍       | 965M/3.94G [00:02<00:06, 468MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  26%|██▌       | 1.02G/3.94G [00:02<00:06, 470MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  27%|██▋       | 1.07G/3.94G [00:02<00:06, 471MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  28%|██▊       | 1.12G/3.94G [00:02<00:05, 473MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  30%|██▉       | 1.17G/3.94G [00:02<00:06, 439MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  31%|███       | 1.23G/3.94G [00:02<00:06, 441MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  32%|███▏      | 1.28G/3.94G [00:02<00:05, 450MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  34%|███▍      | 1.33G/3.94G [00:03<00:05, 458MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  35%|███▌      | 1.38G/3.94G [00:03<00:05, 463MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  36%|███▋      | 1.44G/3.94G [00:03<00:05, 466MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  38%|███▊      | 1.49G/3.94G [00:03<00:05, 468MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  39%|███▉      | 1.54G/3.94G [00:03<00:05, 468MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  40%|████      | 1.59G/3.94G [00:03<00:04, 470MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  42%|████▏     | 1.65G/3.94G [00:03<00:04, 473MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  43%|████▎     | 1.70G/3.94G [00:03<00:04, 473MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  44%|████▍     | 1.75G/3.94G [00:03<00:04, 475MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  46%|████▌     | 1.80G/3.94G [00:04<00:04, 477MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  47%|████▋     | 1.86G/3.94G [00:04<00:04, 484MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  49%|████▊     | 1.92G/3.94G [00:04<00:04, 498MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  50%|█████     | 1.97G/3.94G [00:04<00:03, 503MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  51%|█████▏    | 2.02G/3.94G [00:04<00:04, 432MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  53%|█████▎    | 2.08G/3.94G [00:04<00:04, 443MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  54%|█████▍    | 2.14G/3.94G [00:04<00:03, 469MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  56%|█████▌    | 2.19G/3.94G [00:04<00:03, 482MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  57%|█████▋    | 2.25G/3.94G [00:05<00:03, 495MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  59%|█████▊    | 2.31G/3.94G [00:05<00:03, 501MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  60%|█████▉    | 2.36G/3.94G [00:05<00:03, 505MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  61%|██████▏   | 2.41G/3.94G [00:05<00:02, 509MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  63%|██████▎   | 2.46G/3.94G [00:05<00:02, 511MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  64%|██████▍   | 2.53G/3.94G [00:05<00:02, 519MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  66%|██████▌   | 2.59G/3.94G [00:05<00:02, 521MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  67%|██████▋   | 2.64G/3.94G [00:05<00:02, 521MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  68%|██████▊   | 2.69G/3.94G [00:05<00:02, 520MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  70%|███████   | 2.76G/3.94G [00:05<00:02, 522MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  71%|███████▏  | 2.81G/3.94G [00:06<00:02, 522MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  73%|███████▎  | 2.87G/3.94G [00:06<00:02, 524MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  75%|███████▍  | 2.94G/3.94G [00:06<00:01, 525MB/s]\n",
      "#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  76%|███████▌  | 3.00G/3.94G [00:06<00:01, 525MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  78%|███████▊  | 3.06G/3.94G [00:06<00:01, 524MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  79%|███████▉  | 3.12G/3.94G [00:06<00:01, 524MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  81%|████████  | 3.18G/3.94G [00:06<00:01, 522MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  82%|████████▏ | 3.24G/3.94G [00:06<00:01, 526MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  84%|████████▍ | 3.30G/3.94G [00:07<00:01, 523MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  85%|████████▌ | 3.37G/3.94G [00:07<00:01, 525MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  87%|████████▋ | 3.43G/3.94G [00:07<00:00, 513MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  88%|████████▊ | 3.48G/3.94G [00:07<00:00, 496MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  90%|████████▉ | 3.53G/3.94G [00:07<00:00, 473MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  91%|█████████ | 3.59G/3.94G [00:07<00:00, 454MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  92%|█████████▏| 3.64G/3.94G [00:07<00:00, 434MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  94%|█████████▎| 3.69G/3.94G [00:07<00:00, 434MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  95%|█████████▌| 3.74G/3.94G [00:07<00:00, 454MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  96%|█████████▋| 3.80G/3.94G [00:08<00:00, 471MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  98%|█████████▊| 3.85G/3.94G [00:08<00:00, 486MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";:  99%|█████████▉| 3.91G/3.94G [00:08<00:00, 496MB/s]#033[A\n",
      "Downloading (…)00002-of-00002.bin\";: 100%|██████████| 3.94G/3.94G [00:08<00:00, 470MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.97s/it]#015Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.80s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.96s/it]#015Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.79s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.97s/it]#015Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.79s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.97s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.79s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.96s/it]#015Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.79s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.97s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.80s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 16.99s/it]#015Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.81s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 17.00s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:37<00:00, 18.82s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.52s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.18s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.57s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.27s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.55s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.76s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.99s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.12s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.65s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.38s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00,  9.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.72s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.08s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.41s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.29s/it]\n",
      "[2023-04-28 08:41:02,117] [INFO] [comm.py:652:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.19s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 11.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.27s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.13s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.15s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.32s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.14s/it]\n",
      "NCCL version 2.14.3+cuda11.7\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu117/cpu_adam...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\n",
      "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\n",
      "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.794156551361084 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.797168731689453 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.70408797264099 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.81994652748108 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.80345582962036 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.80047583580017 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.79069685935974 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 27.816258192062378 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\n",
      "[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.826326608657837 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.627042770385742 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.731390476226807 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.726452112197876 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.727780818939209 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.82829737663269 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.827976703643799 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 14.827982902526855 seconds\n",
      "Parameter Offload: Total persistent parameters: 1712128 in 258 params\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003573894500732422 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0003457069396972656 seconds\n",
      "Time to load utils op: 0.00034618377685546875 seconds\n",
      "Time to load utils op: 0.000400543212890625 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00046753883361816406 seconds\n",
      "Time to load utils op: 0.0003352165222167969 seconds\n",
      "Time to load utils op: 0.0005376338958740234 seconds\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-04-28 08:42:22.821: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00032639503479003906 seconds\n",
      "0%|          | 0/113 [00:00<?, ?it/s]\n",
      "[2023-04-28 08:42:22.850 algo-1:302 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:300 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:304 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:305 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:306 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:303 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.850 algo-1:301 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.880 algo-1:306 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:305 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:300 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:302 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:304 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:303 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:301 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.880 algo-1:306 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.880 algo-1:305 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.880 algo-1:300 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.880 algo-1:302 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.880 algo-1:304 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.880 algo-1:303 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.881 algo-1:306 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:305 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:300 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:302 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:304 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:303 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:301 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.881 algo-1:301 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.881 algo-1:306 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:305 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:300 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:306 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.881 algo-1:304 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:302 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:305 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.881 algo-1:303 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.881 algo-1:300 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.881 algo-1:304 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.881 algo-1:302 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.881 algo-1:303 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.882 algo-1:301 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.882 algo-1:301 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-04-28 08:42:22.924: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-28 08:42:22.951 algo-1:299 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-28 08:42:22.980 algo-1:299 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-28 08:42:22.981 algo-1:299 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-28 08:42:22.981 algo-1:299 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-28 08:42:22.981 algo-1:299 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-28 08:42:22.981 algo-1:299 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:1 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:3 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:5 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:6 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:4 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:7 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      "│ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      "│                                                                              │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│ ❱ 144 │   main()                                                             │\n",
      "│   145                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      "│                                                                              │\n",
      "│   137                                                                        │\n",
      "│   138 def main():                                                            │\n",
      "│   139 │   args, _ = parse_arge()                                             │\n",
      "│ ❱ 140 │   training_function(args)                                            │\n",
      "│   141                                                                        │\n",
      "│   142                                                                        │\n",
      "│   143 if __name__ == \"__main__\":                                             │\n",
      "│                                                                              │\n",
      "│ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      "│                                                                              │\n",
      "│   125 │   )                                                                  │\n",
      "│   126 │                                                                      │\n",
      "│   127 │   # Start training                                                   │\n",
      "│ ❱ 128 │   trainer.train()                                                    │\n",
      "│   129 │                                                                      │\n",
      "│   130 │                                                                      │\n",
      "│   131 │   # save model                                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      "│                                                                              │\n",
      "│   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      "│   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      "│   1661 │   │   )                                                             │\n",
      "│ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      "│   1663 │   │   │   args=args,                                                │\n",
      "│   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      "│   1665 │   │   │   trial=trial,                                              │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      "│ _inner_training_loop                                                         │\n",
      "│                                                                              │\n",
      "│   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      "│   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      "│   1928 │   │   │   │   else:                                                 │\n",
      "│ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      "│   1930 │   │   │   │                                                         │\n",
      "│   1931 │   │   │   │   if (                                                  │\n",
      "│   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      "│ training_step                                                                │\n",
      "│                                                                              │\n",
      "│   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      "│   2697 │   │                                                                 │\n",
      "│   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      "│ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      "│   2700 │   │                                                                 │\n",
      "│   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      "│   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      "│ compute_loss                                                                 │\n",
      "│                                                                              │\n",
      "│   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      "│   2729 │   │   else:                                                         │\n",
      "│   2730 │   │   │   labels = None                                             │\n",
      "│ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      "│   2732 │   │   # Save past state if it exists                                │\n",
      "│   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      "│   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1191 │   │   # this function, and just call forward.                       │\n",
      "│   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      "│   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      "│ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      "│   1195 │   │   # Do not call functions when jit is used                      │\n",
      "│   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      "│   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      "│ wrapped_fn                                                                   │\n",
      "│                                                                              │\n",
      "│    8 │   function call.\"\"\"                                                   │\n",
      "│    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      "│   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      "│ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      "│   12 │   │   get_accelerator().range_pop()                                   │\n",
      "│   13 │   │   return ret_val                                                  │\n",
      "│   14                                                                         │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      "│   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      "│   1845 │   │                                                                 │\n",
      "│ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      "│   1847 │   │                                                                 │\n",
      "│   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      "│   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:662 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   659 │   │   ```\"\"\"                                                         │\n",
      "│   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      "│   661 │   │                                                                  │\n",
      "│ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      "│   663 │   │   │   input_ids,                                                 │\n",
      "│   664 │   │   │   attention_mask=attention_mask,                             │\n",
      "│   665 │   │   │   position_ids=position_ids,                                 │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      "│ _gpt_neox.py:518 in forward                                                  │\n",
      "│                                                                              │\n",
      "│   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      "│   516 │   │                                                                  │\n",
      "│   517 │   │   if inputs_embeds is None:                                      │\n",
      "│ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      "│   519 │   │                                                                  │\n",
      "│   520 │   │   hidden_states = inputs_embeds                                  │\n",
      "│   521                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      "│ _call_impl                                                                   │\n",
      "│                                                                              │\n",
      "│   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      "│   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      "│   1211 │   │                                                                 │\n",
      "│ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      "│   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      "│   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      "│   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      "│ new_forward                                                                  │\n",
      "│                                                                              │\n",
      "│   162 │   │   │   with torch.no_grad():                                      │\n",
      "│   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      "│   164 │   │   else:                                                          │\n",
      "│ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      "│   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      "│   167 │                                                                      │\n",
      "│   168 │   module.forward = new_forward                                       │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      "│ forward                                                                      │\n",
      "│                                                                              │\n",
      "│   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      "│   158 │                                                                      │\n",
      "│   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      "│ ❱ 160 │   │   return F.embedding(                                            │\n",
      "│   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      "│   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      "│   163                                                                        │\n",
      "│                                                                              │\n",
      "│ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      "│ embedding                                                                    │\n",
      "│                                                                              │\n",
      "│   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      "│   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      "│   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      "│ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      "│   2211                                                                       │\n",
      "│   2212                                                                       │\n",
      "│   2213 def embedding_bag(                                                    │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least \n",
      "two devices, cuda:2 and cuda:0! (when checking argument for argument index in \n",
      "method wrapper__index_select)\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 300 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 301 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 302 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 303 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 304 closing signal SIGTERM\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 305 closing signal SIGTERM\n",
      "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 306) of binary: /opt/conda/bin/python3.9\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      "sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\n",
      "run(args)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\n",
      "elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      "raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "run_clm.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2023-04-28_08:42:31\n",
      "  host      : algo-1\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 1 (pid: 306)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "2023-04-28 08:42:39,821 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-28 08:42:39,822 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "2023-04-28 08:42:39,823 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2023-04-28 08:42:39,823 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"RuntimeError: Expected all tensors to be on the same device, but found at least\n",
      " two devices, cuda:1 and cuda:0! (when checking argument for argument index in\n",
      " method wrapper__index_select)\n",
      " ╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
      " │ /opt/ml/code/run_clm.py:144 in <module>                                      │\n",
      " │                                                                              │\n",
      " │   141                                                                        │\n",
      " │   142                                                                        │\n",
      " │   143 if __name__ == \"__main__\":                                             │\n",
      " │ ❱ 144 │   main()                                                             │\n",
      " │   145                                                                        │\n",
      " │ /opt/ml/code/run_clm.py:140 in main                                          │\n",
      " │   137                                                                        │\n",
      " │   138 def main():                                                            │\n",
      " │   139 │   args, _ = parse_arge()                                             │\n",
      " │ ❱ 140 │   training_function(args)                                            │\n",
      " │ /opt/ml/code/run_clm.py:128 in training_function                             │\n",
      " │   125 │   )                                                                  │\n",
      " │   126 │                                                                      │\n",
      " │   127 │   # Start training                                                   │\n",
      " │ ❱ 128 │   trainer.train()                                                    │\n",
      " │   129 │                                                                      │\n",
      " │   130 │                                                                      │\n",
      " │   131 │   # save model                                                       │\n",
      " │ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1662 in train │\n",
      " │   1659 │   │   inner_training_loop = find_executable_batch_size(             │\n",
      " │   1660 │   │   │   self._inner_training_loop, self._train_batch_size, args.a │\n",
      " │   1661 │   │   )                                                             │\n",
      " │ ❱ 1662 │   │   return inner_training_loop(                                   │\n",
      " │   1663 │   │   │   args=args,                                                │\n",
      " │   1664 │   │   │   resume_from_checkpoint=resume_from_checkpoint,            │\n",
      " │   1665 │   │   │   trial=trial,                                              │\n",
      " │ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1929 in       │\n",
      " │ _inner_training_loop                                                         │\n",
      " │   1926 │   │   │   │   │   with model.no_sync():                             │\n",
      " │   1927 │   │   │   │   │   │   tr_loss_step = self.training_step(model, inpu │\n",
      " │   1928 │   │   │   │   else:                                                 │\n",
      " │ ❱ 1929 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)  │\n",
      " │   1930 │   │   │   │                                                         │\n",
      " │   1931 │   │   │   │   if (                                                  │\n",
      " │   1932 │   │   │   │   │   args.logging_nan_inf_filter                       │\n",
      " │ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2699 in       │\n",
      " │ training_step                                                                │\n",
      " │   2696 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device │\n",
      " │   2697 │   │                                                                 │\n",
      " │   2698 │   │   with self.compute_loss_context_manager():                     │\n",
      " │ ❱ 2699 │   │   │   loss = self.compute_loss(model, inputs)                   │\n",
      " │   2700 │   │                                                                 │\n",
      " │   2701 │   │   if self.args.n_gpu > 1:                                       │\n",
      " │   2702 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu para │\n",
      " │ /opt/conda/lib/python3.9/site-packages/transformers/trainer.py:2731 in       │\n",
      " │ compute_loss                                                                 │\n",
      " │   2728 │   │   │   labels = inputs.pop(\"labels\")                             │\n",
      " │   2729 │   │   else:                                                         │\n",
      " │   2730 │   │   │   labels = None                                             │\n",
      " │ ❱ 2731 │   │   outputs = model(**inputs)                                     │\n",
      " │   2732 │   │   # Save past state if it exists                                │\n",
      " │   2733 │   │   # TODO: this needs to be fixed and made cleaner later.        │\n",
      " │   2734 │   │   if self.args.past_index >= 0:                                 │\n",
      " │ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1194 in    │\n",
      " │ _call_impl                                                                   │\n",
      " │   1191 │   │   # this function, and just call forward.                       │\n",
      " │   1192 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │\n",
      " │   1193 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │\n",
      " │ ❱ 1194 │   │   │   return forward_call(*input, **kwargs)                     │\n",
      " │   1195 │   │   # Do not call functions when jit is used                      │\n",
      " │   1196 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │\n",
      " │   1197 │   │   if self._backward_hooks or _global_backward_hooks:            │\n",
      " │ /opt/conda/lib/python3.9/site-packages/deepspeed/utils/nvtx.py:11 in         │\n",
      " │ wrapped_fn                                                                   │\n",
      " │    8 │   function call.\"\"\"                                                   │\n",
      " │    9 │   def wrapped_fn(*args, **kwargs):                                    │\n",
      " │   10 │   │   get_accelerator().range_push(func.__qualname__)                 │\n",
      " │ ❱ 11 │   │   ret_val = func(*args, **kwargs)                                 │\n",
      " │   12 │   │   get_accelerator().range_pop()                                   │\n",
      " │   13 │   │   return ret_val                                                  │\n",
      " │   14                                                                         │\n",
      " │ /opt/conda/lib/python3.9/site-packages/deepspeed/runtime/engine.py:1846 in   │\n",
      " │ forward                                                                      │\n",
      " │   1843 │   │   if self.fp16_auto_cast():                                     │\n",
      " │   1844 │   │   │   inputs = self._cast_inputs_half(inputs)                   │\n",
      " │   1845 │   │                                                                 │\n",
      " │ ❱ 1846 │   │   loss = self.module(*inputs, **kwargs)                         │\n",
      " │   1847 │   │                                                                 │\n",
      " │   1848 │   │   if self.zero_optimization_partition_weights():                │\n",
      " │   1849 │   │   │   # Disable automated discovery of external parameters      │\n",
      " │ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1212 in    │\n",
      " │   1209 │   │   │   bw_hook = hooks.BackwardHook(self, full_backward_hooks)   │\n",
      " │   1210 │   │   │   input = bw_hook.setup_input_hook(input)                   │\n",
      " │   1211 │   │                                                                 │\n",
      " │ ❱ 1212 │   │   result = forward_call(*input, **kwargs)                       │\n",
      " │   1213 │   │   if _global_forward_hooks or self._forward_hooks:              │\n",
      " │   1214 │   │   │   for hook in (*_global_forward_hooks.values(), *self._forw │\n",
      " │   1215 │   │   │   │   hook_result = hook(self, input, result)               │\n",
      " │ /opt/conda/lib/python3.9/site-packages/accelerate/hooks.py:165 in            │\n",
      " │ new_forward                                                                  │\n",
      " │   162 │   │   │   with torch.no_grad():                                      │\n",
      " │   163 │   │   │   │   output = old_forward(*args, **kwargs)                  │\n",
      " │   164 │   │   else:                                                          │\n",
      " │ ❱ 165 │   │   │   output = old_forward(*args, **kwargs)                      │\n",
      " │   166 │   │   return module._hf_hook.post_forward(module, output)            │\n",
      " │   167 │                                                                      │\n",
      " │   168 │   module.forward = new_forward                                       │\n",
      " │ /opt/conda/lib/python3.9/site-packages/transformers/models/gpt_neox/modeling │\n",
      " │ _gpt_neox.py:662 in forward                                                  │\n",
      " │   659 │   │   ```\"\"\"                                                         │\n",
      " │   660 │   │   return_dict = return_dict if return_dict is not None else self │\n",
      " │   661 │   │                                                                  │\n",
      " │ ❱ 662 │   │   outputs = self.gpt_neox(                                       │\n",
      " │   663 │   │   │   input_ids,                                                 │\n",
      " │   664 │   │   │   attention_mask=attention_mask,                             │\n",
      " │   665 │   │   │   position_ids=position_ids,                                 │\n",
      " │ _gpt_neox.py:518 in forward                                                  │\n",
      " │   515 │   │   head_mask = self.get_head_mask(head_mask, self.config.num_hidd │\n",
      " │   516 │   │                                                                  │\n",
      " │   517 │   │   if inputs_embeds is None:                                      │\n",
      " │ ❱ 518 │   │   │   inputs_embeds = self.embed_in(input_ids)                   │\n",
      " │   519 │   │                                                                  │\n",
      " │   520 │   │   hidden_states = inputs_embeds                                  │\n",
      " │   521                                                                        │\n",
      " │ /opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py:160 in     │\n",
      " │   157 │   │   │   │   self.weight[self.padding_idx].fill_(0)                 │\n",
      " │   158 │                                                                      │\n",
      " │   159 │   def forward(self, input: Tensor) -> Tensor:                        │\n",
      " │ ❱ 160 │   │   return F.embedding(                                            │\n",
      " │   161 │   │   │   input, self.weight, self.padding_idx, self.max_norm,       │\n",
      " │   162 │   │   │   self.norm_type, self.scale_grad_by_freq, self.sparse)      │\n",
      " │   163                                                                        │\n",
      " │ /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:2210 in        │\n",
      " │ embedding                                                                    │\n",
      " │   2207 │   │   #   torch.embedding_renorm_                                   │\n",
      " │   2208 │   │   # remove once script supports set_grad_enabled                │\n",
      " │   2209 │   │   _no_grad_embedding_renorm_(weight, input, max_norm, norm_type │\n",
      " │ ❱ 2210 │   return torch.embedding(weight, input, padding_idx, scale_grad_by_ │\n",
      " │   2211                                                                       │\n",
      " │   2212                                                                       │\n",
      " │   2213 def embedding_bag(                                                    │\n",
      " ╰──────────────────────────────────────────────────────────────────────────────╯\n",
      " two devices, cuda:3 and cuda:0! (when checking argument for argument index in\n",
      " two devices, cuda:5 and cuda:0! (when checking argument for argument index in\n",
      " two devices, cuda:6 and cuda:0! (when checking argument for argument index in\n",
      " two devices, cuda:4 and cuda:0! (when checking argument for argument index in\n",
      " two devices, cuda:7 and cuda:0! (when checking argument for argument index in\n",
      " two devices, cuda:2 and cuda:0! (when checking argument for argument index in\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 300 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 301 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 302 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 303 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 304 closing signal SIGTERM\n",
      " WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 305 closing signal SIGTERM\n",
      " ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 306) of binary: /opt/conda/bin/python3.9\n",
      " Traceback (most recent call last)\n",
      " File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      " sys.exit(main())\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      " return f(*args, **kwargs)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\n",
      " run(args)\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\n",
      " elastic_launch(\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      " return launch_agent(self._config, self._entrypoint, list(args))\n",
      " File \"/opt/conda/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 246, in launch_agent\n",
      " raise ChildFailedError(\n",
      " torch.distributed.elastic.multiprocessing.errors.ChildFailedError\n",
      " ============================================================\n",
      " run_clm.py FAILED\n",
      " ------------------------------------------------------------\n",
      " Failures\n",
      " <NO_OTHER_FAILURES>\n",
      " Root Cause (first observed failure)\n",
      " [0]\n",
      " time      : 2023-04-28_08:42:31\n",
      " host      : algo-1\n",
      " rank      : 7 (local_rank: 7)\n",
      " exitcode  : 1 (pid: 306)\n",
      " error_file: <N/A>\n",
      " traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\"\n",
      "Command \"torchrun --nnodes 1 --nproc_per_node 8 run_clm.py --bf16 True --dataset_path /opt/ml/input/data/training --ds_config ds_offload.json --epochs 1 --gradient_checkpointing True --model_id EleutherAI/pythia-6.9b --per_device_train_batch_size 1\"\n",
      "2023-04-28 08:42:39,823 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\n",
      "2023-04-28 08:42:56 Uploading - Uploading generated training model\n",
      "2023-04-28 08:42:56 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2023-04-28-08-32-57-582: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: Expected all tensors to be on the same device, but found at least\n two devices, cuda:1 and cuda:0! (when checking argument for argument index in\n method wrapper__index_select)\n ╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n │ /opt/ml/code/run_clm.py:144 in <module>                                      │\n │                                                                              │\n │   141                                                                        │\n │   142                                                                        │\n │   143 if __name__ == \"__main__\":                                             │\n │ ❱ 144 │   main()                                                             │\n │   145                                                                        │\n │ /o, exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/notebooks/sagemaker/25_pytorch_fsdp_model_parallelism/sagemaker-notebook.ipynb Cell 17\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/notebooks/sagemaker/25_pytorch_fsdp_model_parallelism/sagemaker-notebook.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m: training_input_path}\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/notebooks/sagemaker/25_pytorch_fsdp_model_parallelism/sagemaker-notebook.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/notebooks/sagemaker/25_pytorch_fsdp_model_parallelism/sagemaker-notebook.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m huggingface_estimator\u001b[39m.\u001b[39;49mfit(data, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/estimator.py:1198\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1197\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 1198\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/estimator.py:2344\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   2345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:4628\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4607\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4608\u001b[0m     \u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4609\u001b[0m \n\u001b[1;32m   4610\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4626\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4627\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4628\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:6490\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6487\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6489\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 6490\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   6491\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   6492\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:6543\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   6538\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   6539\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6540\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6541\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6542\u001b[0m     )\n\u001b[0;32m-> 6543\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6544\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6545\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6546\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6547\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2023-04-28-08-32-57-582: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"RuntimeError: Expected all tensors to be on the same device, but found at least\n two devices, cuda:1 and cuda:0! (when checking argument for argument index in\n method wrapper__index_select)\n ╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n │ /opt/ml/code/run_clm.py:144 in <module>                                      │\n │                                                                              │\n │   141                                                                        │\n │   142                                                                        │\n │   143 if __name__ == \"__main__\":                                             │\n │ ❱ 144 │   main()                                                             │\n │   145                                                                        │\n │ /o, exit code: 1"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainign took `20632` seconds, which is about `5.7` hours. The `ml.g5.2xlarge` instance we used costs `$1.515` per hour. So the total cost for training BLOOMZ 7B was is `$8.63`. We could reduce the cost by using a spot instance, but the training time could increase, by waiting or restarts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
