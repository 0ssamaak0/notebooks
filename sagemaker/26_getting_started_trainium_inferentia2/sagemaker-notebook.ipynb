{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating Hugging Face Transformers with AWS Accelerators and Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help you get started on how to train and deploy Hugging Face Transformers on Amazon SageMaker using AWS Accelerators, including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia2](https://aws.amazon.com/machine-learning/inferentia/). As the field of deep learning continues to evolve, the need for efficient and cost-effective solutions to train and deploy increasingly complex transformers model has become more critical than ever. AWS purpose-built accelerators are designed to deliver high performance at the lowest cost for deep learning inference and training.\n",
    "\n",
    "This notebook walks you through an end-to-end example on how to train a RoBERTa model with Hugging Face on AWS Trainium, and deploy it on AWS SageMaker using AWS Inferentia2 accelerators for inference. Benefit from faster time-to-train, up to 50% cost-to-train savings, and up to 4x higher throughput and 10x lower latency for inference compared to its first-generation.\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. Setup AWS environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-tune RoBERTa using Hugging Face Transformers and Optimum Neuron on AWS Trainium\n",
    "4. Deploy model to inferntia2 and run inference \n",
    "\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.28.0\" \"datasets[s3]==2.9.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `emotion` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [emotion](https://github.com/dair-ai/emotion_dataset) dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples.\n",
    "\n",
    "```python\n",
    "{\n",
    "  'text': 'im feeling rather festive here in south florida', \n",
    "  'label': 1\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `emotion` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (/home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98197fec3e84e319bd6b4f525a6c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 16000\n",
      "Validation dataset size: 2000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"philschmid/emotion\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(dataset['validation'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our inputs (text) to token IDs. This is done by a 🤗 Transformers Tokenizer. If you are not sure what this means, check out **[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)** of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 07:40:26.787782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 07:40:27.399962: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-02 07:40:27.622595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:27.622608: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-02 07:40:29.439889: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:29.439944: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 07:40:29.439949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"roberta-base\"\n",
    "\n",
    "# Load tokenizer of RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS Trainium requires the inputs to be of a static shape. To optimize our training throughput want to understand how long our inputs are to efficiently pad them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-64679c6b492027f9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 88\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"text\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "print(f\"Max source length: {max_source_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our max sequence length is 87. We are going to use 128 as our max sequence length for training and inference and pad all inputs to this length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0bd063247c4121923a284caf7173c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-a4f094e327cece66.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/philschmid___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd/cache-b3c48eb89cf1fc2f.arrow\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length',max_length=MAX_LENGTH, truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "# rename label to labels to match the expected input\n",
    "train_dataset =  tokenized_dataset['train'].rename_column(\"label\", \"labels\")\n",
    "validation_dataset =  tokenized_dataset['validation'].rename_column(\"label\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets print another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([    0,   118,  2198,  1346,    14,    51,   115,  3999,    33,    41,\n",
      "         3031, 24672,    53,  1782,    24,    95, 10122,    15, 19750,     5,\n",
      "          619,     9,     5,   157,   626,   278,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "print(tokenized_dataset['train'][randint(0, len(dataset['train']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537cb1a78fb4470c845e323be21c7ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3371d12bbf94b1084f74129405b8fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/startup-loft/train'\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/startup-loft/test'\n",
    "validation_dataset.save_to_disk(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tune RoBERTa using Hugging Face Transformers and Optimum Neuron on AWS Trainium\n",
    "\n",
    "Normally we would use the [Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer) and [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments) to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `TrainiumTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `TrainiumTrainer` also comes with a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step, which would be needed at the beginning of training. This can reduce the training time by ~3x. \n",
    "\n",
    "The `TrainiumTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`. You only have to adjust the import in your training script. \n",
    "\n",
    "```diff\n",
    "- from transformers import Trainer\n",
    "+ from optimum.neuron import TrainiumTrainer as Trainer\n",
    "```\n",
    "\n",
    "We prepared a simple [train.py](./scripts/train.py) training script based on the [\"Getting started with Pytorch 2.0 and Hugging Face Transformers”](https://www.philschmid.de/getting-started-pytorch-2-0-transformers#3-fine-tune--evaluate-bert-model-with-the-hugging-face-trainer) blog post with the `TrainiumTrainier`.\n",
    "\n",
    "In order to train on Amazon SageMaker we need to create a `HuggingFace` Estimator. The Estimator defines, which fine-tuning script (`entry_point`), which `instance_type`, which `hyperparameters`, etc should be used.\n",
    "\n",
    "Amazon SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainium_image_uri=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-fsdp-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id': model_id, # model id from huggingface.co/models\n",
    "    'lr': 5e-5, # enable gradient checkpointing\n",
    "    'bf16': True, # enable mixed precision training\n",
    "    'per_device_train_batch_size': 16, # optimizer\n",
    "    'epochs': 3, # number of epochs to train\n",
    "}\n",
    "\n",
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train_with_export.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.2xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=200,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    image_uri=trainium_image_uri,\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-02 07:40:44 Starting - Starting the training job...\n",
      "2023-06-02 07:40:59 Starting - Preparing the instances for training.........\n",
      "2023-06-02 07:42:32 Downloading - Downloading input data...\n",
      "2023-06-02 07:42:52 Training - Downloading the training image..............................\n",
      "2023-06-02 07:47:59 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-06-02 07:48:33,753 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-06-02 07:48:33,755 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-06-02 07:48:34,797 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\n",
      "2023-06-02 07:48:34,808 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-06-02 07:48:34,810 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-06-02 07:48:34,810 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-06-02 07:48:34,975 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-06-02 07:48:36,013 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\n",
      "2023-06-02 07:48:36,026 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-06-02 07:48:37,069 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\n",
      "2023-06-02 07:48:37,080 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-06-02 07:48:37,082 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-06-02 07:48:38,125 sagemaker-training-toolkit INFO     Found 2 neurons on this instance\n",
      "2023-06-02 07:48:38,136 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.trn1.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bf16\": true,\n",
      "        \"epochs\": 3,\n",
      "        \"lr\": 5e-05,\n",
      "        \"model_id\": \"roberta-base\",\n",
      "        \"per_device_train_batch_size\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_with_export\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 2,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.trn1.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.trn1.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_with_export.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"bf16\":true,\"epochs\":3,\"lr\":5e-05,\"model_id\":\"roberta-base\",\"per_device_train_batch_size\":16}\n",
      "SM_USER_ENTRY_POINT=train_with_export.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.trn1.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.trn1.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train_with_export\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=2\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.trn1.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.trn1.2xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bf16\":true,\"epochs\":3,\"lr\":5e-05,\"model_id\":\"roberta-base\",\"per_device_train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-pytorch-training-neuronx-2023-06-02-07-40-42-712/source/sourcedir.tar.gz\",\"module_name\":\"train_with_export\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":2,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.trn1.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.trn1.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_with_export.py\"}\n",
      "SM_USER_ARGS=[\"--bf16\",\"True\",\"--epochs\",\"3\",\"--lr\",\"5e-05\",\"--model_id\",\"roberta-base\",\"--per_device_train_batch_size\",\"16\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_BF16=true\n",
      "SM_HP_EPOCHS=3\n",
      "SM_HP_LR=5e-05\n",
      "SM_HP_MODEL_ID=roberta-base\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=16\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python38.zip:/usr/local/lib/python3.8:/usr/local/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 2 train_with_export.py --bf16 True --epochs 3 --lr 5e-05 --model_id roberta-base --per_device_train_batch_size 16\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]\n",
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 11.8MB/s]\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 481/481 [00:00<00:00, 123kB/s]\n",
      "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 44.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 137MB/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 128MB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 41.9M/501M [00:00<00:01, 381MB/s]\n",
      "Downloading pytorch_model.bin:  19%|█▉        | 94.4M/501M [00:00<00:00, 417MB/s]\n",
      "Downloading pytorch_model.bin:  29%|██▉       | 147M/501M [00:00<00:00, 433MB/s]\n",
      "Downloading pytorch_model.bin:  40%|███▉      | 199M/501M [00:00<00:00, 461MB/s]\n",
      "Downloading pytorch_model.bin:  50%|█████     | 252M/501M [00:00<00:00, 467MB/s]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 304M/501M [00:00<00:00, 481MB/s]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 357M/501M [00:00<00:00, 478MB/s]\n",
      "Downloading pytorch_model.bin:  82%|████████▏ | 409M/501M [00:00<00:00, 475MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 461M/501M [00:01<00:00, 464MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 501M/501M [00:01<00:00, 461MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Disabling DDP because it is currently not playing well with multiple workers training, for more information please refer to https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/finetune_hftrainer.html#multi-worker-training\n",
      "Disabling DDP because it is currently not playing well with multiple workers training, for more information please refer to https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/finetune_hftrainer.html#multi-worker-training\n",
      "***** Running training *****\n",
      "***** Running training *****\n",
      "  Num examples = 16,000\n",
      "Num examples = 16,000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,500\n",
      "Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,500\n",
      "Number of trainable parameters = 124,650,246\n",
      "Number of trainable parameters = 124,650,246\n",
      "0%|          | 0/1500 [00:00<?, ?it/s]\n",
      "2023-06-02 07:48:52.000915: INFO ||NCC_WRAPPER||: No candidate found under /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_277530155584971264.\n",
      "2023-06-02 07:48:52.000916: INFO ||NCC_WRAPPER||: Cache dir for the neff: /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_277530155584971264/MODULE_0_SyncTensorsGraph.206_277530155584971264_algo-1-9e35bc0e-110-5fd20c8c7ad73/6ec70840-d793-4b33-ade2-6605b27133db\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-06-02 07:48:54.000673: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]#033[A\n",
      "Downloading (…)9d/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)9d/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4eaf3015ff.neff:   0%|          | 0.00/9.61k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)9-5fb4eaf3015ff.neff: 100%|██████████| 9.61k/9.61k [00:00<00:00, 2.57MB/s]\n",
      "Downloading (…)9-5fb4e83155a6a.neff:   0%|          | 0.00/126k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9-5fb4e83155a6a.neff: 100%|██████████| 126k/126k [00:00<00:00, 19.6MB/s]\n",
      "Downloading (…)7b/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)7b/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)2-5fb56cc4d351f.neff:   0%|          | 0.00/126k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)2-5fb56cc4d351f.neff: 100%|██████████| 126k/126k [00:00<00:00, 127MB/s]\n",
      "Downloading (…)5fb4eaf3015ff.hlo.pb:   0%|          | 0.00/981 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eaf3015ff.hlo.pb: 100%|██████████| 981/981 [00:00<00:00, 271kB/s]\n",
      "Fetching 15 files:   7%|▋         | 1/15 [00:00<00:02,  6.27it/s]#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4e83155a6a.hlo.pb:   0%|          | 0.00/40.6k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4e83155a6a.hlo.pb: 100%|██████████| 40.6k/40.6k [00:00<00:00, 9.00MB/s]\n",
      "Downloading (…)5fb4e8367093e.hlo.pb:   0%|          | 0.00/1.27M [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4e8367093e.hlo.pb: 100%|██████████| 1.27M/1.27M [00:00<00:00, 93.1MB/s]\n",
      "Downloading (…)99/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)99/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4e8367093e.neff:   0%|          | 0.00/9.01M [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb56cc4d351f.hlo.pb:   0%|          | 0.00/40.6k [00:00<?, ?B/s]\n",
      "#033[A#033[A#033[A\n",
      "Downloading (…)5fb56cc4d351f.hlo.pb: 100%|██████████| 40.6k/40.6k [00:00<00:00, 29.7MB/s]\n",
      "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Downloading (…)5fb4e94d748b7.hlo.pb:   0%|          | 0.00/1.34M [00:00<?, ?B/s]#033[A#033[A#033[A\n",
      "Downloading (…)9-5fb4eaf3015ff.neff:   0%|          | 0.00/9.61k [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)9-5fb4e8367093e.neff: 100%|██████████| 9.01M/9.01M [00:00<00:00, 83.5MB/s]#033[A#033[A\n",
      "Downloading (…)9d/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4e94d748b7.hlo.pb: 100%|██████████| 1.34M/1.34M [00:00<00:00, 85.1MB/s]\n",
      "Downloading (…)9-5fb4e8367093e.neff: 100%|██████████| 9.01M/9.01M [00:00<00:00, 80.9MB/s]\n",
      "Downloading (…)9-5fb4eaf3015ff.neff: 100%|██████████| 9.61k/9.61k [00:00<00:00, 654kB/s]\n",
      "Fetching 15 files:  27%|██▋       | 4/15 [00:00<00:01, 10.54it/s]#033[A\n",
      "Downloading (…)9-5fb4e94d748b7.neff:   0%|          | 0.00/9.95M [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9d/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4eaf3015ff.hlo.pb:   0%|          | 0.00/981 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb4eaf3015ff.hlo.pb: 100%|██████████| 981/981 [00:00<00:00, 221kB/s]\n",
      "Downloading (…)9-5fb4e83155a6a.neff:   0%|          | 0.00/126k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4e83155a6a.neff: 100%|██████████| 126k/126k [00:00<00:00, 8.86MB/s]\n",
      "Downloading (…)9-5fb4e94d748b7.neff: 100%|██████████| 9.95M/9.95M [00:00<00:00, 248MB/s]\n",
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 33.64it/s]\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)2-5fb56cc4d351f.neff:   0%|          | 0.00/126k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)2-5fb56cc4d351f.neff: 100%|██████████| 126k/126k [00:00<00:00, 27.0MB/s]\n",
      "Downloading (…)5fb4e83155a6a.hlo.pb:   0%|          | 0.00/40.6k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)5fb4e83155a6a.hlo.pb: 100%|██████████| 40.6k/40.6k [00:00<00:00, 20.0MB/s]\n",
      "Downloading (…)9-5fb4e8367093e.neff:   0%|          | 0.00/9.01M [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)b3/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)7b/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)7b/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4e8367093e.hlo.pb:   0%|          | 0.00/1.27M [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4e8367093e.hlo.pb: 100%|██████████| 1.27M/1.27M [00:00<00:00, 124MB/s]\n",
      "Downloading (…)5fb56cc4d351f.hlo.pb:   0%|          | 0.00/40.6k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4e94d748b7.hlo.pb:   0%|          | 0.00/1.34M [00:00<?, ?B/s]#033[A#033[A#033[A\n",
      "Downloading (…)5fb56cc4d351f.hlo.pb: 100%|██████████| 40.6k/40.6k [00:00<00:00, 6.77MB/s]\n",
      "Downloading (…)9-5fb4e94d748b7.neff:   0%|          | 0.00/9.95M [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9-5fb4e8367093e.neff: 100%|██████████| 9.01M/9.01M [00:00<00:00, 128MB/s]\n",
      "Downloading (…)5fb4e94d748b7.hlo.pb: 100%|██████████| 1.34M/1.34M [00:00<00:00, 94.2MB/s]\n",
      "Fetching 15 files:  27%|██▋       | 4/15 [00:00<00:00, 17.72it/s]\n",
      "0%|          | 1/1500 [00:04<2:04:06,  4.97s/it]\n",
      "Downloading (…)99/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)99/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4e94d748b7.neff: 100%|██████████| 9.95M/9.95M [00:00<00:00, 170MB/s]\n",
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 53.59it/s]\n",
      "2023-06-02 07:48:58.000938: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_13800415530336736598/MODULE_1_SyncTensorsGraph.22493_13800415530336736598_algo-1-9a186a03-109-5fb4e8367093e/97d1472a-e49d-482a-b426-5d15ef66777b/MODULE_1_SyncTensorsGraph.22493_13800415530336736598_algo-1-9a186a03-109-5fb4e8367093e.neff. Exiting with a successfully compiled graph\n",
      "2023-Jun-02 07:48:59.0910 110:124 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-Jun-02 07:48:59.0910 110:124 [0] init.cc:99 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "0%|          | 2/1500 [00:07<1:23:23,  3.34s/it]\n",
      "2023-06-02 07:49:03.000278: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_3569918580282600246/MODULE_2_SyncTensorsGraph.22493_3569918580282600246_algo-1-e53e3ee6-109-5fb4e94d748b7/94732337-32d3-41d7-9e78-80817241a7b3/MODULE_2_SyncTensorsGraph.22493_3569918580282600246_algo-1-e53e3ee6-109-5fb4e94d748b7.neff. Exiting with a successfully compiled graph\n",
      "0%|          | 3/1500 [00:11<1:36:54,  3.88s/it]\n",
      "0%|          | 4/1500 [00:14<1:22:36,  3.31s/it]\n",
      "0%|          | 5/1500 [00:14<53:45,  2.16s/it]\n",
      "0%|          | 6/1500 [00:14<36:19,  1.46s/it]\n",
      "0%|          | 7/1500 [00:14<25:23,  1.02s/it]\n",
      "1%|          | 8/1500 [00:14<18:13,  1.36it/s]\n",
      "1%|          | 9/1500 [00:14<13:30,  1.84it/s]\n",
      "1%|          | 11/1500 [00:14<08:18,  2.98it/s]\n",
      "1%|          | 12/1500 [00:15<06:57,  3.57it/s]\n",
      "1%|          | 13/1500 [00:15<05:48,  4.27it/s]\n",
      "1%|          | 14/1500 [00:15<04:57,  4.99it/s]\n",
      "1%|          | 16/1500 [00:15<03:54,  6.32it/s]\n",
      "1%|          | 18/1500 [00:15<03:19,  7.42it/s]\n",
      "1%|▏         | 19/1500 [00:15<03:11,  7.74it/s]\n",
      "1%|▏         | 20/1500 [00:15<03:03,  8.08it/s]\n",
      "1%|▏         | 22/1500 [00:16<02:48,  8.79it/s]\n",
      "2%|▏         | 23/1500 [00:16<02:43,  9.01it/s]\n",
      "2%|▏         | 24/1500 [00:16<02:43,  9.03it/s]\n",
      "2%|▏         | 25/1500 [00:16<02:40,  9.19it/s]\n",
      "2%|▏         | 26/1500 [00:16<02:39,  9.26it/s]\n",
      "2%|▏         | 27/1500 [00:16<02:38,  9.30it/s]\n",
      "2%|▏         | 28/1500 [00:16<02:50,  8.65it/s]\n",
      "2%|▏         | 30/1500 [00:16<02:37,  9.32it/s]\n",
      "2%|▏         | 31/1500 [00:17<02:40,  9.14it/s]\n",
      "2%|▏         | 33/1500 [00:17<02:34,  9.49it/s]\n",
      "2%|▏         | 34/1500 [00:17<02:33,  9.57it/s]\n",
      "2%|▏         | 35/1500 [00:17<02:34,  9.46it/s]\n",
      "2%|▏         | 36/1500 [00:17<02:38,  9.24it/s]\n",
      "2%|▏         | 37/1500 [00:17<02:35,  9.42it/s]\n",
      "3%|▎         | 38/1500 [00:17<02:34,  9.48it/s]\n",
      "3%|▎         | 39/1500 [00:17<02:41,  9.03it/s]\n",
      "3%|▎         | 40/1500 [00:18<02:47,  8.71it/s]\n",
      "3%|▎         | 41/1500 [00:18<02:48,  8.66it/s]\n",
      "3%|▎         | 42/1500 [00:18<02:45,  8.80it/s]\n",
      "3%|▎         | 43/1500 [00:18<02:42,  8.97it/s]\n",
      "3%|▎         | 44/1500 [00:18<02:42,  8.94it/s]\n",
      "3%|▎         | 45/1500 [00:18<02:38,  9.21it/s]\n",
      "3%|▎         | 46/1500 [00:18<02:35,  9.36it/s]\n",
      "3%|▎         | 47/1500 [00:18<02:36,  9.31it/s]\n",
      "3%|▎         | 49/1500 [00:18<02:33,  9.44it/s]\n",
      "3%|▎         | 50/1500 [00:19<02:31,  9.57it/s]\n",
      "3%|▎         | 51/1500 [00:19<02:31,  9.59it/s]\n",
      "4%|▎         | 53/1500 [00:19<02:33,  9.41it/s]\n",
      "4%|▎         | 55/1500 [00:19<02:26,  9.84it/s]\n",
      "4%|▎         | 56/1500 [00:19<02:27,  9.81it/s]\n",
      "4%|▍         | 58/1500 [00:19<02:26,  9.86it/s]\n",
      "4%|▍         | 59/1500 [00:20<02:27,  9.76it/s]\n",
      "4%|▍         | 60/1500 [00:20<02:32,  9.44it/s]\n",
      "4%|▍         | 61/1500 [00:20<02:32,  9.44it/s]\n",
      "4%|▍         | 62/1500 [00:20<02:32,  9.45it/s]\n",
      "4%|▍         | 64/1500 [00:20<02:29,  9.59it/s]\n",
      "4%|▍         | 65/1500 [00:20<02:30,  9.54it/s]\n",
      "4%|▍         | 67/1500 [00:20<02:29,  9.56it/s]\n",
      "5%|▍         | 68/1500 [00:20<02:30,  9.53it/s]\n",
      "5%|▍         | 69/1500 [00:21<02:29,  9.58it/s]\n",
      "5%|▍         | 70/1500 [00:21<02:28,  9.62it/s]\n",
      "5%|▍         | 71/1500 [00:21<02:29,  9.54it/s]\n",
      "5%|▍         | 72/1500 [00:21<02:31,  9.41it/s]\n",
      "5%|▍         | 73/1500 [00:21<02:31,  9.42it/s]\n",
      "5%|▌         | 75/1500 [00:21<02:24,  9.85it/s]\n",
      "5%|▌         | 76/1500 [00:21<02:30,  9.43it/s]\n",
      "5%|▌         | 78/1500 [00:21<02:27,  9.65it/s]\n",
      "5%|▌         | 79/1500 [00:22<02:29,  9.48it/s]\n",
      "5%|▌         | 80/1500 [00:22<02:39,  8.90it/s]\n",
      "5%|▌         | 82/1500 [00:22<02:29,  9.51it/s]\n",
      "6%|▌         | 83/1500 [00:22<02:28,  9.55it/s]\n",
      "6%|▌         | 84/1500 [00:22<02:32,  9.28it/s]\n",
      "6%|▌         | 85/1500 [00:22<02:32,  9.29it/s]\n",
      "6%|▌         | 86/1500 [00:22<02:31,  9.35it/s]\n",
      "6%|▌         | 87/1500 [00:22<02:29,  9.43it/s]\n",
      "6%|▌         | 88/1500 [00:23<02:31,  9.34it/s]\n",
      "6%|▌         | 90/1500 [00:23<02:24,  9.73it/s]\n",
      "6%|▌         | 92/1500 [00:23<02:25,  9.66it/s]\n",
      "6%|▌         | 93/1500 [00:23<02:25,  9.68it/s]\n",
      "6%|▋         | 94/1500 [00:23<02:24,  9.71it/s]\n",
      "6%|▋         | 95/1500 [00:23<02:25,  9.63it/s]\n",
      "6%|▋         | 96/1500 [00:23<02:29,  9.41it/s]\n",
      "6%|▋         | 97/1500 [00:24<02:28,  9.45it/s]\n",
      "7%|▋         | 98/1500 [00:24<02:27,  9.52it/s]\n",
      "7%|▋         | 99/1500 [00:24<02:28,  9.43it/s]\n",
      "7%|▋         | 100/1500 [00:24<02:36,  8.95it/s]\n",
      "7%|▋         | 102/1500 [00:24<02:26,  9.57it/s]\n",
      "7%|▋         | 103/1500 [00:24<02:31,  9.25it/s]\n",
      "7%|▋         | 104/1500 [00:24<02:28,  9.38it/s]\n",
      "7%|▋         | 105/1500 [00:24<02:30,  9.28it/s]\n",
      "7%|▋         | 106/1500 [00:24<02:35,  8.97it/s]\n",
      "7%|▋         | 108/1500 [00:25<02:34,  9.01it/s]\n",
      "7%|▋         | 109/1500 [00:25<02:32,  9.10it/s]\n",
      "7%|▋         | 110/1500 [00:25<02:30,  9.22it/s]\n",
      "7%|▋         | 111/1500 [00:25<02:30,  9.24it/s]\n",
      "7%|▋         | 112/1500 [00:25<02:32,  9.09it/s]\n",
      "8%|▊         | 113/1500 [00:25<02:31,  9.13it/s]\n",
      "8%|▊         | 115/1500 [00:25<02:24,  9.56it/s]\n",
      "8%|▊         | 116/1500 [00:26<02:26,  9.42it/s]\n",
      "8%|▊         | 118/1500 [00:26<02:22,  9.69it/s]\n",
      "8%|▊         | 119/1500 [00:26<02:24,  9.55it/s]\n",
      "8%|▊         | 121/1500 [00:26<02:32,  9.05it/s]\n",
      "8%|▊         | 122/1500 [00:26<02:37,  8.73it/s]\n",
      "8%|▊         | 123/1500 [00:26<02:38,  8.69it/s]\n",
      "8%|▊         | 124/1500 [00:26<02:33,  8.97it/s]\n",
      "8%|▊         | 126/1500 [00:27<02:25,  9.44it/s]\n",
      "8%|▊         | 127/1500 [00:27<02:24,  9.52it/s]\n",
      "9%|▊         | 128/1500 [00:27<02:25,  9.45it/s]\n",
      "9%|▊         | 129/1500 [00:27<02:25,  9.40it/s]\n",
      "9%|▊         | 131/1500 [00:27<02:18,  9.88it/s]\n",
      "9%|▉         | 132/1500 [00:27<02:25,  9.38it/s]\n",
      "9%|▉         | 134/1500 [00:27<02:21,  9.66it/s]\n",
      "9%|▉         | 135/1500 [00:28<02:26,  9.34it/s]\n",
      "9%|▉         | 136/1500 [00:28<02:25,  9.37it/s]\n",
      "9%|▉         | 138/1500 [00:28<02:22,  9.54it/s]\n",
      "9%|▉         | 139/1500 [00:28<02:24,  9.43it/s]\n",
      "9%|▉         | 140/1500 [00:28<02:26,  9.27it/s]\n",
      "9%|▉         | 141/1500 [00:28<02:24,  9.40it/s]\n",
      "9%|▉         | 142/1500 [00:28<02:26,  9.27it/s]\n",
      "10%|▉         | 143/1500 [00:28<02:26,  9.25it/s]\n",
      "10%|▉         | 145/1500 [00:29<02:20,  9.63it/s]\n",
      "10%|▉         | 146/1500 [00:29<02:22,  9.53it/s]\n",
      "10%|▉         | 148/1500 [00:29<02:21,  9.56it/s]\n",
      "10%|▉         | 149/1500 [00:29<02:26,  9.22it/s]\n",
      "10%|█         | 150/1500 [00:29<02:25,  9.29it/s]\n",
      "10%|█         | 151/1500 [00:29<02:22,  9.43it/s]\n",
      "10%|█         | 152/1500 [00:29<02:23,  9.38it/s]\n",
      "10%|█         | 153/1500 [00:30<02:23,  9.39it/s]\n",
      "10%|█         | 155/1500 [00:30<02:29,  9.00it/s]\n",
      "10%|█         | 156/1500 [00:30<02:37,  8.53it/s]\n",
      "11%|█         | 158/1500 [00:30<02:27,  9.10it/s]\n",
      "11%|█         | 159/1500 [00:30<02:28,  9.00it/s]\n",
      "11%|█         | 160/1500 [00:30<02:28,  9.05it/s]\n",
      "11%|█         | 161/1500 [00:30<02:26,  9.13it/s]\n",
      "11%|█         | 163/1500 [00:31<02:23,  9.29it/s]\n",
      "11%|█         | 164/1500 [00:31<02:23,  9.32it/s]\n",
      "11%|█         | 165/1500 [00:31<02:22,  9.40it/s]\n",
      "11%|█         | 166/1500 [00:31<02:22,  9.36it/s]\n",
      "11%|█         | 167/1500 [00:31<02:22,  9.37it/s]\n",
      "11%|█         | 168/1500 [00:31<02:23,  9.30it/s]\n",
      "11%|█▏        | 170/1500 [00:31<02:18,  9.60it/s]\n",
      "11%|█▏        | 171/1500 [00:31<02:20,  9.48it/s]\n",
      "11%|█▏        | 172/1500 [00:32<02:26,  9.04it/s]\n",
      "12%|█▏        | 174/1500 [00:32<02:17,  9.67it/s]\n",
      "12%|█▏        | 176/1500 [00:32<02:25,  9.08it/s]\n",
      "12%|█▏        | 178/1500 [00:32<02:22,  9.29it/s]\n",
      "12%|█▏        | 180/1500 [00:32<02:23,  9.18it/s]\n",
      "12%|█▏        | 181/1500 [00:33<02:22,  9.24it/s]\n",
      "12%|█▏        | 182/1500 [00:33<02:21,  9.32it/s]\n",
      "12%|█▏        | 183/1500 [00:33<02:22,  9.22it/s]\n",
      "12%|█▏        | 184/1500 [00:33<02:24,  9.08it/s]\n",
      "12%|█▏        | 185/1500 [00:33<02:32,  8.64it/s]\n",
      "12%|█▏        | 186/1500 [00:33<02:27,  8.94it/s]\n",
      "12%|█▏        | 187/1500 [00:33<02:36,  8.38it/s]\n",
      "13%|█▎        | 188/1500 [00:33<02:30,  8.69it/s]\n",
      "13%|█▎        | 189/1500 [00:34<02:38,  8.25it/s]\n",
      "13%|█▎        | 191/1500 [00:34<02:26,  8.96it/s]\n",
      "13%|█▎        | 192/1500 [00:34<02:28,  8.83it/s]\n",
      "13%|█▎        | 193/1500 [00:34<02:24,  9.06it/s]\n",
      "13%|█▎        | 195/1500 [00:34<02:26,  8.91it/s]\n",
      "13%|█▎        | 197/1500 [00:34<02:22,  9.12it/s]\n",
      "13%|█▎        | 199/1500 [00:35<02:18,  9.42it/s]\n",
      "13%|█▎        | 200/1500 [00:35<02:20,  9.22it/s]\n",
      "13%|█▎        | 202/1500 [00:35<02:19,  9.32it/s]\n",
      "14%|█▎        | 203/1500 [00:35<02:26,  8.86it/s]\n",
      "14%|█▎        | 204/1500 [00:35<02:26,  8.86it/s]\n",
      "14%|█▎        | 205/1500 [00:35<02:28,  8.75it/s]\n",
      "14%|█▍        | 207/1500 [00:35<02:19,  9.27it/s]\n",
      "14%|█▍        | 208/1500 [00:36<02:20,  9.18it/s]\n",
      "14%|█▍        | 210/1500 [00:36<02:15,  9.55it/s]\n",
      "14%|█▍        | 211/1500 [00:36<02:13,  9.62it/s]\n",
      "14%|█▍        | 212/1500 [00:36<02:25,  8.85it/s]\n",
      "14%|█▍        | 214/1500 [00:36<02:18,  9.31it/s]\n",
      "14%|█▍        | 215/1500 [00:36<02:16,  9.43it/s]\n",
      "14%|█▍        | 216/1500 [00:36<02:16,  9.40it/s]\n",
      "15%|█▍        | 218/1500 [00:37<02:19,  9.16it/s]\n",
      "15%|█▍        | 219/1500 [00:37<02:18,  9.26it/s]\n",
      "15%|█▍        | 220/1500 [00:37<02:18,  9.26it/s]\n",
      "15%|█▍        | 221/1500 [00:37<02:19,  9.14it/s]\n",
      "15%|█▍        | 223/1500 [00:37<02:13,  9.53it/s]\n",
      "15%|█▍        | 224/1500 [00:37<02:19,  9.15it/s]\n",
      "15%|█▌        | 225/1500 [00:37<02:18,  9.23it/s]\n",
      "15%|█▌        | 227/1500 [00:38<02:13,  9.57it/s]\n",
      "15%|█▌        | 228/1500 [00:38<02:19,  9.15it/s]\n",
      "15%|█▌        | 229/1500 [00:38<02:18,  9.17it/s]\n",
      "15%|█▌        | 230/1500 [00:38<02:16,  9.27it/s]\n",
      "15%|█▌        | 231/1500 [00:38<02:22,  8.90it/s]\n",
      "15%|█▌        | 232/1500 [00:38<02:19,  9.12it/s]\n",
      "16%|█▌        | 234/1500 [00:38<02:12,  9.58it/s]\n",
      "16%|█▌        | 235/1500 [00:38<02:13,  9.51it/s]\n",
      "16%|█▌        | 236/1500 [00:39<02:17,  9.21it/s]\n",
      "16%|█▌        | 238/1500 [00:39<02:14,  9.37it/s]\n",
      "16%|█▌        | 239/1500 [00:39<02:13,  9.44it/s]\n",
      "16%|█▌        | 240/1500 [00:39<02:14,  9.33it/s]\n",
      "16%|█▌        | 242/1500 [00:39<02:11,  9.60it/s]\n",
      "16%|█▌        | 243/1500 [00:39<02:13,  9.44it/s]\n",
      "16%|█▋        | 244/1500 [00:39<02:16,  9.17it/s]\n",
      "16%|█▋        | 245/1500 [00:40<02:15,  9.24it/s]\n",
      "16%|█▋        | 246/1500 [00:40<02:14,  9.32it/s]\n",
      "17%|█▋        | 248/1500 [00:40<02:09,  9.64it/s]\n",
      "17%|█▋        | 249/1500 [00:40<02:08,  9.70it/s]\n",
      "17%|█▋        | 250/1500 [00:40<02:07,  9.77it/s]\n",
      "17%|█▋        | 251/1500 [00:40<02:19,  8.93it/s]\n",
      "17%|█▋        | 253/1500 [00:40<02:09,  9.60it/s]\n",
      "17%|█▋        | 255/1500 [00:41<02:09,  9.61it/s]\n",
      "17%|█▋        | 256/1500 [00:41<02:10,  9.52it/s]\n",
      "17%|█▋        | 257/1500 [00:41<02:11,  9.47it/s]\n",
      "17%|█▋        | 258/1500 [00:41<02:12,  9.40it/s]\n",
      "17%|█▋        | 259/1500 [00:41<02:18,  8.96it/s]\n",
      "17%|█▋        | 261/1500 [00:41<02:11,  9.39it/s]\n",
      "18%|█▊        | 263/1500 [00:41<02:09,  9.57it/s]\n",
      "18%|█▊        | 264/1500 [00:42<02:14,  9.19it/s]\n",
      "18%|█▊        | 266/1500 [00:42<02:10,  9.43it/s]\n",
      "18%|█▊        | 267/1500 [00:42<02:16,  9.06it/s]\n",
      "18%|█▊        | 269/1500 [00:42<02:12,  9.31it/s]\n",
      "18%|█▊        | 270/1500 [00:42<02:11,  9.33it/s]\n",
      "18%|█▊        | 271/1500 [00:42<02:13,  9.21it/s]\n",
      "18%|█▊        | 273/1500 [00:43<02:07,  9.62it/s]\n",
      "18%|█▊        | 274/1500 [00:43<02:06,  9.67it/s]\n",
      "18%|█▊        | 275/1500 [00:43<02:10,  9.35it/s]\n",
      "18%|█▊        | 276/1500 [00:43<02:12,  9.23it/s]\n",
      "19%|█▊        | 278/1500 [00:43<02:08,  9.52it/s]\n",
      "19%|█▊        | 279/1500 [00:43<02:08,  9.52it/s]\n",
      "19%|█▊        | 280/1500 [00:43<02:09,  9.39it/s]\n",
      "19%|█▉        | 282/1500 [00:43<02:05,  9.70it/s]\n",
      "19%|█▉        | 283/1500 [00:44<02:04,  9.74it/s]\n",
      "19%|█▉        | 284/1500 [00:44<02:06,  9.58it/s]\n",
      "19%|█▉        | 286/1500 [00:44<02:03,  9.83it/s]\n",
      "19%|█▉        | 287/1500 [00:44<02:04,  9.76it/s]\n",
      "19%|█▉        | 288/1500 [00:44<02:05,  9.62it/s]\n",
      "19%|█▉        | 290/1500 [00:44<02:05,  9.66it/s]\n",
      "19%|█▉        | 291/1500 [00:44<02:06,  9.59it/s]\n",
      "20%|█▉        | 293/1500 [00:45<02:04,  9.70it/s]\n",
      "20%|█▉        | 294/1500 [00:45<02:04,  9.69it/s]\n",
      "20%|█▉        | 295/1500 [00:45<02:05,  9.57it/s]\n",
      "20%|█▉        | 296/1500 [00:45<02:07,  9.45it/s]\n",
      "20%|█▉        | 298/1500 [00:45<02:04,  9.66it/s]\n",
      "20%|█▉        | 299/1500 [00:45<02:04,  9.62it/s]\n",
      "20%|██        | 300/1500 [00:45<02:08,  9.37it/s]\n",
      "20%|██        | 301/1500 [00:45<02:09,  9.27it/s]\n",
      "20%|██        | 303/1500 [00:46<02:02,  9.76it/s]\n",
      "20%|██        | 304/1500 [00:46<02:12,  9.00it/s]\n",
      "20%|██        | 306/1500 [00:46<02:06,  9.46it/s]\n",
      "20%|██        | 307/1500 [00:46<02:05,  9.49it/s]\n",
      "21%|██        | 308/1500 [00:46<02:05,  9.52it/s]\n",
      "21%|██        | 309/1500 [00:46<02:03,  9.62it/s]\n",
      "21%|██        | 310/1500 [00:46<02:06,  9.44it/s]\n",
      "21%|██        | 311/1500 [00:46<02:06,  9.36it/s]\n",
      "21%|██        | 312/1500 [00:47<02:13,  8.92it/s]\n",
      "21%|██        | 313/1500 [00:47<02:12,  8.97it/s]\n",
      "21%|██        | 314/1500 [00:47<02:18,  8.56it/s]\n",
      "21%|██        | 316/1500 [00:47<02:09,  9.13it/s]\n",
      "21%|██        | 318/1500 [00:47<02:04,  9.52it/s]\n",
      "21%|██▏       | 319/1500 [00:47<02:04,  9.45it/s]\n",
      "21%|██▏       | 320/1500 [00:47<02:07,  9.28it/s]\n",
      "21%|██▏       | 322/1500 [00:48<02:03,  9.58it/s]\n",
      "22%|██▏       | 324/1500 [00:48<02:02,  9.64it/s]\n",
      "22%|██▏       | 325/1500 [00:48<02:08,  9.16it/s]\n",
      "22%|██▏       | 327/1500 [00:48<02:09,  9.07it/s]\n",
      "22%|██▏       | 328/1500 [00:48<02:07,  9.17it/s]\n",
      "22%|██▏       | 330/1500 [00:49<02:03,  9.48it/s]\n",
      "22%|██▏       | 331/1500 [00:49<02:10,  8.95it/s]\n",
      "22%|██▏       | 332/1500 [00:49<02:08,  9.09it/s]\n",
      "22%|██▏       | 334/1500 [00:49<02:03,  9.44it/s]\n",
      "22%|██▏       | 335/1500 [00:49<02:03,  9.41it/s]\n",
      "22%|██▏       | 336/1500 [00:49<02:04,  9.37it/s]\n",
      "23%|██▎       | 338/1500 [00:49<02:01,  9.57it/s]\n",
      "23%|██▎       | 339/1500 [00:49<02:01,  9.58it/s]\n",
      "23%|██▎       | 340/1500 [00:50<02:03,  9.40it/s]\n",
      "23%|██▎       | 341/1500 [00:50<02:07,  9.11it/s]\n",
      "23%|██▎       | 343/1500 [00:50<02:00,  9.61it/s]\n",
      "23%|██▎       | 344/1500 [00:50<02:01,  9.55it/s]\n",
      "23%|██▎       | 346/1500 [00:50<02:00,  9.57it/s]\n",
      "23%|██▎       | 347/1500 [00:50<02:00,  9.58it/s]\n",
      "23%|██▎       | 348/1500 [00:50<02:02,  9.39it/s]\n",
      "23%|██▎       | 349/1500 [00:51<02:01,  9.44it/s]\n",
      "23%|██▎       | 350/1500 [00:51<02:02,  9.38it/s]\n",
      "23%|██▎       | 352/1500 [00:51<01:59,  9.64it/s]\n",
      "24%|██▎       | 354/1500 [00:51<01:59,  9.59it/s]\n",
      "24%|██▎       | 355/1500 [00:51<01:58,  9.63it/s]\n",
      "24%|██▎       | 356/1500 [00:51<01:58,  9.65it/s]\n",
      "24%|██▍       | 357/1500 [00:51<01:59,  9.55it/s]\n",
      "24%|██▍       | 359/1500 [00:52<01:59,  9.58it/s]\n",
      "24%|██▍       | 360/1500 [00:52<02:04,  9.16it/s]\n",
      "24%|██▍       | 361/1500 [00:52<02:07,  8.94it/s]\n",
      "24%|██▍       | 362/1500 [00:52<02:06,  8.96it/s]\n",
      "24%|██▍       | 364/1500 [00:52<02:03,  9.17it/s]\n",
      "24%|██▍       | 366/1500 [00:52<01:57,  9.63it/s]\n",
      "24%|██▍       | 367/1500 [00:52<01:58,  9.59it/s]\n",
      "25%|██▍       | 368/1500 [00:53<01:59,  9.44it/s]\n",
      "25%|██▍       | 370/1500 [00:53<02:01,  9.29it/s]\n",
      "25%|██▍       | 372/1500 [00:53<01:59,  9.46it/s]\n",
      "25%|██▍       | 374/1500 [00:53<01:57,  9.62it/s]\n",
      "25%|██▌       | 375/1500 [00:53<02:02,  9.21it/s]\n",
      "25%|██▌       | 377/1500 [00:54<01:56,  9.64it/s]\n",
      "25%|██▌       | 378/1500 [00:54<01:57,  9.56it/s]\n",
      "25%|██▌       | 379/1500 [00:54<01:57,  9.57it/s]\n",
      "25%|██▌       | 380/1500 [00:54<01:59,  9.41it/s]\n",
      "25%|██▌       | 382/1500 [00:54<01:55,  9.64it/s]\n",
      "26%|██▌       | 383/1500 [00:54<01:55,  9.63it/s]\n",
      "26%|██▌       | 384/1500 [00:54<01:56,  9.59it/s]\n",
      "26%|██▌       | 385/1500 [00:54<01:55,  9.62it/s]\n",
      "26%|██▌       | 386/1500 [00:54<02:02,  9.13it/s]\n",
      "26%|██▌       | 387/1500 [00:55<01:59,  9.35it/s]\n",
      "26%|██▌       | 388/1500 [00:55<02:05,  8.88it/s]\n",
      "26%|██▌       | 390/1500 [00:55<01:56,  9.57it/s]\n",
      "26%|██▌       | 391/1500 [00:55<01:56,  9.50it/s]\n",
      "26%|██▌       | 392/1500 [00:55<01:58,  9.35it/s]\n",
      "26%|██▋       | 394/1500 [00:55<01:56,  9.50it/s]\n",
      "26%|██▋       | 395/1500 [00:55<01:56,  9.45it/s]\n",
      "26%|██▋       | 396/1500 [00:56<01:57,  9.43it/s]\n",
      "26%|██▋       | 397/1500 [00:56<01:55,  9.51it/s]\n",
      "27%|██▋       | 399/1500 [00:56<01:54,  9.62it/s]\n",
      "27%|██▋       | 400/1500 [00:56<01:54,  9.59it/s]\n",
      "27%|██▋       | 401/1500 [00:56<02:02,  8.94it/s]\n",
      "27%|██▋       | 403/1500 [00:56<01:55,  9.52it/s]\n",
      "27%|██▋       | 404/1500 [00:56<01:56,  9.42it/s]\n",
      "27%|██▋       | 405/1500 [00:56<01:56,  9.43it/s]\n",
      "27%|██▋       | 406/1500 [00:57<01:54,  9.53it/s]\n",
      "27%|██▋       | 408/1500 [00:57<01:57,  9.30it/s]\n",
      "27%|██▋       | 410/1500 [00:57<01:52,  9.68it/s]\n",
      "27%|██▋       | 411/1500 [00:57<01:55,  9.46it/s]\n",
      "28%|██▊       | 413/1500 [00:57<01:55,  9.44it/s]\n",
      "28%|██▊       | 414/1500 [00:57<01:55,  9.44it/s]\n",
      "28%|██▊       | 415/1500 [00:58<01:54,  9.51it/s]\n",
      "28%|██▊       | 416/1500 [00:58<01:52,  9.61it/s]\n",
      "28%|██▊       | 418/1500 [00:58<01:55,  9.36it/s]\n",
      "28%|██▊       | 420/1500 [00:58<01:51,  9.66it/s]\n",
      "28%|██▊       | 422/1500 [00:58<01:50,  9.74it/s]\n",
      "28%|██▊       | 423/1500 [00:58<01:50,  9.76it/s]\n",
      "28%|██▊       | 424/1500 [00:58<01:51,  9.66it/s]\n",
      "28%|██▊       | 425/1500 [00:59<01:51,  9.67it/s]\n",
      "28%|██▊       | 426/1500 [00:59<01:50,  9.70it/s]\n",
      "28%|██▊       | 427/1500 [00:59<01:57,  9.13it/s]\n",
      "29%|██▊       | 428/1500 [00:59<01:56,  9.17it/s]\n",
      "29%|██▊       | 430/1500 [00:59<01:51,  9.62it/s]\n",
      "29%|██▊       | 431/1500 [00:59<01:51,  9.62it/s]\n",
      "29%|██▉       | 432/1500 [00:59<01:50,  9.63it/s]\n",
      "29%|██▉       | 434/1500 [01:00<01:47,  9.87it/s]\n",
      "29%|██▉       | 435/1500 [01:00<01:50,  9.67it/s]\n",
      "29%|██▉       | 436/1500 [01:00<01:52,  9.48it/s]\n",
      "29%|██▉       | 438/1500 [01:00<01:49,  9.66it/s]\n",
      "29%|██▉       | 439/1500 [01:00<01:49,  9.68it/s]\n",
      "29%|██▉       | 440/1500 [01:00<01:51,  9.50it/s]\n",
      "29%|██▉       | 442/1500 [01:00<01:49,  9.65it/s]\n",
      "30%|██▉       | 444/1500 [01:01<01:50,  9.52it/s]\n",
      "30%|██▉       | 445/1500 [01:01<01:50,  9.54it/s]\n",
      "30%|██▉       | 446/1500 [01:01<01:54,  9.20it/s]\n",
      "30%|██▉       | 448/1500 [01:01<01:49,  9.62it/s]\n",
      "30%|███       | 450/1500 [01:01<01:47,  9.77it/s]\n",
      "30%|███       | 452/1500 [01:01<01:49,  9.53it/s]\n",
      "30%|███       | 454/1500 [01:02<01:48,  9.68it/s]\n",
      "30%|███       | 455/1500 [01:02<01:47,  9.74it/s]\n",
      "30%|███       | 456/1500 [01:02<01:48,  9.60it/s]\n",
      "30%|███       | 457/1500 [01:02<01:48,  9.63it/s]\n",
      "31%|███       | 458/1500 [01:02<01:51,  9.30it/s]\n",
      "31%|███       | 459/1500 [01:02<01:52,  9.29it/s]\n",
      "31%|███       | 460/1500 [01:02<01:51,  9.36it/s]\n",
      "31%|███       | 461/1500 [01:02<01:50,  9.39it/s]\n",
      "31%|███       | 462/1500 [01:02<01:49,  9.46it/s]\n",
      "31%|███       | 463/1500 [01:03<01:53,  9.17it/s]\n",
      "31%|███       | 464/1500 [01:03<01:54,  9.04it/s]\n",
      "31%|███       | 465/1500 [01:03<01:51,  9.28it/s]\n",
      "31%|███       | 467/1500 [01:03<01:45,  9.80it/s]\n",
      "31%|███       | 468/1500 [01:03<01:49,  9.39it/s]\n",
      "31%|███▏      | 469/1500 [01:03<01:49,  9.46it/s]\n",
      "31%|███▏      | 470/1500 [01:03<01:55,  8.93it/s]\n",
      "31%|███▏      | 471/1500 [01:03<01:59,  8.63it/s]\n",
      "32%|███▏      | 473/1500 [01:04<01:49,  9.38it/s]\n",
      "32%|███▏      | 474/1500 [01:04<01:48,  9.47it/s]\n",
      "32%|███▏      | 475/1500 [01:04<01:48,  9.45it/s]\n",
      "32%|███▏      | 476/1500 [01:04<01:50,  9.24it/s]\n",
      "32%|███▏      | 478/1500 [01:04<01:46,  9.56it/s]\n",
      "32%|███▏      | 479/1500 [01:04<01:47,  9.48it/s]\n",
      "32%|███▏      | 480/1500 [01:04<01:49,  9.29it/s]\n",
      "32%|███▏      | 482/1500 [01:05<01:45,  9.69it/s]\n",
      "32%|███▏      | 483/1500 [01:05<01:47,  9.50it/s]\n",
      "32%|███▏      | 484/1500 [01:05<02:08,  7.90it/s]\n",
      "32%|███▏      | 486/1500 [01:05<01:55,  8.75it/s]\n",
      "32%|███▏      | 487/1500 [01:05<01:52,  9.01it/s]\n",
      "33%|███▎      | 488/1500 [01:05<01:50,  9.16it/s]\n",
      "33%|███▎      | 489/1500 [01:05<01:53,  8.88it/s]\n",
      "33%|███▎      | 491/1500 [01:06<01:47,  9.39it/s]\n",
      "33%|███▎      | 493/1500 [01:06<01:43,  9.71it/s]\n",
      "33%|███▎      | 494/1500 [01:06<01:45,  9.57it/s]\n",
      "33%|███▎      | 495/1500 [01:06<01:49,  9.17it/s]\n",
      "33%|███▎      | 497/1500 [01:06<01:44,  9.62it/s]\n",
      "33%|███▎      | 498/1500 [01:06<01:44,  9.55it/s]\n",
      "33%|███▎      | 499/1500 [01:06<01:43,  9.63it/s]\n",
      "2023-06-02 07:50:00.000037: INFO ||NCC_WRAPPER||: No candidate found under /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_569571592325889951.\n",
      "2023-06-02 07:50:00.000038: INFO ||NCC_WRAPPER||: Cache dir for the neff: /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_569571592325889951/MODULE_3_SyncTensorsGraph.4_569571592325889951_algo-1-7a0df2db-110-5fd20ccc7ddd3/7dc06601-560f-43ac-b992-22d21a128aa7\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-06-02 07:50:01.000297: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "2023-06-02 07:50:01.000380: INFO ||NCC_WRAPPER||: No candidate found under /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_13329092655006576560.\n",
      "2023-06-02 07:50:01.000380: INFO ||NCC_WRAPPER||: Cache dir for the neff: /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_13329092655006576560/MODULE_4_SyncTensorsGraph.20_13329092655006576560_algo-1-35f1f784-110-5fd20ccdc5e37/52655a23-74b2-4ff1-a6a1-61efe80f966c\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-06-02 07:50:02.000130: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "{'loss': 0.518, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "33%|███▎      | 500/1500 [01:09<01:43,  9.63it/s]\n",
      "2023-06-02 07:50:02.000234: INFO ||NCC_WRAPPER||: No candidate found under /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_931450859643720011.\n",
      "2023-06-02 07:50:02.000234: INFO ||NCC_WRAPPER||: Cache dir for the neff: /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_931450859643720011/MODULE_5_SyncTensorsGraph.6_931450859643720011_algo-1-2a0363d7-110-5fd20cce96271/76a32a77-8459-4579-897f-db8ee5b38783\n",
      ".\n",
      "Compiler status PASS\n",
      "2023-06-02 07:50:02.000946: INFO ||NCC_WRAPPER||: Exiting with a successfully compiled graph\n",
      "Disabling DDP because it is currently not playing well with multiple workers training, for more information please refer to https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/finetune_hftrainer.html#multi-worker-training\n",
      "Disabling DDP because it is currently not playing well with multiple workers training, for more information please refer to https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/finetune_hftrainer.html#multi-worker-training\n",
      "***** Running Evaluation *****\n",
      "***** Running Evaluation *****\n",
      "Num examples = 2000\n",
      "  Batch size = 8\n",
      "Num examples = 2000\n",
      "  Batch size = 8\n",
      "Fetching 27 files:   0%|          | 0/27 [00:00<?, ?it/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4eac7a3c73.neff:   0%|          | 0.00/9.01k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9-5fb4eac7a3c73.neff: 100%|██████████| 9.01k/9.01k [00:00<00:00, 9.31MB/s]\n",
      "Downloading (…)f9/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)f9/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)2-5fb56d04eaea0.neff:   0%|          | 0.00/9.03k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)2-5fb56d04eaea0.neff: 100%|██████████| 9.03k/9.03k [00:00<00:00, 2.43MB/s]\n",
      "Downloading (…)5fb4eac7a3c73.hlo.pb:   0%|          | 0.00/940 [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4eac7a3c73.hlo.pb: 100%|██████████| 940/940 [00:00<00:00, 920kB/s]\n",
      "Downloading (…)9-5fb4eaebe572e.neff:   0%|          | 0.00/8.26k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)9-5fb4eaebe572e.neff: 100%|██████████| 8.26k/8.26k [00:00<00:00, 2.11MB/s]\n",
      "Downloading (…)28/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)28/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4eaeca8a63.neff:   0%|          | 0.00/7.78k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)9-5fb4eaeca8a63.neff: 100%|██████████| 7.78k/7.78k [00:00<00:00, 7.73MB/s]\n",
      "Downloading (…)65/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)65/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)23/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)23/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Fetching 27 files:  19%|█▊        | 5/27 [00:00<00:00, 38.91it/s]\n",
      "#033[A\n",
      "Downloading (…)5fb4eaebe572e.hlo.pb:   0%|          | 0.00/330 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eaebe572e.hlo.pb: 100%|██████████| 330/330 [00:00<00:00, 89.2kB/s]\n",
      "Downloading (…)2-5fb56d042a00e.neff:   0%|          | 0.00/7.86k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)2-5fb56d042a00e.neff: 100%|██████████| 7.86k/7.86k [00:00<00:00, 7.82MB/s]\n",
      "Downloading (…)5fb56d04eaea0.hlo.pb:   0%|          | 0.00/940 [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb56d04eaea0.hlo.pb: 100%|██████████| 940/940 [00:00<00:00, 688kB/s]\n",
      "Downloading (…)5fb4eacb9c544.hlo.pb:   0%|          | 0.00/208k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)21/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A#033[A\n",
      "Downloading (…)21/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4eacb9c544.hlo.pb: 100%|██████████| 208k/208k [00:00<00:00, 17.7MB/s]\n",
      "Downloading (…)9-5fb4eac6e2d6e.neff:   0%|          | 0.00/7.78k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9-5fb4eac6e2d6e.neff: 100%|██████████| 7.78k/7.78k [00:00<00:00, 9.53MB/s]\n",
      "Downloading (…)5fb4eaeca8a63.hlo.pb:   0%|          | 0.00/365 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eaeca8a63.hlo.pb: 100%|██████████| 365/365 [00:00<00:00, 359kB/s]\n",
      "Fetching 27 files:   0%|          | 0/27 [00:00<?, ?it/s]\n",
      "Downloading (…)f4/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)f4/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4eac86df30.neff:   0%|          | 0.00/8.19k [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)9-5fb4eac86df30.neff: 100%|██████████| 8.19k/8.19k [00:00<00:00, 8.41MB/s]\n",
      "Downloading (…)2-5fb56d04eaea0.neff:   0%|          | 0.00/9.03k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4eacb9c544.neff:   0%|          | 0.00/1.18M [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)2-5fb56d04eaea0.neff: 100%|██████████| 9.03k/9.03k [00:00<00:00, 5.34MB/s]\n",
      "Downloading (…)9-5fb4eac7a3c73.neff:   0%|          | 0.00/9.01k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4eac7a3c73.neff: 100%|██████████| 9.01k/9.01k [00:00<00:00, 8.24MB/s]\n",
      "Downloading (…)23/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)23/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb56d042a00e.hlo.pb:   0%|          | 0.00/326 [00:00<?, ?B/s]#033[A#033[A#033[A\n",
      "Downloading (…)5fb56d042a00e.hlo.pb: 100%|██████████| 326/326 [00:00<00:00, 323kB/s]\n",
      "Downloading (…)9-5fb4eacb9c544.neff: 100%|██████████| 1.18M/1.18M [00:00<00:00, 91.3MB/s]\n",
      "Fetching 27 files:  52%|█████▏    | 14/27 [00:00<00:00, 60.11it/s]#033[A\n",
      "Downloading (…)5fb56d04eaea0.hlo.pb:   0%|          | 0.00/940 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb56d04eaea0.hlo.pb: 100%|██████████| 940/940 [00:00<00:00, 924kB/s]\n",
      "Downloading (…)2-5fb56d05b5bb7.neff:   0%|          | 0.00/8.17k [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)2-5fb56d05b5bb7.neff: 100%|██████████| 8.17k/8.17k [00:00<00:00, 8.86MB/s]\n",
      "Downloading (…)5fb4eaebe572e.hlo.pb:   0%|          | 0.00/330 [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)96/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eaebe572e.hlo.pb: 100%|██████████| 330/330 [00:00<00:00, 152kB/s]\n",
      "Downloading (…)9-5fb4eaebe572e.neff:   0%|          | 0.00/8.26k [00:00<?, ?B/s]#015Downloading (…)96/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4eaebe572e.neff: 100%|██████████| 8.26k/8.26k [00:00<00:00, 2.17MB/s]\n",
      "Downloading (…)9-5fb4eaeca8a63.neff:   0%|          | 0.00/7.78k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)9-5fb4eaeca8a63.neff: 100%|██████████| 7.78k/7.78k [00:00<00:00, 7.57MB/s]\n",
      "Downloading (…)21/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)21/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4eac6e2d6e.hlo.pb:   0%|          | 0.00/326 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eac7a3c73.hlo.pb:   0%|          | 0.00/940 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb4eac6e2d6e.hlo.pb: 100%|██████████| 326/326 [00:00<00:00, 333kB/s]\n",
      "Downloading (…)5fb4eac7a3c73.hlo.pb: 100%|██████████| 940/940 [00:00<00:00, 254kB/s]\n",
      "Downloading (…)5fb4eacb9c544.hlo.pb:   0%|          | 0.00/208k [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (…)65/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "#033[A#033[A\n",
      "Fetching 27 files:   4%|▎         | 1/27 [00:00<00:03,  7.78it/s]\n",
      "Downloading (…)65/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4eaeca8a63.hlo.pb:   0%|          | 0.00/365 [00:00<?, ?B/s]\n",
      "#033[A#033[A\n",
      "Downloading (…)5fb4eaeca8a63.hlo.pb: 100%|██████████| 365/365 [00:00<00:00, 370kB/s]\n",
      "Downloading (…)5fb4eacb9c544.hlo.pb: 100%|██████████| 208k/208k [00:00<00:00, 18.2MB/s]\n",
      "Downloading (…)2-5fb56d042a00e.neff:   0%|          | 0.00/7.86k [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)2-5fb56d042a00e.neff: 100%|██████████| 7.86k/7.86k [00:00<00:00, 10.6MB/s]\n",
      "Downloading (…)9-5fb4eacb9c544.neff:   0%|          | 0.00/1.18M [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb4eac86df30.hlo.pb:   0%|          | 0.00/399 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb4eac86df30.hlo.pb: 100%|██████████| 399/399 [00:00<00:00, 366kB/s]\n",
      "Downloading (…)9-5fb4eacb9c544.neff: 100%|██████████| 1.18M/1.18M [00:00<00:00, 96.6MB/s]\n",
      "Downloading (…)b7/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)b7/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)9-5fb4eac6e2d6e.neff:   0%|          | 0.00/7.78k [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb56d042a00e.hlo.pb:   0%|          | 0.00/326 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)9-5fb4eac6e2d6e.neff: 100%|██████████| 7.78k/7.78k [00:00<00:00, 3.03MB/s]\n",
      "Downloading (…)5fb56d042a00e.hlo.pb: 100%|██████████| 326/326 [00:00<00:00, 285kB/s]\n",
      "Downloading (…)9-5fb4eac86df30.neff:   0%|          | 0.00/8.19k [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)9-5fb4eac86df30.neff: 100%|██████████| 8.19k/8.19k [00:00<00:00, 8.49MB/s]\n",
      "Downloading (…)d6/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)d6/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb56d05b5bb7.hlo.pb:   0%|          | 0.00/399 [00:00<?, ?B/s]#033[A#033[A\n",
      "Downloading (…)5fb56d05b5bb7.hlo.pb: 100%|██████████| 399/399 [00:00<00:00, 394kB/s]\n",
      "Downloading (…)f4/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)f4/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)2-5fb56d05b5bb7.neff:   0%|          | 0.00/8.17k [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)2-5fb56d05b5bb7.neff: 100%|██████████| 8.17k/8.17k [00:00<00:00, 14.6MB/s]\n",
      "Downloading (…)5fb4eac6e2d6e.hlo.pb:   0%|          | 0.00/326 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb4eac6e2d6e.hlo.pb: 100%|██████████| 326/326 [00:00<00:00, 318kB/s]\n",
      "Downloading (…)b7/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)b7/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)96/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)96/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb4eac86df30.hlo.pb:   0%|          | 0.00/399 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb4eac86df30.hlo.pb: 100%|██████████| 399/399 [00:00<00:00, 413kB/s]\n",
      "Downloading (…)28/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)28/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Downloading (…)5fb56d05b5bb7.hlo.pb:   0%|          | 0.00/399 [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)5fb56d05b5bb7.hlo.pb: 100%|██████████| 399/399 [00:00<00:00, 451kB/s]\n",
      "Downloading (…)d6/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A#033[A\n",
      "Downloading (…)d6/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Fetching 27 files:  85%|████████▌ | 23/27 [00:00<00:00, 37.35it/s]#033[A\n",
      "Fetching 27 files: 100%|██████████| 27/27 [00:00<00:00, 46.85it/s]\n",
      "Downloading (…)f9/compile_flags.txt: 0.00B [00:00, ?B/s]#033[A\n",
      "Downloading (…)f9/compile_flags.txt: 0.00B [00:00, ?B/s]\n",
      "Fetching 27 files:  11%|█         | 3/27 [00:00<00:03,  7.05it/s]\n",
      "Fetching 27 files: 100%|██████████| 27/27 [00:00<00:00, 63.99it/s]\n",
      "2023-06-02 07:50:06.000228: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_18424450281362268497/MODULE_6_SyncTensorsGraph.4570_18424450281362268497_algo-1-4371afeb-109-5fb4eacb9c544/a2531b71-c94f-4c31-bce1-9949712c8521/MODULE_6_SyncTensorsGraph.4570_18424450281362268497_algo-1-4371afeb-109-5fb4eacb9c544.neff. Exiting with a successfully compiled graph\n",
      "2023-06-02 07:50:06.000545: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_13837218930383410181/MODULE_7_SyncTensorsGraph.4_13837218930383410181_algo-1-4d9b6bec-109-5fb4eaebe572e/c8519aa1-d930-4e3d-8da5-e7997e157528/MODULE_7_SyncTensorsGraph.4_13837218930383410181_algo-1-4d9b6bec-109-5fb4eaebe572e.neff. Exiting with a successfully compiled graph\n",
      "2023-06-02 07:50:06.000626: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_14019065114978382472/MODULE_8_SyncTensorsGraph.4_14019065114978382472_algo-1-24a9d5a2-109-5fb4eaeca8a63/13042a90-c11f-4386-82a7-7df1f7e45965/MODULE_8_SyncTensorsGraph.4_14019065114978382472_algo-1-24a9d5a2-109-5fb4eaeca8a63.neff. Exiting with a successfully compiled graph\n",
      "0%|          | 0/125 [00:00<?, ?it/s]#033[A\n",
      "2%|▏         | 2/125 [00:01<01:05,  1.87it/s]#033[A\n",
      "4%|▍         | 5/125 [00:01<00:23,  5.01it/s]#033[A\n",
      "6%|▋         | 8/125 [00:01<00:14,  8.09it/s]#033[A\n",
      "9%|▉         | 11/125 [00:01<00:10, 11.07it/s]#033[A\n",
      "11%|█         | 14/125 [00:01<00:07, 14.00it/s]#033[A\n",
      "14%|█▎        | 17/125 [00:01<00:06, 16.52it/s]#033[A\n",
      "16%|█▌        | 20/125 [00:01<00:05, 18.17it/s]#033[A\n",
      "18%|█▊        | 23/125 [00:01<00:05, 20.12it/s]#033[A\n",
      "21%|██        | 26/125 [00:02<00:04, 20.57it/s]#033[A\n",
      "23%|██▎       | 29/125 [00:02<00:04, 21.03it/s]#033[A\n",
      "26%|██▌       | 32/125 [00:02<00:04, 22.16it/s]#033[A\n",
      "28%|██▊       | 35/125 [00:02<00:03, 23.10it/s]#033[A\n",
      "30%|███       | 38/125 [00:02<00:03, 23.79it/s]#033[A\n",
      "33%|███▎      | 41/125 [00:02<00:03, 24.21it/s]#033[A\n",
      "35%|███▌      | 44/125 [00:02<00:03, 23.95it/s]#033[A\n",
      "38%|███▊      | 47/125 [00:02<00:03, 24.51it/s]#033[A\n",
      "40%|████      | 50/125 [00:03<00:03, 24.60it/s]#033[A\n",
      "42%|████▏     | 53/125 [00:03<00:03, 23.50it/s]#033[A\n",
      "45%|████▍     | 56/125 [00:03<00:02, 23.95it/s]#033[A\n",
      "47%|████▋     | 59/125 [00:03<00:02, 24.51it/s]#033[A\n",
      "50%|████▉     | 62/125 [00:03<00:02, 24.67it/s]#033[A\n",
      "52%|█████▏    | 65/125 [00:03<00:02, 24.80it/s]#033[A\n",
      "54%|█████▍    | 68/125 [00:03<00:02, 24.84it/s]#033[A\n",
      "57%|█████▋    | 71/125 [00:03<00:02, 24.93it/s]#033[A\n",
      "59%|█████▉    | 74/125 [00:04<00:02, 23.69it/s]#033[A\n",
      "62%|██████▏   | 77/125 [00:04<00:01, 24.02it/s]#033[A\n",
      "64%|██████▍   | 80/125 [00:04<00:01, 24.35it/s]#033[A\n",
      "66%|██████▋   | 83/125 [00:04<00:01, 24.87it/s]#033[A\n",
      "69%|██████▉   | 86/125 [00:04<00:01, 24.98it/s]#033[A\n",
      "71%|███████   | 89/125 [00:04<00:01, 23.71it/s]#033[A\n",
      "74%|███████▎  | 92/125 [00:04<00:01, 23.46it/s]#033[A\n",
      "76%|███████▌  | 95/125 [00:04<00:01, 23.33it/s]#033[A\n",
      "78%|███████▊  | 98/125 [00:05<00:01, 23.74it/s]#033[A\n",
      "81%|████████  | 101/125 [00:05<00:00, 24.22it/s]#033[A\n",
      "83%|████████▎ | 104/125 [00:05<00:00, 24.54it/s]#033[A\n",
      "86%|████████▌ | 107/125 [00:05<00:00, 24.22it/s]#033[A\n",
      "88%|████████▊ | 110/125 [00:05<00:00, 24.52it/s]#033[A\n",
      "90%|█████████ | 113/125 [00:05<00:00, 24.91it/s]#033[A\n",
      "93%|█████████▎| 116/125 [00:05<00:00, 23.58it/s]#033[A\n",
      "95%|█████████▌| 119/125 [00:05<00:00, 24.23it/s]#033[A\n",
      "98%|█████████▊| 122/125 [00:06<00:00, 24.69it/s]#033[A\n",
      "100%|██████████| 125/125 [00:06<00:00, 24.37it/s]#033[A\n",
      "#033[A\n",
      "{'eval_loss': 0.2943398356437683, 'eval_f1': 0.9040426893370366, 'eval_runtime': 9.9125, 'eval_samples_per_second': 201.766, 'eval_steps_per_second': 12.61, 'epoch': 1.0}\n",
      "33%|███▎      | 500/1500 [01:20<01:43,  9.63it/s]\n",
      "#015100%|██████████| 125/125 [00:06<00:00, 24.37it/s]#033[A\n",
      "#033[A\n",
      "2023-06-02 07:50:12.000882: INFO ||NCC_WRAPPER||: Using a cached neff at /tmp/tmpmdp8amnm/neuron-compile-cache/USER_neuroncc-2.5.0.28+1be23f232/MODULE_1188293368889951551/MODULE_9_SyncTensorsGraph.22_1188293368889951551_algo-1-c18d4e69-109-5fb4eaf3015ff/d7fa7b39-5c63-4379-a0c2-c90383ec699d/MODULE_9_SyncTensorsGraph.22_1188293368889951551_algo-1-c18d4e69-109-5fb4eaf3015ff.neff. Exiting with a successfully compiled graph\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f35-0f8659fd09f7bce47e65e275)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f35-78d8d92b332de54a0df3ddd2)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f35-7ff8d706745f1afa015b942a)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f36-4095298d14f4f1037ea25b6a)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f36-02f037c0012d781f562db281)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n",
      "Could not push the cached model to the repo aws-neuron/optimum-neuron-cache, most likely due to not having the write permission for this repo. Exact error: 401 Client Error. (Request ID: Root=1-64799f36-30b8f96d750ad7c11bcba418)\n",
      "Repository Not Found for url: https://huggingface.co/api/models/aws-neuron/optimum-neuron-cache/preupload/main.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "Invalid username or password.\n",
      "Note: Creating a commit assumes that the repo already exists on the Huggingface Hub. Please use `create_repo` if it's not the case..\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how a model is trained and deployed with Amazon SageMaker:\n",
    "![assets](./assets/platform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy model to inferntia2 and run inference \n",
    "\n",
    "Now that we have trained our model, we want to deploy it to `inferentia2` on Amazon SageMaker so that we can use it for inference. When deploying models to `inferentia2`, we need to compile the model with the `neuron-sdk` or `optimum-neuron`. \n",
    "\n",
    "If you want to learn more about how to compile models for `inferentia2`, check out the [Optimum Neuron documentation](https://huggingface.co/docs/optimum-neuron/guides/export_model). \n",
    "\n",
    "As first we need to install `optimum-neuron` and the required neuron runtime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for amazon linux 2\n",
    "# !sudo yum install aws-neuronx-runtime-lib-2.* -y\n",
    "# for ubuntu 20.04\n",
    "!sudo apt-get install aws-neuronx-runtime-lib=2.12.* -y \n",
    "!pip install optimum optimum-neuron==0.0.3 --upgrade\n",
    "!pip install neuronx-cc==2.5.* torch-neuronx==1.13.0.1.6.1 --extra-index-url https://pip.repos.neuron.amazonaws.com --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load our trained model from S3 and compile it for `inferentia2`. We are using the `export()` function from `optimum-neuron` to compile our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.exporters.neuron import export\n",
    "from optimum.exporters.neuron.model_configs import RobertaNeuronConfig\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from tempfile import TemporaryDirectory\n",
    "import shutil\n",
    "\n",
    "with TemporaryDirectory() as tmp_dir:\n",
    "    # S3Downloader.download(model_data, tmp_dir)\n",
    "    S3Downloader.download(huggingface_estimator.model_data, tmp_dir)\n",
    "    shutil.unpack_archive(f\"{tmp_dir}/model.tar.gz\", tmp_dir)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(tmp_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tmp_dir)\n",
    "\n",
    "    neuron_config = RobertaNeuronConfig(config=model.config,\n",
    "                                           task=\"text-classification\",\n",
    "                                           batch_size=1, \n",
    "                                           sequence_length=128\n",
    "                                           )\n",
    "    output_path = Path(f\"tmp\")\n",
    "    # Export to Neuron model\n",
    "    export(\n",
    "        model=model,\n",
    "        config=neuron_config,\n",
    "        output=output_path.joinpath(\"neuron_model.pt\"),\n",
    "        auto_cast=\"all\",\n",
    "        auto_cast_type=\"bf16\",\n",
    "    )\n",
    "    model.config.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of the [pipeline feature](https://huggingface.co/transformers/main_classes/pipelines.html) from 🤗 Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)].\n",
    "\n",
    "Currently, this feature is not supported with AWS Inferentia2, which means we need to provide an `inference.py` script for running inference.\n",
    "\n",
    "To use the inference script, we need to create an `inference.py` script. In our example, we are going to overwrite the `model_fn` to load our neuron model and the `predict_fn` to create a text-classification pipeline.\n",
    "\n",
    "If you want to know more about the `inference.py` script check out this **[example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb)**. It explains amongst other things what `model_fn` and `predict_fn` are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "# saved weights name\n",
    "TRACED_WEIGHTS_NAME = \"neuron_model.pt\"\n",
    "TRACED_SEQUENCE_LEGNTH = 128\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load tokenizer and neuron model from model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = torch.jit.load(os.path.join(model_dir, TRACED_WEIGHTS_NAME))\n",
    "    model_config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer, model_config\n",
    "\n",
    "\n",
    "def predict_fn(data, model_tokenizer_model_config):\n",
    "    # destruct model, tokenizer and model config\n",
    "    model, tokenizer, model_config = model_tokenizer_model_config\n",
    "\n",
    "    # create embeddings for inputs\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    embeddings = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=TRACED_SEQUENCE_LEGNTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # convert to tuple for neuron model\n",
    "    neuron_inputs = tuple(embeddings.values())\n",
    "\n",
    "    # run prediciton\n",
    "    with torch.no_grad():\n",
    "        predictions = model(*neuron_inputs)[0]\n",
    "        scores = torch.nn.Softmax(dim=1)(predictions)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return [{\"label\": model_config.id2label[item.argmax().item()], \"score\": item.max().item()} for item in scores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts and inference script. We can do this with the `tar` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import os \n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\"\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/startup-loft/compiled-models\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have uploaded our `model.tar.gz` to Amazon S3 can we create a custom `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "When we create the endpoint, SageMaker automatically provisions the specified inference instances and deploys our model to them. We can then send inference requests to the endpoint and receive predictions from our model. We can use the `deploy()` method from  our HuggingFace estimator, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferentia2_image_uri=\"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.12\",  # transformers version used\n",
    "   image_uri=inferentia2_image_uri,\n",
    "   py_version='py37',            # python version used\n",
    "   model_server_workers=2,\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model via neuron-cc\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia2 Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferentia2 is the second generation purpose built Machine Learning inference accelerator from AWS. The Inferentia2 device architecture is depicted below:\n",
    "\n",
    "![assets](./assets/inferentia2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_input = {\"inputs\":\"I love using the new Inferentia2 instance on Amazon SageMaker.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "predictor = HuggingFacePredictor(endpoint_name=\"huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled RoBERTa to AWS Inferentia2 on Amazon SageMaker. Now, let's test its performance. As a dummy load test, we will loop and send 5,000 requests to our endpoint and inspect the performance in cloudwatch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.aws.amazon.com/cloudwatch/home?region=us-east-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'us-east-2~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20huggingface-pytorch-inference-neuronx-m-2023-05-10-14-09-36-983\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:59<00:00, 11.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_requests = 5_000  # 1m requests\n",
    "for i in tqdm(range(total_requests)):\n",
    "    predictor.predict(sentiment_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average latency for our RoBERTA model is 1-2ms for a sequence length of 128.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up running endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
